[
  {
    "objectID": "initial_notes.html",
    "href": "initial_notes.html",
    "title": "Prerequisite Studies",
    "section": "",
    "text": "an interval that contains at least a specified proportion \\(\\gamma\\) of the population, with a specified degree of confidence, \\(100 \\left( 1 - \\alpha\\right)\\%\\)\nLet \\(\\mathcal{X} = \\left\\{ X_1, X_2, \\dots, X_n \\right\\}\\) be a random sample from univariate distribution \\(F(\\cdot)\\) and suppose \\(X \\sim F(\\cdot)\\), where \\(X\\) is independent of the random sample, then a subset of \\(\\mathbb{R}\\), say \\(T = T (\\mathcal{X})\\), computed from the random sample, is a \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval for \\(X\\) if it satisfies the ff: (The probability (over all possible samples \\(\\mathcal{X}\\)) that the tolerance interval \\(T\\) constructed from the sample will capture at least a proportion \\(\\gamma\\) of the population is equal to \\(1-\\alpha\\))\nIf I keep collecting samples and building intervals over and over, then 95% of the time, those intervals will contain at least 90% of the population. - That’s a (\\(\\gamma\\), \\(1-\\alpha\\)) tolerance interval.\n\n\\[\nP_{\\mathcal{X}} \\left\\{ P_X \\left( X \\in T \\mid \\mathcal{X} \\right) \\geq \\gamma \\right\\} = 1 - \\alpha\n\\]\n\nset \\(T\\) could be of the form:\n\n\\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\) for a two-sided tolerance interval\n\\(\\left(-\\infty, U(\\mathcal{X}) \\right)\\) for a one-sided upper tolerance interval\n\\(\\left( L(\\mathcal{X}), +\\infty) \\right)\\) for a one-sided lower tolerance interval\n\\(U(\\mathcal{X})\\) and \\(L(\\mathcal{X})\\): upper and lower tolerance limits\n\nsample use case: deriving appropriate process capability limits (\\(U(\\mathcal{X})\\) and \\(L(\\mathcal{X})\\)) for a manufactured product (\\(X\\)), so that with a given level of confidence (\\(1 - \\alpha\\)), they contain the capability measurements of at least a specified proportion of units (\\(\\gamma\\)) from the sampled manufacturing process (ie, gusto mo may confidence ka na yung tolerance interval mo, contains \\(\\gamma\\) proportion nung samples)\ntwo-sided interval\n\nWhat interval will contain \\(p\\) percent of the population measurements?\n\nLower tolerance limit: \\(Y_L = \\hat{Y} - k_2s\\)\nUpper tolerance limit: \\(Y_L = \\hat{Y} + k_2s\\)\n\none-sided intervals\n\nWhat interval guarantees that \\(p\\) percent of population measurements will not fall below a lower limit? Lower tolerance limit: \\(Y_L = \\hat{Y} - k_1s\\)\nWhat interval guarantees that \\(p\\) percent of population measurements will not exceed an upper limit? Lower tolerance limit: \\(Y_U = \\hat{Y} + k_1s\\)\n\n\n\n\n\nUse the method for a parametric distribution if you can safely assume that your sample comes from a population that follows that distribution.\nIf your data follow a parametric distribution, then a method that uses that distribution is more precise and economical than the nonparametric method. A method that uses a distribution achieves smaller margins of error with fewer observations, as long as the chosen distribution is appropriate for your data.\n\nMinitab\n\n\n\n\nSuppose \\(X_{(1)}, X_{(2)}, \\dots, X_{(n)}\\) are order statistics of a random sample from a population with a univariate continuous distribution function \\(F(x)\\).\nWilk’s approach: 2 sided interval\n\nsets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\)\nThe values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\). It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that\nthe nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\nWilk’s approach: 1 sided interval: Lower tolerance limit\n\nsets \\(L(\\mathcal{X}) = X_{(r)}\\) as the lower limit of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval.\nThe value of \\(r\\) is taken to be the largest integer for which \\(P(Y \\ge r \\mid n, 1- \\gamma) \\ge 1 - \\alpha\\) where \\(Y \\sim \\text{Binomial}(n, 1-\\gamma)\\).\nThe non parametric one-sided lower tolerance interval is given by \\((X_{(r)}, \\infty)\\)\n\nWilk’s approach: 1 sided interval: Upper tolerance limit\n\nsets \\(U(\\mathcal{X}) = X_{S}\\), where it can be shown that\n\\(s=n-r+1\\) and \\(r\\) is the value derived for the lower tolerance limit.\nThe nonparametric one-sided upper tolerance interval is given by \\((-\\infty, X_{(n-r+1)})\\)\n\nThere is a minimum sample size requirement in computing the nonparametric tolerance intervals. This depends on the values of \\(\\gamma\\) and \\(1-\\alpha\\)\nThe coverage probabilities associated with nonparametric tolerance intervals are known to be conservative.\nAlthough in this study we shall only be using Wilks’ approach, we mention that recently some developments aimed at improving coverage probabilities of nonparametric tolerance intervals have been available in literature.\n\n\n\n\nSuppose that we take a sample of \\(N=25\\) silicon wafers from a lot and measure their thickness in order to find tolerance limits within which a proportion \\(p = 0.90\\) of the wafers in the lot fall with confidence \\(\\alpha = 0.99\\). Since the standard deviation, \\(s\\), is computed from the sample of 25 wafers, the degree of freedom is \\(\\nu = N - 1\\)\n\nThe dataParametric approachNonparametric approach\n\n\n\n## \"Direct\" calculation of a tolerance interval.\n\n## Read data and name variables.\n\n# URL of the dataset\n#url &lt;- \"https://www.itl.nist.gov/div898/handbook/datasets/MPC62.DAT\"\n\n# Download the file (you can specify a path where you want to save it)\n# download.file(url, destfile = \"MPC62.DAT\")\n\n# Load the dataset into R\nmdat = read.table(\"rdata/MPC62.DAT\",header=FALSE, skip=50)\ncolnames(mdat) = c(\"cr\", \"wafer\", \"mo\", \"day\", \"h\", \"min\", \"op\", \n                 \"hum\", \"probe\", \"temp\", \"y\", \"sw\", \"df\")\n\nhead(mdat)\n\n     cr wafer mo day  h min op hum probe   temp      y    sw df\n1 51939   137  3  24 18   1  1  42  2362 23.003 97.070 0.085  5\n2 51939   137  3  25 12  41  1  35  2362 23.115 97.049 0.052  5\n3 51939   137  3  25 15  57  1  33  2362 23.196 97.048 0.038  5\n4 51939   137  3  28 10  10  2  47  2362 23.383 97.084 0.036  5\n5 51939   137  3  28 13  31  2  44  2362 23.491 97.106 0.049  5\n6 51939   137  3  28 17  33  1  43  2362 23.352 97.014 0.036  5\n\n\n\nlibrary(tolerance)\n\ntolerance package, version 3.0.0, Released 2024-04-18\nThis package is based upon work supported by the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\n\n\n\nThrough tolerance package\n\npout = normtol.int(mdat$y, \n                   alpha=0.01, # chosen such that 1-alpha is the conf level\n                   P=.90, # proportion of the pop to be covered by this TI\n                   side=2, # 1-sided or 2-sided tolerance interval\n                   method = \"HE2\", # method for calculating the k-factors\n                   m=100 # maxnum of subintervals to use in the integrate fcn\n                   )\npout\n\n  alpha   P    x.bar 2-sided.lower 2-sided.upper\n1  0.01 0.9 97.06984        97.003      97.13668\n\n\nTI Interpretation: In this example, you can be 99% confident that at least 90% of all thickness measurements are between approximately 97.0030038 and 97.1366762.\nManually deriving the endpoints:\nThe \\(k\\) factors are determined so that the intervals cover at least a proportion \\(p\\) of the population with confidence \\(\\alpha\\). The value of \\(p\\) is also referred to as the coverage factor. This assumes normal distribution. This is the approximate value for \\(k_2\\) from Howe, 1969.\n\\[\n\\sqrt{\n\\nu \\left(1+ \\frac{1}{N} \\right) \\times \\frac {z_{\\left( \\frac{1+p}{2}\\right)}^2}\n{\\chi^2_{1-\\alpha, \\nu}}\n}\n\\]\n\n## Compute the approximate k2 factor for a two-sided tolerance interval. \n## For this example, the standard deviation is computed from the sample,\n## so the degrees of freedom are nu = N - 1.\nN = 25\nnu = N - 1 # df used to estimate the sd\np = 0.90 # proportion\ng = 0.99 # alpha\nz2 = (qnorm((1+p)/2))**2 # critical values of the normal distribution\nc2 = qchisq(1-g,nu) # lower critical values of the chi-square distribution\nk2 = sqrt(nu*(1 + 1/N)*z2/c2)\nk2\n\n[1] 2.494063\n\n## Compute the exact k2 factor for a two-sided tolerance interval using \n## the K.factor function in the tolerance library\n## \"HE2\" is a second method due to Howe, which performs similarly to the \n## Weissberg-Beatty method, but is computationally simpler\nK2 = K.factor(n=N, f=nu, alpha=1-g, P=p, side=2, method=\"HE2\", m=1000)\nK2\n\n[1] 2.494063\n\n\n\npout$x.bar-K2*sd(mdat$y); pout$x.bar+K2*sd(mdat$y)\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\n\nplottol(pout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\nParametric and univariate\nGiven a sample size \\(n\\), sample mean \\(\\bar{x}\\), and sample standard deviation \\(s\\), the two-sided normal tolerance interval is:\n\\[\n\\bar{x} \\pm k \\cdot s\n\\]\nwhere \\(k\\) is a factor based on:\n\\(n\\), confidence level, \\((1-\\alpha)\\), and population coverage \\(\\gamma\\)\nIn R, to find the smallest value \\(m\\) such that \\(P(Y \\le m-1) \\ge 1-\\alpha\\) for a binomial distribution \\(Y \\sim \\text{Binomial}(n, \\gamma)\\), you can use the cumulative distribution function (CDF) for the Binomial distribution, which is available in the pbinom() function.\nThe condition \\(P(Y \\le m-1) \\ge 1-\\alpha\\) suggests that you are looking for the quantile \\(m\\) such that the cumulative probability is at least \\(1-\\alpha\\).\nRef\n\n\nThrough tolerance package\n\nnpout = nptol.int(mdat$y,\n                  alpha = 0.01, \n                  P = 0.90, \n                  side = 2, \n                  method = \"WILKS\",\n                  upper = NULL, \n                  lower = NULL)\nnpout\n\n  alpha   P 2-sided.lower 2-sided.upper\n1  0.01 0.9        97.014        97.114\n\n\nTI Interpretation: You can be 99% confident that at least 90% of all thickness measurements are between approximately 97.014 and 97.114.\nManually deriving the endpoints:\nWilk’s approach: 2 sided interval\n- sets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\)\n- The values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\).\n- It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that - the nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\n# 1 &lt;= r &lt; s &lt; N = 25\n# let s = n−r+1\n# s - r = (n-r+1) - r = m \n# (n-m+1)/2 = r\n\n# Parameters\nn &lt;- 25    # Number of trials\ngamma &lt;- 0.90 # Probability of success in each trial\nalpha &lt;- 0.01 # Significance level (1 - confidence level)\n\n# Find the smallest m such that P(Y &lt;= m-1) &gt;= 1 - alpha ie find the \n# smallest value of m such that the cumulative probability just reaches \n# or exceeds the confidence level 1-alpha\nm &lt;- qbinom(1 - alpha, size = n, prob = gamma)\n\nr = (n-m+1)/2\ns = m+r\nsort(mdat$y)[ceiling(r)]; sort(mdat$y)[floor(n-r+1)]\n\n[1] 97.014\n\n\n[1] 97.114\n\n\n\nplottol(npout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) means \\(X\\) is normally distributed.\n\\(Y \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\) means \\(\\log Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nA lognormal random variable is one whose logarithm is normally distributed.\nIf you assume the data itself is normal (norm): The raw data \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nIf you assume the log of the data is normal (lnorm): You are modeling \\(\\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) so \\(X \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\)\nMean is zero in the log scale means \\(\\log(Y) \\sim \\mathcal{N}(0, \\sigma^2)\\) which is equivalent to \\(Y \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\)\nThis also tells that the mean of \\(Y\\) is not 0. In fact, \\(\\mathbb{E}[Y] =  e^{\\mu+\\sigma^2/2}\\) so if \\(\\mu = 0\\) then, \\(\\mathbb{E}[Y] =  e^{\\sigma^2/2} &gt; 1\\)\n\nset.seed(5)\nlog_x &lt;- rnorm(1000, 0, 1)\nset.seed(5)\nlnorm_x &lt;- rlnorm(1000, 0, 1)\n\npar(mfrow = c(1, 2))\nhist(log_x, main = \"Histogram: log(x)\", xlab = \"log(x)\")\n\nhist(exp(log_x), main = \"exp(log_x) & lnorm_x\", xlab = \"exp(log_x) & lnorm(x)\",\n     col = rgb(1, 0, 0, 0.5), xlim = range(c(exp(log_x), lnorm_x)), breaks = 30)\n\n# Overlay histogram of lnorm_x\nhist(lnorm_x, col = rgb(0, 0, 1, 0.3), add = TRUE, breaks = 30)\n\n# Add a legend\nlegend(\"topright\", legend = c(\"log_y\", \"lnorm_x\"),\n       fill = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.2)))\n\n\n\n\n\n\n\n\n\nsum(exp(log_x)==lnorm_x)\n\n[1] 1000\n\n\n\n\n\n\nfind_silver_bw &lt;- function(sample){\n  n &lt;- length(sample)\n  iqr &lt;- IQR(sample)\n  sd &lt;- sd(sample)\n  return(0.9*n^(-1/5)*min(c(sd, iqr/1.34)))\n}\n\nget_grid &lt;- function(sample){\n  lb &lt;- min(sort(sample)) - sd(sample)\n  ub &lt;- max(sort(sample)) + sd(sample)\n  return(seq(from=lb, to=ub, by=0.0001))\n}\n\ngaussian_kernel &lt;- function(bandwidth, grid, datapoint) {\n  return((1/(bandwidth*sqrt(2*pi))) * exp(-0.5*((grid-datapoint)/bandwidth)^2))\n#  return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))\n}\n\n\nkde_manual &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    sum(gaussian_kernel(bandwidth, grid_val, datapoints) / n)\n  })\n}\n\nkde_manual_presum &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    gaussian_kernel(bandwidth, grid_val, datapoints)\n  })\n}\n\n\n# KDE for CDF estimation using Gaussian kernel\nkde_cdf &lt;- function(grid, datapoints, bandwidth) {\n  sapply(grid, function(grid_val) {\n    mean(pnorm((grid_val - datapoints) / bandwidth))\n  })\n}\n\nReference 1 Plotting KDE\nKernel - Kernel functions are used to estimate density of random variables and as weighing function in non-parametric regression.\n\nx_data &lt;- sort(c(65, 75, 67, 79, 81, 91))\nx_data_sd &lt;- sd(x_data)\nx_data_iqr &lt;- IQR(x_data)\nx_data_n &lt;- length(x_data)\nh_silver &lt;- 0.9*x_data_n^(-1/5)*min(c(x_data_sd, x_data_iqr/1.34))\n\n\nxi &lt;- 65#x_data[1]\nh &lt;- 5.5 #h_silver\nmin_val &lt;- 40#min(x_data)-round(h)\nmax_val &lt;- 120#max(x_data)+round(h)\nx &lt;- seq(from = min_val, to = max_val, by = 1) # grid\n\n# Estimate PDF\noutmat &lt;- t(kde_manual_presum(x, x_data, h))\nestimated_density &lt;- kde_manual(x, x_data, h)\n\n# Estimate CDF\nestimated_cdf &lt;- kde_cdf(x, x_data, h)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\nrownames(outmat) &lt;- x\ncolnames(outmat) &lt;- x_data\nmat &lt;- cbind(grid = rownames(outmat), outmat)\nrownames(outmat) &lt;- 1:nrow(outmat)\n\ndf_long &lt;- as.data.frame(mat,as.numeric) %&gt;%\n  pivot_longer(cols = colnames(outmat), names_to = \"sample\", values_to = \"k\")\n\ndf_long$grid &lt;- as.numeric(df_long$grid)\ndf_long$sample &lt;- as.factor(df_long$sample)\ndf_long$k &lt;- as.numeric(df_long$k)\ndf_long &lt;- df_long %&gt;% mutate(norm_kern = k/6)\ncomp &lt;- df_long %&gt;% group_by(grid) %&gt;% summarize(composite=sum(k)/x_data_n)\nggplot() +\n  geom_line(data = df_long, aes(x = grid, y = norm_kern, color = sample, group = sample), alpha = 0.75) +\n  geom_line(data = comp, aes(x = grid, y = composite), alpha = 0.5) +\n  xlim(min_val, max_val) +\n  geom_vline(xintercept = c(65, 67, 75, 79, 81, 91), linetype = \"dotted\", size = 0.2) +\n  theme_minimal() +\n  ggtitle(\"Kernels at different data points\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nx_data &lt;- sort(c(65, 75, 67, 79, 81, 91))\nx_data_sd &lt;- sd(x_data)\nx_data_iqr &lt;- IQR(x_data)\nx_data_n &lt;- length(x_data)\nh_silver &lt;- 0.9*x_data_n^(-1/5)*min(c(x_data_sd, x_data_iqr/1.34))\n\nxi &lt;- 65#x_data[1]\nh &lt;- h_silver\nmin_val &lt;- 40#min(x_data)-round(h)\nmax_val &lt;- 120#max(x_data)+round(h)\nx &lt;- seq(from = min_val, to = max_val, by = 1) # grid\n\n# Approximate CDF using left Riemann sums\ndx &lt;- diff(x)            # Spacing between x points (assumes equally spaced)\ndx &lt;- c(dx, tail(dx, 1))      # Pad to same length\n\ncdf_from_pdf &lt;- cumsum(comp[['composite']] * dx)\n\nplot(x, cdf_from_pdf, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDFs Comparison\")\n\nlines(x, estimated_cdf, lwd = 2, col = \"red\", lty = 2)\n\nlines(x, ecdf(x_data)(x), col = \"darkgreen\", lwd = 2, lty = 3)\nlegend(\"bottomright\", \n       legend = c(\"Numerical Integration\", \"Kernel CDF (pnorm)\", \"Empirical CDF\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), \n       lty = c(1, 2, 3), \n       lwd = 2)\n\n\n\n\n\n\n\n\n\nexp_log_x &lt;- exp(log_x)\nh &lt;- find_silver_bw(exp_log_x)\nestimated_cdf_sim &lt;- kde_cdf(get_grid(exp_log_x), exp_log_x, h)\nestimated_pdf_sim &lt;- kde_manual(get_grid(exp_log_x), exp_log_x, h)\nget_grid_exp_log_x &lt;- get_grid(exp_log_x)\n\npar(mfrow = c(1, 3))\nhist(exp(log_x), main = \"Histogram: exp(log_x)\", xlab = \"exp(log_x)\",\n     col = rgb(1, 0, 0, 0.5), xlim = range(c(exp(log_x), lnorm_x)), breaks = 30)\nplot(get_grid_exp_log_x, estimated_cdf_sim, type = \"l\", lwd = 1, col = \"red\",\n     xlab = \"exp(log_x)\", ylab = \"KDE-Estimated CDF\",\n     main = \"Estimated CDF\")\nplot(get_grid_exp_log_x, estimated_pdf_sim, type = \"l\", lwd = 1, col = \"red\",\n     xlab = \"exp(log_x)\", ylab = \"KDE-Estimated PDF\",\n     main = \"Estimated PDF\")\n\n\n\n\n\n\n\n\n\nlibrary(\"GoFKernel\")\n\nLoading required package: KernSmooth\n\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\n# kde_cdf_value &lt;- function(x, datapoints, bandwidth) {\n#   mean(pnorm((x - datapoints) / bandwidth))\n# }\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\n\n# kde_cdf_inverse &lt;- function(probabilities, datapoints, bandwidth, lower = -10, upper = 10) {\n#   sapply(probabilities, function(p) {\n#     root_func &lt;- function(x) kde_cdf_value(x, datapoints, bandwidth) - p\n#     uniroot(root_func, lower = lower, upper = upper)$root\n#   })\n# }\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\nget_upper_limit &lt;- function(p_data){\n  h &lt;- find_silver_bw(p_data)\n  \n  cdf_j &lt;- kde_cdf_function(p_data, h)\n  U_mat &lt;- sapply(p_data, cdf_j)\n\n  Y_mat &lt;- pmax(U_mat, 1 - U_mat)\n\n  npout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n  \n  return(npout$`1-sided.upper`)\n}\n\nget_tolerance_interval &lt;- function(p_data, maxk=max(k_j)){\n  h &lt;- find_silver_bw(p_data)\n  cdf_fun &lt;- kde_cdf_function(p_data,h)\n\n  inv_cdf &lt;- inverse(cdf_fun,lower=min(p_data) - 3 * h,upper=max(p_data) + 3 * h)\n  lower_bound &lt;- inv_cdf(1 - maxk)\n  upper_bound &lt;- inv_cdf(maxk)\n\n  return(c(lower_bound, upper_bound))\n}\n\n\n\n\n\nhist(kde_cdf(exp(log_x), exp(log_x), find_silver_bw(exp(log_x))), probability = TRUE, \n     main = \"Histogram of PIT-transformed values\",\n     xlab = \"PIT values\")\nabline(h = 1, col = \"red\", lwd = 2) \n\n\n\n\n\n\n\n\nProbability Integral Transform"
  },
  {
    "objectID": "initial_notes.html#questions",
    "href": "initial_notes.html#questions",
    "title": "",
    "section": "",
    "text": "two-sided interval\n\nWhat interval will contain \\(p\\) percent of the population measurements?\n\nLower tolerance limit: \\(Y_L = \\hat{Y} - k_2s\\)\nUpper tolerance limit: \\(Y_L = \\hat{Y} + k_2s\\)\n\none-sided intervals\n\nWhat interval guarantees that \\(p\\) percent of population measurements will not fall below a lower limit? - Lower tolerance limit: \\(Y_L = \\hat{Y} - k_1s\\)\nWhat interval guarantees that \\(p\\) percent of population measurements will not exceed an upper limit - Lower tolerance limit: \\(Y_U = \\hat{Y} + k_1s\\)\n\n\nThe \\(k\\) factors are determined so that the intervals cover at least a proportion \\(p\\) of the population with confidence \\(\\alpha\\).\nThe value of \\(p\\) is also referred to as the coverage factor."
  },
  {
    "objectID": "initial_notes.html#univariate-tolerance-intervals-two-sided",
    "href": "initial_notes.html#univariate-tolerance-intervals-two-sided",
    "title": "",
    "section": "Univariate Tolerance Intervals (Two-sided)",
    "text": "Univariate Tolerance Intervals (Two-sided)\n\nParametric\nTolerance factor assuming normal distribution\nThis is the approximate value for \\(k_2\\) from Howe, 1969.\n$$ $$\n\n## Compute the approximate and exact k2 factor.\n\n## Compute the approximate k2 factor for a two-sided tolerance interval. \n## For this example, the standard deviation is computed from the sample,\n## so the degrees of freedom are nu = N - 1.\nN = 25\nnu = N - 1 # df used to estimate the sd\np = 0.90 # proportion\ng = 0.99 # alpha\nz2 = (qnorm((1+p)/2))**2 # z critical values of the normal distribution\nc2 = qchisq(1-g,nu) # chi X lower critical values of the chi-square distribution\nk2 = sqrt(nu*(1 + 1/N)*z2/c2)\nk2\n\n[1] 2.494063\n\n\n2-sided tolerance factor thru tolerance package\n\n## Compute the exact k2 factor for a two-sided tolerance interval using \n## the K.factor function in the tolerance library\n## \"HE2\" is a second method due to Howe, which performs similarly to the Weissberg-Beatty method, but is computationally simpler\nlibrary(tolerance)\n\ntolerance package, version 3.0.0, Released 2024-04-18\nThis package is based upon work supported by the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\nK2 = K.factor(n=N, f=nu, alpha=1-g, P=p, side=2, method=\"HE2\", m=1000)\nK2\n\n[1] 2.494063\n\n\n\n\nExample\nSuppose that we take a sample of \\(N=25\\) silicon wafers from a lot and measure their thickness in order to find tolerance limits within which a proportion \\(p = 0.90\\) of the wafers in the lot fall with confidence \\(\\alpha = 0.99\\). Since the standard deviation, \\(s\\), is computed from the sample of 25 wafers, the df are \\(\\nu = N - 1\\)\n\nThe dataParametric approachNonparametric approachAside\n\n\n\n## \"Direct\" calculation of a tolerance interval.\n\n## Read data and name variables.\n\n# URL of the dataset\n#url &lt;- \"https://www.itl.nist.gov/div898/handbook/datasets/MPC62.DAT\"\n\n# Download the file (you can specify a path where you want to save it)\n# download.file(url, destfile = \"MPC62.DAT\")\n\n# Load the dataset into R\nmdat = read.table(\"MPC62.DAT\",header=FALSE, skip=50)\ncolnames(mdat) = c(\"cr\", \"wafer\", \"mo\", \"day\", \"h\", \"min\", \"op\", \n                 \"hum\", \"probe\", \"temp\", \"y\", \"sw\", \"df\")\n\nhead(mdat)\n\n     cr wafer mo day  h min op hum probe   temp      y    sw df\n1 51939   137  3  24 18   1  1  42  2362 23.003 97.070 0.085  5\n2 51939   137  3  25 12  41  1  35  2362 23.115 97.049 0.052  5\n3 51939   137  3  25 15  57  1  33  2362 23.196 97.048 0.038  5\n4 51939   137  3  28 10  10  2  47  2362 23.383 97.084 0.036  5\n5 51939   137  3  28 13  31  2  44  2362 23.491 97.106 0.049  5\n6 51939   137  3  28 17  33  1  43  2362 23.352 97.014 0.036  5\n\n\n\n\ntolerance package - parametric\n\nlibrary(tolerance)\npout = normtol.int(mdat$y, \n                   alpha=0.01, # The level chosen such that 1-alpha is the confidence level.\n                   P=.90, # The proportion of the population to be covered by this tolerance interval.\n                   side=2, # 1-sided or 2-sided tolerance interval\n                   method = \"HE2\", # method for calculating the k-factors\n                   m=100 # maximum number of subintervals to be used in the integrate function; larger more accurate\n                   )\npout\n\n  alpha   P    x.bar 2-sided.lower 2-sided.upper\n1  0.01 0.9 97.06984        97.003      97.13668\n\n\nHighlighting the endpoints from pout:\n\npout$`2-sided.lower`; pout$`2-sided.upper`\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\nManually deriving the endpoints:\n\npout$x.bar-K2*sd(mdat$y); pout$x.bar+K2*sd(mdat$y)\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\n\nplottol(pout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\nRef\n\n\ntolerance package - nonparametric\n\nnpout = nptol.int(mdat$y,\n                  alpha = 0.01, \n                  P = 0.90, \n                  side = 2, \n                  method = \"WILKS\",\n                  upper = NULL, \n                  lower = NULL)\nnpout\n\n  alpha   P 2-sided.lower 2-sided.upper\n1  0.01 0.9        97.014        97.114\n\n\nHighlighting the endpoints from npout:\n\nnpout$`2-sided.lower`; npout$`2-sided.upper`\n\n[1] 97.014\n\n\n[1] 97.114\n\n\nManually deriving the endpoints:\nWilk’s approach: 2 sided interval - sets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\) - The values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\). It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that - the nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\n# 1 &lt;= r &lt; s &lt; N = 25\n\n\n# let s = n−r+1\n# s - r = (n-r+1) - r = m \n# (n-m+1)/2 = r\n\n# Parameters\nn &lt;- 25    # Number of trials\ngamma &lt;- 0.90 # Probability of success in each trial\nalpha &lt;- 0.01 # Significance level (1 - confidence level)\n\n# Find the smallest m such that P(Y &lt;= m-1) &gt;= 1 - alpha\n# ie find the smallest value of m such that the cumulative probability just reaches or exceeds the confidence level 1-alpha\nm &lt;- qbinom(1 - alpha, size = n, prob = gamma)\n\nr = (n-m+1)/2\ns = m+r\nsort(mdat$y)[ceiling(r)]; sort(mdat$y)[floor(n-r+1)]\n\n[1] 97.014\n\n\n[1] 97.114\n\n\n\nplottol(npout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\n\n\n\nset.seed(5)\nsimulated &lt;- rlnorm(1000, 0, 1)\nhist(simulated)\n\n\n\n\n\n\n\n\n\nfind_silver_bw &lt;- function(sample){\n  n &lt;- length(sample)\n  iqr &lt;- IQR(sample)\n  sd &lt;- sd(sample)\n  return(0.9*n^(-1/5)*min(c(sd, iqr/1.34)))\n}\n\nget_grid &lt;- function(sample){\n  lb &lt;- min(sort(sample)) - sd(sample)\n  ub &lt;- max(sort(sample)) + sd(sample)\n  return(seq(from=lb, to=ub, by=0.0001))\n}\n\ngaussian_kernel &lt;- function(bandwidth, grid, datapoint) {\n  return((1/(bandwidth*sqrt(2*pi))) * exp(-0.5*((grid-datapoint)/bandwidth)^2))\n#  return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))\n}\n\n\n# fcn to estimate \nkde_manual &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    sum(gaussian_kernel(bandwidth, grid_val, datapoints) / n)\n  })\n}\n\nkde_manual_presum &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    gaussian_kernel(bandwidth, grid_val, datapoints)\n  })\n}\n\n\n# KDE for CDF estimation using Gaussian kernel\nkde_cdf &lt;- function(grid, datapoints, bandwidth) {\n  sapply(grid, function(grid_val) {\n    mean(pnorm((grid_val - datapoints) / bandwidth))\n  })\n}\n\n\nestimated_cdf_sim &lt;- kde_cdf(get_grid(simulated), simulated, find_silver_bw(simulated))\n\n\nplot(get_grid(simulated), estimated_cdf_sim, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDFs Comparison\")\n\n\n\n\n\n\n\n\n\nestimated_pdf_sim &lt;- kde_manual(get_grid(simulated), simulated, find_silver_bw(simulated))\n\n\nplot(get_grid(simulated), estimated_pdf_sim, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated PDF\",\n     main = \"Estimated CDFs Comparison\")\n\n\n\n\n\n\n\n\n\nlibrary(\"GoFKernel\")\n\nLoading required package: KernSmooth\n\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\nkde_cdf_value &lt;- function(x, datapoints, bandwidth) {\n  mean(pnorm((x - datapoints) / bandwidth))\n}\n\nkde_cdf_inverse &lt;- function(probabilities, datapoints, bandwidth, lower = -10, upper = 10) {\n  sapply(probabilities, function(p) {\n    root_func &lt;- function(x) kde_cdf_value(x, datapoints, bandwidth) - p\n    uniroot(root_func, lower = lower, upper = upper)$root\n  })\n}\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\n\n\n\n# =================\n\nU_mat &lt;- kde_cdf(simulated, simulated, find_silver_bw(simulated))\n#U_mat &lt;- sapply(p_data, function(i) kde_cdf(i, p_data, h))\n\n# Step 4: Compute Y_ij = max{U_ij, 1 - U_ij}\nY_mat &lt;- pmax(U_mat, 1 - U_mat)\n\n#u&lt;-data.frame(original = transformed_data, complement = 1 - transformed_data)\n#y1 &lt;- apply(u, 1, max)\nnpout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                upper = NULL, lower = NULL)\np &lt;- npout$`1-sided.upper`\n\nf.inv &lt;- inverse(kde_cdf_function(simulated,find_silver_bw(simulated)),lower=-100,upper=100)\n  \n#f.inv(p);f.inv(1-p); qlnorm(1-p); exp(qnorm(1-p))\n\n\npnorm(log(f.inv(p)),0,1) - pnorm(log(f.inv(1-p)),0,1)\n\n[1] 0.9274381\n\n\n\nplnorm(f.inv(p),0,1) - plnorm(f.inv(1-p),0,1)\n\n[1] 0.9274381\n\n\n\nf.inv &lt;- inverse(function(i)kde_cdf(i, simulated,find_silver_bw(simulated)),lower=-1000,upper=1000)\n  \np &lt;- 0.97\nf.inv(p); qnorm(p)\n\n[1] 6.773139\n\n\n[1] 1.880794\n\n\n\nf &lt;- function(x) pbeta(x, shape1=2, shape2=3)\nf.inv &lt;- inverse(f,lower=0,upper=1)\nf.inv(.2)\n\n[1] 0.2123161\n\nqbeta(p=0.2, shape1=2, shape2=3)\n\n[1] 0.2123171"
  },
  {
    "objectID": "initial_notes.html#kernel-density-estimation",
    "href": "initial_notes.html#kernel-density-estimation",
    "title": "Prerequisite Studies",
    "section": "",
    "text": "find_silver_bw &lt;- function(sample){\n  n &lt;- length(sample)\n  iqr &lt;- IQR(sample)\n  sd &lt;- sd(sample)\n  return(0.9*n^(-1/5)*min(c(sd, iqr/1.34)))\n}\n\nget_grid &lt;- function(sample){\n  lb &lt;- min(sort(sample)) - sd(sample)\n  ub &lt;- max(sort(sample)) + sd(sample)\n  return(seq(from=lb, to=ub, by=0.0001))\n}\n\ngaussian_kernel &lt;- function(bandwidth, grid, datapoint) {\n  return((1/(bandwidth*sqrt(2*pi))) * exp(-0.5*((grid-datapoint)/bandwidth)^2))\n#  return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))\n}\n\n\nkde_manual &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    sum(gaussian_kernel(bandwidth, grid_val, datapoints) / n)\n  })\n}\n\nkde_manual_presum &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    gaussian_kernel(bandwidth, grid_val, datapoints)\n  })\n}\n\n\n# KDE for CDF estimation using Gaussian kernel\nkde_cdf &lt;- function(grid, datapoints, bandwidth) {\n  sapply(grid, function(grid_val) {\n    mean(pnorm((grid_val - datapoints) / bandwidth))\n  })\n}\n\nReference 1 Plotting KDE\nKernel - Kernel functions are used to estimate density of random variables and as weighing function in non-parametric regression.\n\nx_data &lt;- sort(c(65, 75, 67, 79, 81, 91))\nx_data_sd &lt;- sd(x_data)\nx_data_iqr &lt;- IQR(x_data)\nx_data_n &lt;- length(x_data)\nh_silver &lt;- 0.9*x_data_n^(-1/5)*min(c(x_data_sd, x_data_iqr/1.34))\n\n\nxi &lt;- 65#x_data[1]\nh &lt;- 5.5 #h_silver\nmin_val &lt;- 40#min(x_data)-round(h)\nmax_val &lt;- 120#max(x_data)+round(h)\nx &lt;- seq(from = min_val, to = max_val, by = 1) # grid\n\n# Estimate PDF\noutmat &lt;- t(kde_manual_presum(x, x_data, h))\nestimated_density &lt;- kde_manual(x, x_data, h)\n\n# Estimate CDF\nestimated_cdf &lt;- kde_cdf(x, x_data, h)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\nrownames(outmat) &lt;- x\ncolnames(outmat) &lt;- x_data\nmat &lt;- cbind(grid = rownames(outmat), outmat)\nrownames(outmat) &lt;- 1:nrow(outmat)\n\ndf_long &lt;- as.data.frame(mat,as.numeric) %&gt;%\n  pivot_longer(cols = colnames(outmat), names_to = \"sample\", values_to = \"k\")\n\ndf_long$grid &lt;- as.numeric(df_long$grid)\ndf_long$sample &lt;- as.factor(df_long$sample)\ndf_long$k &lt;- as.numeric(df_long$k)\ndf_long &lt;- df_long %&gt;% mutate(norm_kern = k/6)\ncomp &lt;- df_long %&gt;% group_by(grid) %&gt;% summarize(composite=sum(k)/x_data_n)\nggplot() +\n  geom_line(data = df_long, aes(x = grid, y = norm_kern, color = sample, group = sample), alpha = 0.75) +\n  geom_line(data = comp, aes(x = grid, y = composite), alpha = 0.5) +\n  xlim(min_val, max_val) +\n  geom_vline(xintercept = c(65, 67, 75, 79, 81, 91), linetype = \"dotted\", size = 0.2) +\n  theme_minimal() +\n  ggtitle(\"Kernels at different data points\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nx_data &lt;- sort(c(65, 75, 67, 79, 81, 91))\nx_data_sd &lt;- sd(x_data)\nx_data_iqr &lt;- IQR(x_data)\nx_data_n &lt;- length(x_data)\nh_silver &lt;- 0.9*x_data_n^(-1/5)*min(c(x_data_sd, x_data_iqr/1.34))\n\nxi &lt;- 65#x_data[1]\nh &lt;- h_silver\nmin_val &lt;- 40#min(x_data)-round(h)\nmax_val &lt;- 120#max(x_data)+round(h)\nx &lt;- seq(from = min_val, to = max_val, by = 1) # grid\n\n# Approximate CDF using left Riemann sums\ndx &lt;- diff(x)            # Spacing between x points (assumes equally spaced)\ndx &lt;- c(dx, tail(dx, 1))      # Pad to same length\n\ncdf_from_pdf &lt;- cumsum(comp[['composite']] * dx)\n\nplot(x, cdf_from_pdf, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDFs Comparison\")\n\nlines(x, estimated_cdf, lwd = 2, col = \"red\", lty = 2)\n\nlines(x, ecdf(x_data)(x), col = \"darkgreen\", lwd = 2, lty = 3)\nlegend(\"bottomright\", \n       legend = c(\"Numerical Integration\", \"Kernel CDF (pnorm)\", \"Empirical CDF\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), \n       lty = c(1, 2, 3), \n       lwd = 2)\n\n\n\n\n\n\n\n\n\nexp_log_x &lt;- exp(log_x)\nh &lt;- find_silver_bw(exp_log_x)\nestimated_cdf_sim &lt;- kde_cdf(get_grid(exp_log_x), exp_log_x, h)\nestimated_pdf_sim &lt;- kde_manual(get_grid(exp_log_x), exp_log_x, h)\nget_grid_exp_log_x &lt;- get_grid(exp_log_x)\n\npar(mfrow = c(1, 3))\nhist(exp(log_x), main = \"Histogram: exp(log_x)\", xlab = \"exp(log_x)\",\n     col = rgb(1, 0, 0, 0.5), xlim = range(c(exp(log_x), lnorm_x)), breaks = 30)\nplot(get_grid_exp_log_x, estimated_cdf_sim, type = \"l\", lwd = 1, col = \"red\",\n     xlab = \"exp(log_x)\", ylab = \"KDE-Estimated CDF\",\n     main = \"Estimated CDF\")\nplot(get_grid_exp_log_x, estimated_pdf_sim, type = \"l\", lwd = 1, col = \"red\",\n     xlab = \"exp(log_x)\", ylab = \"KDE-Estimated PDF\",\n     main = \"Estimated PDF\")\n\n\n\n\n\n\n\n\n\nlibrary(\"GoFKernel\")\n\nLoading required package: KernSmooth\n\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\n# kde_cdf_value &lt;- function(x, datapoints, bandwidth) {\n#   mean(pnorm((x - datapoints) / bandwidth))\n# }\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\n\n# kde_cdf_inverse &lt;- function(probabilities, datapoints, bandwidth, lower = -10, upper = 10) {\n#   sapply(probabilities, function(p) {\n#     root_func &lt;- function(x) kde_cdf_value(x, datapoints, bandwidth) - p\n#     uniroot(root_func, lower = lower, upper = upper)$root\n#   })\n# }\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\nget_upper_limit &lt;- function(p_data){\n  h &lt;- find_silver_bw(p_data)\n  \n  cdf_j &lt;- kde_cdf_function(p_data, h)\n  U_mat &lt;- sapply(p_data, cdf_j)\n\n  Y_mat &lt;- pmax(U_mat, 1 - U_mat)\n\n  npout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n  \n  return(npout$`1-sided.upper`)\n}\n\nget_tolerance_interval &lt;- function(p_data, maxk=max(k_j)){\n  h &lt;- find_silver_bw(p_data)\n  cdf_fun &lt;- kde_cdf_function(p_data,h)\n\n  inv_cdf &lt;- inverse(cdf_fun,lower=min(p_data) - 3 * h,upper=max(p_data) + 3 * h)\n  lower_bound &lt;- inv_cdf(1 - maxk)\n  upper_bound &lt;- inv_cdf(maxk)\n\n  return(c(lower_bound, upper_bound))\n}"
  },
  {
    "objectID": "initial_notes.html#dgp",
    "href": "initial_notes.html#dgp",
    "title": "",
    "section": "DGP",
    "text": "DGP\n\nset.seed(1029)\nuninorm &lt;- rlnorm(100,0,1)\n\n\nnpout = nptol.int(uninorm, alpha = 0.05, P = 0.95, side = 2, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\nnpout\n\n  alpha    P 2-sided.lower 2-sided.upper\n1  0.05 0.95     0.1768112      5.782058\n\n\n\n# STEP 1\nlibrary(\"compositions\")\n\nWelcome to compositions, a package for compositional data analysis.\nFind an intro with \"? compositions\"\n\n\n\nAttaching package: 'compositions'\n\n\nThe following objects are masked from 'package:stats':\n\n    anova, cor, cov, dist, var\n\n\nThe following object is masked from 'package:graphics':\n\n    segments\n\n\nThe following objects are masked from 'package:base':\n\n    %*%, norm, scale, scale.default\n\ngamma &lt;- 0.95\nalpha &lt;- 0.05\nn&lt;-500\nMyVar &lt;- matrix(c(\n1,0.95,0.95,\n0.95,1,0.95,\n0.95,0.95,1),byrow=TRUE,nrow=3)\nMyMean &lt;- c(0,0,0)\np &lt;- 3\n\nlibrary(\"MASS\")\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nset.seed(1724532)\ndatas &lt;- rlnorm.rplus(n,MyMean,MyVar)\n#datas &lt;- mvrnorm(n,MyMean,MyVar, tol = 1e-06, empirical = FALSE)\ncolnames(datas) &lt;- c(\"p_1\", \"p_2\", \"p_3\")\nmat &lt;- cbind(grid = rownames(datas), datas)\nrownames(datas) &lt;- sapply(1:n, function(i) paste0(\"n_\", sprintf(\"%03d\", i)))\n\nh &lt;- find_silver_bw(datas[,1])\nx &lt;- get_grid(datas[,1])\n\n#p1_outmat &lt;-sapply(datas[,1], function(xi) gaussian_kernel(xi = xi, x = x, h = h))\n#dx &lt;- diff(x)\n#dx &lt;- c(dx, tail(dx, 1))\n\nestimated_cdf &lt;- kde_cdf(x, datas[,1], h)\n\n#cdf_vals &lt;- cumsum(estimated_pdf(n ,p1_outmat) * dx) #ESTIMATED CDF\n\nplot(x, estimated_cdf, type = \"l\", lwd = 2,\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDF\")\n\n\n\n\n\n\n\nplot(x, estimated_cdf, type = \"p\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDF\")\n\n\n\n\n\n\n\n\n\nestimated_density &lt;- kde_manual(x, datas[,1], h)\nplot(x, estimated_density, type = \"l\", lwd = 2,\n      xlab = \"x\", ylab = \"Estimated CDF\",\n      main = \"Estimated PDF\")\n\n\n\n\n\n\n\nhist(estimated_density)\n\n\n\n\n\n\n\n\n\nget_upper_limit &lt;- function(p_data){\n  h &lt;- find_silver_bw(p_data)\n  #x &lt;- get_grid(p_data)\n  #estimated_cdf &lt;- kde_cdf(x, p_data, h)\n  #transformed_data &lt;- transform_data(p_data, estimated_cdf, x)\n  \n\n  U_mat &lt;- kde_cdf(p_data, p_data, h)\n  #U_mat &lt;- sapply(p_data, function(i) kde_cdf(i, p_data, h))\n  \n  # Step 4: Compute Y_ij = max{U_ij, 1 - U_ij}\n  Y_mat &lt;- pmax(U_mat, 1 - U_mat)\n  \n  #u&lt;-data.frame(original = transformed_data, complement = 1 - transformed_data)\n  #y1 &lt;- apply(u, 1, max)\n  npout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n  return(npout$`1-sided.upper`)\n}\n\nget_upper_limit(datas[,1])\n\n[1] 0.9661319\n\n\n\n# step 5\nk_j &lt;- apply(datas, 2, get_upper_limit)\n\n\nk_j\n\n      p_1       p_2       p_3 \n0.9661319 0.9661984 0.9677460 \n\n\n\n# step 6\nmax(k_j)\n\n[1] 0.967746\n\n\n\n# step 7\n\n#p_data &lt;- datas#[,1]\n\n# Custom CDF function\n\nget_tolerance_interval &lt;- function(p_data, maxk=max(k_j)){\n  h &lt;- find_silver_bw(p_data)\n  \n  #f.inv &lt;- inverse(function(i) kde_cdf(i, p_data, h),#kde_cdf(i, p_data, h),\n  #                 lower=-100,upper=100)\n  f.inv &lt;- inverse(kde_cdf_function(p_data,h),lower=-100,upper=100)\n  lower_bound &lt;- f.inv(1 - maxk)\n  upper_bound &lt;- f.inv(maxk)\n\n  #print(f.inv(1 - maxk))\n  return(c(lower_bound, upper_bound))\n}"
  },
  {
    "objectID": "initial_notes.html#abstract",
    "href": "initial_notes.html#abstract",
    "title": "Prerequisite Studies",
    "section": "Abstract",
    "text": "Abstract\n\nThis study aims to contribute to the existing knowledge on the computation of STIs by developing procedures to compute nonparametric STIs with accurate coverage.\n\nThe tolerance interval’s property of containing a specified proportion of sampled population values with high degree of confidence makes its computation meaningful.\n\nWhenever there are several populations, it may be of interest to perform simultaneous inference. For this reason, this study proposes methods to construct simultaneous tolerance intervals (STIs).\n\nFurthermore, since in many cases of practical applications the underlying distribution is unknown, the proposed STIs are derived under a nonparametric setting.\n\nThe proposed procedures andalgorithms are then assessed through performance metrics, such as estimated coverage probabilities and expected lengths.\n\nThe performance of the proposed methodology is also compared with the Bonferroni correction approach.\n\nThe proposed methods show accurate results, with coverage probabilities close to the nominal level.\n\nThe nonparametric STIs computed using the proposed methods are generally better than the ones obtained through a Bonferroni-corrected approach.\n\nA real-life application on the assessment of liver function is presented to illustrate the use of the proposed method."
  },
  {
    "objectID": "initial_notes.html#definitions",
    "href": "initial_notes.html#definitions",
    "title": "Prerequisite Studies",
    "section": "More Definitions",
    "text": "More Definitions\nSimultaneous inference\n- Whenever there are several populations, it is of interest to construct simultaneous statistical intervals from the sample data.\n- Having several simultaneous interval estimates that are naïvely constructed only increases the likelihood that at least one of such inferential statements does not hold true.\n- For this reason, this study considers integrating the criteria for simultaneous inference in computing statistical tolerance intervals so that the resulting coverage probabilities are close to the nominal confidence level.\nCurrent studies\n- The STIs that are available in literature rely on the assumption of normality. However, in many applications, this assumption is unwarranted. For instance, Wright and Royston (1999) point out that positive skewness is common among laboratory measurements, which are the bases for the construction of reference intervals. Thus, there is a need to establish STIs through nonparametric means.\n- While approaches to compute nonparametric tolerance regions are available in some existing studies [see, for instance Young and Mathew (2020) and Lucagbo (2021)], these studies do not use the criterion for simultaneous inference.\nKernel density estimation\n- The methodologies proposed in this study involve estimating the Cumulative Distribution Function (CDF) through kernel density estimation (KDE), which is a generalization of density estimation through histograms.\n- A weight function in histogram construction is simply replaced by another function, called the kernel function, denoted by \\(K(\\cdot)\\), which satisfies the ff conditions: \\(K(\\cdot) \\ge 0\\) and \\(\\int^{\\infty}_{-\\infty} K(x) dx = 1\\)\n- This function is often taken as a symmetric density function, such as a standard normal density function.\n- KDE, as a nonparametric statistical tool, estimates the unknown Probability Density Function (PDF) or CDF.\n- KDE Methodology: Suppose that \\(X_1, X_2, \\dots, X_n\\) is a random sample from some unknown PDF \\(f(x)\\) and CDF \\(F(x)\\). Then, KDE estimates the PDF as \\(\\hat{f}(x) = \\frac{1}{n} \\sum^n_{i=1} \\frac{1}{h} K \\left( \\frac{x-X_i}{h} \\right)\\) where\n- \\(h\\) is the bandwidth that acts as a smoothing parameter. Here, larger values of \\(h\\) result in smoother density estimates. As it gets smaller, the density gets rougher. Optimal choices for \\(h\\) include Silverman’s rule of thumb given by: \\(h = 0.9 n^{-\\frac{1}{5}} min \\left\\{ S, \\frac{IQR}{1.34}\\right\\}\\) where \\(S\\) is the standard deviation of the sample data and IQR is its interquartile range.\n- Common choices for \\(K(\\cdot)\\) include Gaussian kernel and the Epanichikov kernel.\n- This study uses the Gaussian kernel and estimtes the CDF by plugging the PDF of the standard normal distribution into \\(K(\\cdot)\\) and integrating the resulting quantity, giving \\(\\hat{F}(x)=\\frac{1}{n} \\sum^n_{i=1} \\Phi \\left( \\frac{x-X_i}{h} \\right)\\) where \\(\\hat{F}(x)\\) is the estimate of \\(F(x)\\) and \\(\\Phi\\) is the CDF of the standard normal distribution."
  },
  {
    "objectID": "initial_notes.html#methodology",
    "href": "initial_notes.html#methodology",
    "title": "Prerequisite Studies",
    "section": "Methodology",
    "text": "Methodology\n\nData layout & statistical criterion\nHere discusses the data to be used in computing STIs and the criterion to be followed.\nData: random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\) of multivariate measurements with dimension \\(p\\), coming from an unknown continuous distributions, say \\(F_X(\\cdot)\\). Each \\(\\mathbf{X}_i = \\left( X_{i1}, X_{i2}, \\dots, X_{ip}\\right)'\\) is a \\((p \\times 1)\\) column vector of measurements taken from the \\(i\\)th subject, \\(i = 1, 2, \\dots, n\\)\n\n\nTwo sided STIs\nObjective: We want to construct two-sided STIs for each component of \\(\\mathbf{X} = (X_1, X_2, \\dots, X_p)'\\). That is, we want to find a region of the form \\((c_1, d_1) \\times (c_2, d_2) \\times \\dots \\times (c_p, d_p)\\) where \\(c_j\\) and \\(d_j\\), \\(j = 1, 2, \\dots, p\\), are functions of the random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\) such that\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(c_j &lt; X_j &lt;d_j \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nThe quantities \\(\\gamma\\) and \\(\\left( 1-\\alpha\\right)\\) are between 0 and 1 and are referred to as the content probability and confidence level.\nCriterion: With a confidence level of \\(100(1-\\alpha)\\%\\), each marginal interval \\((c_j, d_j), j = 1, 2, \\dots, p\\), should have a content of at least \\(\\gamma\\).\n\n\nOne sided STI\nObjective: We want to construct 1-sided STIs.\nThese only have either lower limits only or upper limits only. That is, we want to find a region of the forms\n\\[\n(c_1, \\infty) \\times (c_2, \\infty) \\times \\dots, (c_p, \\infty)\n\\]\nand\n\\[\n(-\\infty, d_1) \\times (-\\infty, d_2) \\times \\dots (-\\infty, d_p)\n\\]\nwhere \\(c_j\\) and \\(d_j\\), \\(j=1, 2, \\dots p\\), are still functions of the random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\), such that\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(X_j &gt; c_j \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nand\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(X_j &lt; d_j \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nIdea:The process of obtaining simultaneous tolerance limits for the random vector \\(X = \\left( X_1, X_2, \\dots, X_p \\right)\\) with \\(p\\)-variate distribution \\(F_X(\\cdot)\\) can also be thought of as constructing simultaneous tolerance limits for the \\(p\\) populations represented by the \\(p\\) components of \\(\\mathbf{X}\\).\nNotes: Methodologies here make no assumptions about the correlation structure of \\(\\mathbf{X}\\) and so procedures also apply to case where \\(X_1, X_2, \\dots, X_p\\) are independent.\n\n\nMethod: Nonparametric two-sided STIs\nReference: Lucagbo (2021), who develops nonparametric rectangular tolerance regions. The main idea of the proposed methods is to transform each component in \\(\\mathbf{X}\\).\nLet \\(F_j(\\cdot)\\) be the CDF of the continuous random variable \\(X_j\\), the \\(j\\)th component of \\(\\mathbf{X}\\). Moreover, let \\(k \\equiv k(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n)\\) and \\(k' \\equiv k'(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n)\\) be some functions of the random sample. The \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\) confidence two-sided STIs for \\(\\mathbf{X}\\) are set to be of the following form:\n\\[\n\\prod^p_{j=1} \\left( F^{-1}_j (k'), F^{-1}_j(k) \\right)\n\\]\nFor each \\(j = 1, 2, \\dots, p\\), the end points of the \\(j\\)th interval are expressed as quantiles of \\(X_j\\). And aligned with the criterion, the values of \\(k\\) and \\(k'\\) are computed to satisfy the following condition:\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(F_j^{-1} \\left(k'\\right) &lt; X_j &lt; F_j^{-1} \\left(k\\right) \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nEquivalently,\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left( k' &lt; F \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nNote that \\(F_j(X_j), j = 1, 2, \\dots, p\\) are identically distributed as \\(U(0, 1)\\) random variables. Hence, the choice of common \\(k\\) and \\(k'\\) for the \\(p\\) components.\nMoreover, using the symmetry property of the uniform distribution, we can choose \\(k'\\) as \\(1-k\\) (ex: Since \\(k' &lt; k\\), let k = 0.7. Since dist is uniform, its max is 1. Hence, \\(k' = 1 - k = 1-0.7 = 0.3 \\rightarrow k'=0.3 &lt; k=0.7\\)). Thus, the criterion can be written as\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left( 1-k &lt; F_j \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\\\\n\\Rightarrow\n\\\\\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( 1-k &lt; F_j \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha \\\n\\\\\\text{take the min prob across j for w/c the prob that at least proportion gamma is within the interval}\n\\\\\n\\text{this is mostly correct/acceptable prob for all j than taking the max one}\n\\]\nSince the CDF \\(F_j(\\cdot), j = 1, 2, \\dots p\\) are unknown, we estimate them marginally via KDE, through the procedure in definitions section\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( 1-k &lt; \\hat{F}_j \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha\n\\]\nis also equivalent to\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( \\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\} \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha\n\\]\nExplanation:\n\\[\n1-k &lt; \\hat{F}_j \\left( X_j \\right) &lt; k\n\\\\\n\\equiv\n\\\\\n1-k &lt; \\hat{F}_j\\left( X_j \\right) \\ \\text{and} \\  \\hat{F}_j \\left( X_j \\right) &lt; k\n\\\\\n\\equiv\n\\\\\n1-\\hat{F}_j\\left( X_j \\right) &lt; k \\ \\text{and} \\  \\hat{F}_j \\left( X_j \\right) &lt; k\n\\\\\n\\text{this shows that both 1-F and F are less than k and is mathematically equal to saying}\n\\\\\n\\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\} &lt; k\n\\]\nDefine \\(Y_j = \\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\}\\) and disregard the “minimum” condition for now.\nNotice \\(k\\) is consistent with the definition of upper tolerance limit of the random variable \\(Y_j\\) (see: 1-sided STI). So, if we focus only on one component \\(Y_j\\), we could estimate \\(k\\) by the upper tolerance limit of \\(Y_j\\) to be computed from the data using this procedure (nonparametric tolerance intervals) by computing the nonparametric upper tolerance limit based on \\(Y_{1j}, Y_{2j}, \\dots, Y_{nj}\\), where \\(Y_{ij} = \\max \\left\\{ \\hat{F}_j(X_{ij}) , 1 - \\hat{F}_j(X_{ij})\\right\\}, i=1,2,\\dots,n\\).\nSince \\(p&gt;1\\) and to account for the “minimum” condition (This means that the weakest (i.e., smallest) probability across all \\(p\\) dimensions should still meet the threshold \\(\\gamma\\)), we obtain an estimate of \\(k\\), say \\(\\hat{k}\\), by taking the maximum of the marginal upper tolerance limits of the \\(Y_j\\)s.\n\\[\n\\hat{k} = \\max \\left\\{ k_1, k_1, \\dots, k_p\\right\\}\n\\]\nwhere \\(k_j\\) is a marginal upper tolerance limit for the probability condition in dimension \\(j\\).\nUsing this conservative estimate \\(\\hat{k}\\) ensures that the minimum of the inner probabilities as shown below is at least \\(\\gamma\\); i.e., in at least \\((1-\\alpha)\\) fraction of all datasets, the weakest probability among all \\(p\\) dimensions meets of exceeds \\(\\gamma\\)\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( \\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\}  &lt; \\hat{k} \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha\n\\]\nNow, going back to the original form:\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(F_j^{-1} (1-\\hat{k}) &lt; X_j &lt; F_j^{-1} (\\hat{k}) \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nWe say, the \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\) confidence two sided STIs for the data are given by\n\\[\n\\prod^p_{j=1}\\left( c_j, d_j \\right) = \\prod^p_{j=1}\\left( \\hat{F}_j^{-1}(1-\\hat{k}), \\hat{F}_j^{-1}(\\hat{k}) \\right)\n\\]\nAlgorithm A: Nonparametric two-sided STIs\n\nData are given by \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\overset{iid}{\\sim}F_{\\mathbf{X}}(\\mathbf{x})\\) where \\(\\mathbf{X}_i = \\left( X_{i1}, X_{i2}, \\dots, X_{ip}\\right)'\\) is a \\((p \\times 1)\\) vector of measurements taken from the \\(i\\)th subject, \\(i = 1, 2, \\dots, n\\). Also \\(F_{\\mathbf{X}}(\\mathbf{x})\\) is some unknown (continuous) distribution, where \\(F_j(x)\\) is the unknown distribution of the \\(j\\)th component\n\n\nget_sigma_matrices &lt;- function(p = 3) {\n  sigma_list &lt;- list(\n    Sigma_01  = 0.05 * diag(p) + 0.95 * matrix(1, p, p),\n    Sigma_02  = 0.5 * diag(p) + 0.5 * matrix(1, p, p),\n    Sigma_03  = 0.8 * diag(p) + 0.2 * matrix(1, p, p)\n  )\n  \n  if (p == 3) {\n    extra &lt;- list(\n      Sigma_04 = matrix(c(1, -0.95, -0.95,\n                          -0.95, 1, 0.95,\n                          -0.95, 0.95, 1), 3, 3, byrow = TRUE),\n\n      Sigma_05 = matrix(c(1, -0.5, -0.5,\n                          -0.5, 1, 0.5,\n                          -0.5, 0.5, 1), 3, 3, byrow = TRUE),\n\n      Sigma_06 = matrix(c(1, -0.2, -0.2,\n                          -0.2, 1, 0.2,\n                          -0.2, 0.2, 1), 3, 3, byrow = TRUE),\n\n      Sigma_07 = matrix(c(1, 0.9, 0.5,\n                          0.9, 1, 0.1,\n                          0.5, 0.1, 1), 3, 3, byrow = TRUE),\n\n      Sigma_08 = matrix(c(1, 0.5, 0.5,\n                          0.5, 1, 0.95,\n                          0.5, 0.95, 1), 3, 3, byrow = TRUE),\n\n      Sigma_09 = matrix(c(1, 0.2, 0.2,\n                          0.2, 1, 0.95,\n                          0.2, 0.95, 1), 3, 3, byrow = TRUE),\n\n      Sigma_10 = matrix(c(1, -0.5, -0.5,\n                          -0.5, 1, 0.95,\n                          -0.5, 0.95, 1), 3, 3, byrow = TRUE)\n    )\n    sigma_list &lt;- c(sigma_list, extra)\n  }\n  return(sigma_list)\n}\n\nsigma_p3 &lt;- get_sigma_matrices(p = 3)\nsigma_p2 &lt;- get_sigma_matrices(p = 2)\n\n\n#library(\"compositions\")\nlibrary(\"MASS\")\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ngamma &lt;- 0.95\nalpha &lt;- 0.05\nn &lt;- 500\np &lt;- 3\nMyVar &lt;- sigma_p3[1]$Sigma_01\nMyMean &lt;- rep(0,p)\n\n\n# STEP 1: Generate the random sample X1,X2,:::,Xn from the multivariate lognormal distribution\nset.seed(123) # for reproducibility\nlog_data &lt;- mvrnorm(n = n, mu = MyMean, Sigma = MyVar)\n\n# Convert to lognormal data by exponentiation\nX &lt;- exp(log_data)\n\ncolnames(X) &lt;- paste0(\"p_\", sprintf(\"%03d\", seq(p)))\n\n\nbase_colors &lt;- adjustcolor(rainbow(p), alpha.f = 0.25)\n# Limit to first p columns\ncolors_to_use &lt;- base_colors[1:p]\ncolumns_to_plot &lt;- X[, 1:p, drop = FALSE]\n\n\npar(mfrow = c(1, p+1))\n# Set up plot\nhist(columns_to_plot[,1], \n     col = colors_to_use[[1]], \n     xlim = range(X), \n     main = \"Overlaid Histograms\", \n     xlab = \"exp(log_x)\")\n\n# Add remaining histograms\nif (p &gt; 1) {\n  for (i in 2:p) {\n    hist(columns_to_plot[,i], \n         col = colors_to_use[[i]], \n         add = TRUE)\n  }\n}\n\n# Add legend\nlegend(\"topright\", \n       legend = colnames(columns_to_plot), \n       fill = unlist(colors_to_use))\n\n\nfor (p in seq(p)) {\n  hist((X[,p]), main = paste0(\"p_\", sprintf(\"%03d\", p)), xlab = \"exp(log_x)\", col = colors_to_use[[p]])\n}\n\n\n\n\n\n\n\n\n\nFor each \\(j = 1, 2, \\dots, p\\), obtain the estimate \\(\\hat{F}_j(x)\\) via KDE using \\(X_{1j}, X_{2j}, \\dots, X_{nj}\\).\n\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  return(function(x) mean(pnorm((x - datapoints) / bandwidth)))\n}\n\nFhat_Xj &lt;- lapply(seq_len(ncol(X)), function(j) {\n  kde_cdf_function(X[, j], find_silver_bw(X[, j]))\n})\n\nnames(Fhat_Xj) &lt;- paste0(\"Fhat_X\", seq_len(ncol(X)))\n\nFhat_Xj\n\n$Fhat_X1\nfunction (x) \nmean(pnorm((x - datapoints)/bandwidth))\n&lt;environment: 0x5641a22ee708&gt;\n\n$Fhat_X2\nfunction (x) \nmean(pnorm((x - datapoints)/bandwidth))\n&lt;bytecode: 0x5641a1ad5e40&gt;\n&lt;environment: 0x5641a28e1cf0&gt;\n\n$Fhat_X3\nfunction (x) \nmean(pnorm((x - datapoints)/bandwidth))\n&lt;bytecode: 0x5641a1ad5e40&gt;\n&lt;environment: 0x5641a1acf938&gt;\n\n\n\nCompute \\(U_{ij} = \\hat{F}_j(X_{ij}), i = 1, 2, \\dots, n\\) and \\(j = 1, 2, \\dots, p\\)\n\n\n# Step 3: compute estimates at each data point\nU_ij &lt;- lapply(seq_len(ncol(X)), function(j) {\n  sapply(X[, j], Fhat_Xj[[j]])\n})\n\nnames(U_ij) &lt;- paste0(\"U_i\", seq_len(ncol(X)))\n\n# Estimate CDFs and evaluate at data points\nfor (i in seq_len(p)) {\n  U_ij[[i]] &lt;- sapply(X[, i], Fhat_Xj[[i]])\n}\n\n# Overlay dot plots in one figure\nplot(X[,1], U_ij[[1]], col = colors_to_use[1], pch = 16,\n     xlab = \"exp(log_x)\", ylab = \"U_ij\",\n     main = \"Overlaid Estimated CDFs\", ylim = c(0, 1))\n\nfor (i in 2:p) {\n  points(X[, i], U_ij[[i]], col = colors_to_use[i], pch = 16)\n}\n\n# Optional legend\nlegend(\"bottomright\", legend = paste0(\"U_i\", 1:p),\n       col = colors_to_use, pch = 16)\n\n\n\n\n\n\n\n\nThis is an aside: Getting the estimated PDF from the estimated CDF.\n\np &lt;- ncol(X)\nbase_colors &lt;- adjustcolor(rainbow(p), alpha.f = 0.2)\ncolors_to_use &lt;- base_colors[1:p]\nbase_colors_kde &lt;- adjustcolor(rainbow(p), alpha.f = .75)\ncolors_to_use_kde &lt;- base_colors_kde[1:p]\n\np &lt;- 3  # number of individual plots\n\npar(mfrow = c(1, p + 1))\n\nbase_colors &lt;- adjustcolor(rainbow(p), alpha.f = 0.25)\ncolors_to_use &lt;- base_colors[1:p]\n\n# 1. Combined overlay histogram (spans entire first row)\nhist(X[, 1], col = colors_to_use[1], xlim = range(X), main = \"Overlaid Histograms\",\n     xlab = \"exp(log_x)\", freq = FALSE,breaks=seq(0,20,find_silver_bw(X[, 1])), border=NA)\nfor (i in 2:p) {\n  hist(X[, i], col = colors_to_use[i], add = TRUE, freq = FALSE,breaks=seq(0,20,find_silver_bw(X[, i])), border=NA)\n}\nlegend(\"topright\", legend = colnames(X)[1:p], fill = colors_to_use)\n\n# 2. Individual histograms with KDE overlays (second row)\nfor (j in seq_len(p)) {\n  # Calculate y-limits dynamically\n  h &lt;- hist(X[, j], plot = FALSE, freq = FALSE, warn.unused=FALSE)\n  grid &lt;- get_grid(X[, j])\n  kde_vals &lt;- kde_manual(grid, X[, j], find_silver_bw(X[, j]))\n  y_max &lt;- max(max(h$density), max(kde_vals))\n  \n  hist(X[, j], main = paste0(\"p_\", sprintf(\"%03d\", j)),breaks=seq(0,20,find_silver_bw(X[, j])),\n       xlab = \"exp(log_x)\", col = colors_to_use[j], freq = FALSE,border=NA)\n  lines(grid, kde_vals, col = adjustcolor(colors_to_use_kde[j], alpha.f = 1), lwd = 1)\n}\n\n\n\n\n\n\n\n\n\nCompute \\(Y_{ij} = \\max \\left\\{ U_{ij}, 1-U_{ij} \\right\\}, i = 1, 2, \\dots, n\\) and \\(j = 1, 2, \\dots, p\\)\n\n\nY_ij &lt;- lapply(U_ij, function(U) {\n  pmax(U, 1 - U)\n})\n\nnames(Y_ij) &lt;- paste0(\"Y_i\", seq_len(ncol(X)))\n\n\nFor each \\(j=1, 2, \\dots, p\\), compute the \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\)-confidence nonparametric upper tolerance limit based on \\(Y_{1j}, Y_{2j}, \\dots, Y_{nj}\\) using the methodology here (nonparametric tolerance intervals). Call this \\(k_j, j = 1, 2, \\dots, p\\)\n\n\nk_j &lt;- lapply(Y_ij, function(Y) {\n  return(\n    nptol.int(Y, \n              alpha = alpha, \n              P = gamma, \n              side = 1, \n              method = \"WILKS\",\n              upper = NULL,\n              lower = NULL)$`1-sided.upper`)\n})\nnames(k_j) &lt;- paste0(\"k_\", seq_len(ncol(X)))\nk_j\n\n$k_1\n[1] 0.9673421\n\n$k_2\n[1] 0.9651831\n\n$k_3\n[1] 0.9663632\n\n\n\nGet the maximum of the \\(k_j\\)s. Call this \\(\\hat{k} = \\max\\{k_1, k_2, \\dots, k_p\\}\\)\n\n\nk_hat &lt;- max(unlist(k_j))\nk_hat\n\n[1] 0.9673421\n\n\n\nFor each \\(j=1, 2, \\dots, p\\), compute \\(c_j =\\hat{F}_j^{-1}(1-\\hat{k})\\) and \\(d_j = \\hat{F}_j^{-1}(\\hat{k})\\)\nThe \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\)-confidence nonparametric two-sided STIs are given by \\((c_j, d_j), j =1, 2, \\dots, p\\)\n\n\ncj_dj &lt;- lapply(Fhat_Xj, function(Fhat) {\n  f.inv &lt;- inverse(Fhat, lower = -100, upper = 100)\n  setNames(c(f.inv(1 - k_hat), f.inv(k_hat)), c(\"c_j\", \"d_j\"))\n})\nnames(cj_dj) &lt;- paste0(\"Finv_i\", seq_len(ncol(X)))\ncj_dj\n\n$Finv_i1\n        c_j         d_j \n-0.04147688  5.57905136 \n\n$Finv_i2\n        c_j         d_j \n-0.02164378  5.47784699 \n\n$Finv_i3\n         c_j          d_j \n-0.004980373  5.764457577 \n\n\ncommon \\(\\hat{k}\\) is to be used across all estimated distributions \\(\\hat{F}_j\\)\n\n\nEvaluation: Nonparametric two-sided STIs\nThis is aimed to evaluate the performance of the proposed methodology.\n\nTo reflect the potential skewness of the observations, the data are generated from a multivariate lognormal distribution with mean vector \\(\\mathbf{0}\\) and covariance matrix \\(\\Sigma\\) in logarithmic scale.\nDGP will be done using “compositions” R package.\nThe confidence level and content are set at \\(1-\\alpha=0.95\\) and \\(\\gamma = 0.95\\).\nWe use the sample sizes \\(n =100,n=300\\), and \\(n=500\\) for the simulations.\nThe covariance matrix \\(\\Sigma\\) is also varied to represent different correlation structures. We run simulations for the values of \\(\\Sigma\\) of the form \\((1-\\rho)\\mathbf{I}_p + \\rho1_p1_p'\\). Here \\(\\mathbf{I}_p\\) is the \\((p \\times p)\\) identity matrix and \\(1_p\\) is a \\((p \\times 1)\\) column vector of 1s. This choice of \\(\\Sigma\\) is a correlation matrix that assumes an exchangeable correlation structure. We run simulations for \\(p=2\\) and \\(p=3\\) for the ff covariances in sigma_p2/sigma_p3\n\nThe choices for the covariance matrices \\(\\Sigma_1\\) to \\(\\Sigma_{10}\\) are made so that they include as wide variety as possible of the correlation values in terms of both size and direction.\nAside: An exchangeable variance-covariance matrix is a structured covariance matrix where all off-diagonal elements are equal, meaning that each pair of variables has the same covariance.\nUnivariate case\nFurthermore, although this study is primarily concerned with multivariate measurements, we shall also investigate the performance of the KDE-based method in the univariate case and compare it with the performance of the standard approach to compute nonparametric tolerance intervals, which is that of Wilks (1941).\n\nFor the univariate case, we generate the simulated samples from the univariate lognormal distribution with log-scale mean 0.\nThe log-scale variances used are \\(\\sigma^2 = 0.95, \\sigma^2=0.5\\), and \\(\\sigma^2 = 0.20\\).\n\nNote that since the proposed methods apply mainly to the multivariate case, in the univariate care we refrain from saying “proposed method”, and instead say “KDE-based method” even though the univariate case is a special case of the multivariate case.\nThe Gaussian kernel is used in obtaining the KDE. Moreover, the bandwidth \\(h\\) used in the simulations is Silverman’s rule of thumb, which is the preferred bandwidth for the Gaussian kernel. Some numerical simulations (not reported here) have also been performed using a different bandwidth choice and the results are very similar.\nIn Step 7 of Algorithm A, the inverse of the estimated distribution functions are calculated using the GoFKernel R package. In addition, nonparametric tolerance limits described are implemented in R through the tolerance R package. To estimate the coverage probability, this study uses \\(M=5000\\) simulated samples in running Monte Carlo simulations.\nTo evaluate, look at the estimated coverage probabilities. The desired or nominal confidence level is 0.95. Also compute the expected lengths of the tolerance intervals for each component for algorithm A.\nAlgorithm: Performance evaluation of the Nonparametric two-sided STIs obtained from Algo A through estimated coverage probability and expected lengths\n\nGenerate the random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\) from the multivariate lognormal distribution with log-scale mean \\(\\boldsymbol{\\mu}\\) and given log-scale covariance matrix \\(\\Sigma\\). Let \\(\\mu_j\\) be the \\(j\\)th element of \\(\\boldsymbol{\\mu}\\) and \\(\\sigma^2_j\\) be the \\(j\\)th diagonal element of \\(\\Sigma\\).\nCompute the 2-sided tolerance limits \\(c_j\\) and \\(d_j\\), \\(j = 1, 2, \\dots, p\\) using the procedure in Algo A.\nCompute \\(\\min_{1 \\leq j \\leq p} \\left\\{ \\Phi \\left( \\log d_j; \\mu_j, \\sigma^2_j\\right) - \\Phi \\left( \\log c_j; \\mu_j, \\sigma^2_j\\right) \\right\\}\\) where \\(\\Phi \\left( x; \\mu_j, \\sigma^2_j\\right)\\) denotes the CDF of the \\(N \\left( \\mu_j, \\sigma_j^2 \\right)\\)\n\n\nget_TI_info &lt;- function(tol_int, mu, var) {\n  info &lt;- lapply(seq_along(tol_int), function(j) {\n    lower &lt;- tol_int[[j]][\"c_j\"]\n    upper &lt;- tol_int[[j]][\"d_j\"]\n    \n    # Coverage on CDF scale (difference of estimated quantiles)\n    coverage &lt;- as.numeric(upper - lower)\n    \n    # TI length on original/lognormal scale\n    ti_length &lt;- plnorm(upper, meanlog = mu[j], sdlog = sqrt(var[j, j])) -\n                 plnorm(lower, meanlog = mu[j], sdlog = sqrt(var[j, j]))\n\n    # Return named vector\n    setNames(c(ti_length, coverage), c(\"TI_length\", \"Coverage\"))\n  })\n\n  # Name each component list\n  names(info) &lt;- paste0(\"p_\", seq_along(tol_int))\n  return(info)\n}\n\nget_TI_info(cj_dj, MyMean, MyVar)\n\n$p_1\nTI_length  Coverage \n0.9571945 5.6205282 \n\n$p_2\nTI_length  Coverage \n0.9555015 5.4994908 \n\n$p_3\nTI_length  Coverage \n0.9600882 5.7694379"
  },
  {
    "objectID": "initial_notes.html#basics",
    "href": "initial_notes.html#basics",
    "title": "",
    "section": "Basics",
    "text": "Basics\n\nTolerance intervals\nParametric and univariate\nGiven a sample size \\(n\\), sample mean \\(\\bar{x}\\), and sample standard deviation \\(s\\), the two-sided normal tolerance interval is:\n\\[\n\\bar{x} \\pm k \\cdot s\n\\]\nwhere \\(k\\) is a factor based on:\n\\(n\\), confidence level, \\((1-\\alpha)\\), and population converage \\(\\gamma\\)\nAssumptions:\n\nnormally distributed data\nrandom sampling\nif unknown mean and variance: We estimate both the mean and the standard deviation from the data. Why it matters: That’s what distinguishes this from a theoretical tolerance interval based on known parameters. It affects the derivation of \\(\\mathcal{k}\\)\nmoderate sample size: What it means: While larger n is better, a sample of 20 is acceptable for parametric TIs, especially if the distribution is normal. Why it matters: Small samples lead to more variability in s, making the TI wider or less reliable.\n\nExample\nCase for unknown \\(\\mu\\) and \\(\\sigma\\): Estimate the mean \\(\\bar{x}\\) and standard deviation, \\(\\sigma\\)\n\n#set.seed(777)\n#v = rnorm(20, mean = 10, sd = 0.25)\n\nhist(mdat$y)\n\n\n\n\n\n\n\n# Q-Q plot\nqqnorm(mdat$y)\nqqline(mdat$y, col = \"blue\")\n\n\n\n\n\n\n\nshapiro.test(mdat$y)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mdat$y\nW = 0.97084, p-value = 0.6663\n\n#W: the test statistic\n#p-value &gt; 0.05 → data is likely normally distributed\n#p-value ≤ 0.05 → data likely not normally distributed\n\nAside:\nIn R, to find the smallest value \\(m\\) such that \\(P(Y \\le m-1) \\ge 1-\\alpha\\) for a binomial distribution \\(Y \\sim \\text{Binomial}(n, \\gamma)\\), you can use the cumulative distribution function (CDF) for the Binomial distribution, which is available in the pbinom() function.\nThe condition \\(P(Y \\le m-1) \\ge 1-\\alpha\\) suggests that you are looking for the quantile \\(m\\) such that the cumulative probability is at least \\(1-\\alpha\\). :::"
  },
  {
    "objectID": "initial_notes.html#sim",
    "href": "initial_notes.html#sim",
    "title": "",
    "section": "SIM",
    "text": "SIM\n\n# STEP 1\nlibrary(\"compositions\")\nlibrary(\"MASS\")\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ngamma &lt;- 0.95\nalpha &lt;- 0.05\nn &lt;- 500\nMyVar &lt;- matrix(c(\n1,0.95,0.95,\n0.95,1,0.95,\n0.95,0.95,1),byrow=TRUE,nrow=3)\nMyMean &lt;- c(0,0,0)\np &lt;- 3\n\nset.seed(7778)\n\n# STEP 1: Generate the random sample X1,X2,:::,Xn from the multivariate lognormal distribution\nset.seed(123) # for reproducibility\nlog_data &lt;- mvrnorm(n = n, mu = MyMean, Sigma = MyVar)\n\n# Convert to lognormal data by exponentiation\ndatas &lt;- exp(log_data)\n\n# Optional: Treat the data as compositional (e.g., close parts to sum to 1)\n# For example, using acomp() from compositions package\n#comp_data &lt;- acomp(lognormal_data)\n\n\nX&lt;-datas\nhist(X[,1], col = rgb(1, 0, 0, 0.4), xlim = range(datas), main = \"Overlaid Histograms\", xlab = \"Values\")\nhist(X[,2], col = rgb(0, 1, 0, 0.4), add = TRUE)\nhist(X[,3], col = rgb(0, 0, 1, 0.4), add = TRUE)\nlegend(\"topright\", legend = c(\"1\", \"2\", \"3\"),\n       fill = c(rgb(1, 0, 0, 0.4), rgb(0, 1, 0, 0.4), rgb(0, 0, 1, 0.4)))\n\n\n\n\n\n\n\ncolnames(X) &lt;- c(\"p_1\", \"p_2\", \"p_3\")\nmat &lt;- cbind(grid = rownames(X), X)\nrownames(X) &lt;- sapply(1:n, function(i) paste0(\"n_\", sprintf(\"%03d\", i)))\n\n\npar(mfrow = c(1, 3))\nhist((X[,1]), main = \"1\", xlab = \"LOG Values\", col = \"skyblue\")\nhist((X[,2]), main = \"2\", xlab = \"LOG Values\", col = \"salmon\")\nhist((X[,3]), main = \"3\", xlab = \"LOG Values\", col = \"lightgreen\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\nk_j &lt;- apply(X, 2, get_upper_limit)\nk_j\n\n      p_1       p_2       p_3 \n0.9673421 0.9651831 0.9663632 \n\nmaxkj &lt;- max(k_j)\nmax(k_j)\n\n[1] 0.9673421\n\n\n\ntols &lt;- apply(X, 2, function(s) get_tolerance_interval(s,maxk=maxkj))\ntols\n\n             p_1         p_2          p_3\n[1,] -0.04147051 -0.02162749 -0.004976622\n[2,]  5.57903556  5.47783908  5.764448735\n\n\n\nsummary(datas)\n\n       V1                V2                 V3          \n Min.   : 0.0411   Min.   : 0.04218   Min.   : 0.04067  \n 1st Qu.: 0.5153   1st Qu.: 0.50900   1st Qu.: 0.52154  \n Median : 1.0054   Median : 0.95882   Median : 0.99798  \n Mean   : 1.5296   Mean   : 1.53999   Mean   : 1.51669  \n 3rd Qu.: 1.9128   3rd Qu.: 1.82951   3rd Qu.: 1.78991  \n Max.   :16.4272   Max.   :14.63768   Max.   :17.19995  \n\n\n\np_data &lt;- datas[,1]\nh &lt;- find_silver_bw(p_data)\n\n#f.inv &lt;- inverse(function(i) kde_cdf(i, p_data, h),#kde_cdf(i, p_data, h),\n#                 lower=-100,upper=100)\nf.inv &lt;- inverse(kde_cdf_function(p_data,h),lower=-100,upper=100)\nlower_bound &lt;- f.inv(1 - maxkj)\nupper_bound &lt;- f.inv(maxkj)\nlower_bound;upper_bound \n\n[1] -0.04147688\n\n\n[1] 5.579051\n\n\n\nstep3 &lt;- function(i){\n  return(plnorm(tols[2,i],MyMean[i],MyVar[i,i]) - plnorm(tols[1,i],MyMean[i],MyVar[i,i]))\n}\n\nstep4 &lt;- function(i){\n  return(tols[2,i] - tols[1,i])\n}\n\n\nI &lt;- min(sapply(1:3, step3)) #&gt;= gamma\nI\n\n[1] 0.9555013\n\n#Lj &lt;- sapply(1:3, step4)\n\n\n# Bonferoni\n\nbonferonni &lt;- function(p_data) {\n  \n  if (length(p_data) &gt;= 119) {\n    to &lt;- nptol.int(p_data, alpha = alpha/3, P = gamma, side = 2, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n\n    lower_bound &lt;- to$`2-sided.lower`\n    upper_bound &lt;- to$`2-sided.upper`\n      \n  }\n  \n  else {\n    lower_bound &lt;- min(p_data)\n    upper_bound &lt;- max(p_data)\n  }\n\n  return(c(lower_bound, upper_bound))\n}\n\n\nrun_simulation &lt;- function(n, MyMean, MyVar, gamma = 0.90, n_sim = 5000) {\n  results &lt;- replicate(n_sim, {\n    log_data &lt;- MASS::mvrnorm(n = n, mu = MyMean, Sigma = MyVar)\n    datas &lt;- exp(log_data)\n    colnames(datas) &lt;- paste0(\"p_\", seq_along(MyMean))\n    rownames(datas) &lt;- sprintf(\"n_%03d\", seq_len(n))\n    \n    k_j &lt;- apply(datas, 2, get_upper_limit)\n    tols &lt;- apply(datas, 2, function(s) get_tolerance_interval(s, maxk = max(k_j)))\n    bonf &lt;- apply(datas, 2, bonferonni)\n\n    step3 &lt;- function(i){\n      plnorm(tols[2, i], mean = MyMean[i], sd = sqrt(MyVar[i, i])) -\n        plnorm(tols[1, i], mean = MyMean[i], sd = sqrt(MyVar[i, i]))\n    }\n    \n    step4 &lt;- function(i){\n      tols[2, i] - tols[1, i]\n    }\n\n    step3b &lt;- function(i){\n      plnorm(bonf[2, i], mean = MyMean[i], sd = sqrt(MyVar[i, i])) -\n        plnorm(bonf[1, i], mean = MyMean[i], sd = sqrt(MyVar[i, i]))\n    }\n    \n    step4b &lt;- function(i){\n      bonf[2, i] - bonf[1, i]\n    }\n\n    I &lt;- min(sapply(seq_along(MyMean), step3)) &gt;= gamma\n    Lj &lt;- sapply(seq_along(MyMean), step4)\n    \n    Ib &lt;- min(sapply(seq_along(MyMean), step3b)) &gt;= gamma\n    Ljb &lt;- sapply(seq_along(MyMean), step4b)\n\n    list(Lj = Lj, I = I, Ljb = Ljb, Ib = Ib)\n  }, simplify = FALSE)\n\n  # Collect and summarize results\n  I_vals &lt;- sapply(results, function(res) res$I)\n  Lj_vals &lt;- t(sapply(results, function(res) res$Lj))\n  Ib_vals &lt;- sapply(results, function(res) res$Ib)\n  Ljb_vals &lt;- t(sapply(results, function(res) res$Ljb))\n\n  list(\n    mean_I = mean(I_vals),\n    colMeans_Lj = colMeans(Lj_vals),\n    mean_Ib = mean(Ib_vals),\n    colMeans_Ljb = colMeans(Ljb_vals),\n    raw_results = results # optional if you want full access later\n  )\n}\n\n\np &lt;- 3  # or any integer\n\n# Sigma_2 = 0.5 * I_p + 0.5 * 1_p 1_p'\nSigma_2 &lt;- 0.5 * diag(p) + 0.5 * matrix(1, p, p)\n\n# Sigma_3 = 0.8 * I_p + 0.2 * 1_p 1_p'\nSigma_3 &lt;- 0.8 * diag(p) + 0.2 * matrix(1, p, p)\n\n\n# Sigma_4\nSigma_4 &lt;- matrix(c(\n  1,   -0.95, -0.95,\n -0.95, 1,     0.95,\n -0.95, 0.95,  1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_5\nSigma_5 &lt;- matrix(c(\n  1,  -0.5, -0.5,\n -0.5,  1,   0.5,\n -0.5,  0.5, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_6\nSigma_6 &lt;- matrix(c(\n  1,  -0.2, -0.2,\n -0.2,  1,   0.2,\n -0.2,  0.2, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_7\nSigma_7 &lt;- matrix(c(\n  1,   0.9,  0.5,\n  0.9, 1,    0.1,\n  0.5, 0.1,  1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_8\nSigma_8 &lt;- matrix(c(\n  1,   0.5,  0.5,\n  0.5, 1,    0.95,\n  0.5, 0.95, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_9\nSigma_9 &lt;- matrix(c(\n  1,   0.2,  0.2,\n  0.2, 1,    0.95,\n  0.2, 0.95, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_10\nSigma_10 &lt;- matrix(c(\n  1,   -0.5, -0.5,\n -0.5,  1,    0.95,\n -0.5,  0.95, 1\n), nrow = 3, byrow = TRUE)\n\n\n# library(foreach)\n# library(doParallel)\n# # Detect available cores and register parallel backend\n# n_cores &lt;- parallel::detectCores() - 1\n# cl &lt;- makeCluster(n_cores)\n# registerDoParallel(cl)\n\n\nget_sigma_matrices &lt;- function(p = 3) {\n  list(\n    Sigma_1 = matrix(c(\n      1,0.95,0.95,\n      0.95,1,0.95,\n      0.95,0.95,1),byrow=TRUE,nrow=3),\n    Sigma_2  = 0.5 * diag(p) + 0.5 * matrix(1, p, p),\n    Sigma_3  = 0.8 * diag(p) + 0.2 * matrix(1, p, p),\n    Sigma_4  = matrix(c(1, -0.95, -0.95, -0.95, 1, 0.95, -0.95, 0.95, 1), 3, 3, byrow=TRUE),\n    Sigma_5  = matrix(c(1, -0.5, -0.5, -0.5, 1, 0.5, -0.5, 0.5, 1), 3, 3, byrow=TRUE),\n    Sigma_6  = matrix(c(1, -0.2, -0.2, -0.2, 1, 0.2, -0.2, 0.2, 1), 3, 3, byrow=TRUE),\n    Sigma_7  = matrix(c(1, 0.9, 0.5, 0.9, 1, 0.1, 0.5, 0.1, 1), 3, 3, byrow=TRUE),\n    Sigma_8  = matrix(c(1, 0.5, 0.5, 0.5, 1, 0.95, 0.5, 0.95, 1), 3, 3, byrow=TRUE),\n    Sigma_9  = matrix(c(1, 0.2, 0.2, 0.2, 1, 0.95, 0.2, 0.95, 1), 3, 3, byrow=TRUE),\n    Sigma_10 = matrix(c(1, -0.5, -0.5, -0.5, 1, 0.95, -0.5, 0.95, 1), 3, 3, byrow=TRUE)\n  )\n}\n\n\n# MyMean &lt;- rep(0, 3)\n# gamma &lt;- 0.95\n# sigma_list &lt;- get_sigma_matrices()\n# n_values &lt;- c(100, 300, 500)\n# \n# results_df &lt;- foreach(n = n_values, .combine = rbind, .packages = c(\"MASS\", \"GoFKernel\", \"tolerance\")) %:%\n#   foreach(sigma_name = names(sigma_list), .combine = rbind) %dopar% {\n#     MyVar &lt;- sigma_list[[sigma_name]]\n#     res &lt;- run_simulation(n, MyMean, MyVar, gamma, n_sim = 5000)\n# \n#     # Create data.frames for proposed and bonferroni methods\n#     proposed_row &lt;- data.frame(\n#       n = n,\n#       gamma = gamma,\n#       sigma = sigma_name,\n#       method = \"proposed\",\n#       mean_I = res$mean_I,\n#       Lj_1 = res$colMeans_Lj[1],\n#       Lj_2 = res$colMeans_Lj[2],\n#       Lj_3 = res$colMeans_Lj[3]\n#     )\n# \n#     bonf_row &lt;- data.frame(\n#       n = n,\n#       gamma = gamma,\n#       sigma = sigma_name,\n#       method = \"bonferroni\",\n#       mean_I = res$mean_Ib,\n#       Lj_1 = res$colMeans_Ljb[1],\n#       Lj_2 = res$colMeans_Ljb[2],\n#       Lj_3 = res$colMeans_Ljb[3]\n#     )\n# \n#     rbind(proposed_row, bonf_row)\n#   }\n\n#stopCluster(cl)\n\n\nload(\"results_df.RData\")\nresults_df\n\n         n gamma    sigma     method mean_I      Lj_1      Lj_2      Lj_3\np_1    100  0.95  Sigma_1   proposed 0.9414  9.432202  9.439163  9.436803\np_11   100  0.95  Sigma_1 bonferroni 0.9218 13.351928 13.385767 13.424624\np_12   100  0.95  Sigma_2   proposed 0.9174  9.393701  9.448329  9.468494\np_111  100  0.95  Sigma_2 bonferroni 0.8936 13.300489 13.375498 13.469624\np_13   100  0.95  Sigma_3   proposed 0.9118  9.417383  9.355093  9.488714\np_112  100  0.95  Sigma_3 bonferroni 0.8952 13.514072 13.539249 13.607965\np_14   100  0.95  Sigma_4   proposed 0.9214  9.442166  9.428619  9.410113\np_113  100  0.95  Sigma_4 bonferroni 0.9150 13.397610 13.483725 13.504636\np_15   100  0.95  Sigma_5   proposed 0.9172  9.508856  9.453177  9.408880\np_114  100  0.95  Sigma_5 bonferroni 0.8932 13.547451 13.607561 13.320073\np_16   100  0.95  Sigma_6   proposed 0.9182  9.437318  9.381537  9.388663\np_115  100  0.95  Sigma_6 bonferroni 0.8864 13.457183 13.359136 13.412068\np_17   100  0.95  Sigma_7   proposed 0.9286  9.479048  9.490746  9.487169\np_116  100  0.95  Sigma_7 bonferroni 0.8978 13.601785 13.501508 13.512159\np_18   100  0.95  Sigma_8   proposed 0.9334  9.440805  9.425150  9.420785\np_117  100  0.95  Sigma_8 bonferroni 0.9092 13.525467 13.447443 13.531976\np_19   100  0.95  Sigma_9   proposed 0.9244  9.459112  9.430767  9.371577\np_118  100  0.95  Sigma_9 bonferroni 0.8996 13.531866 13.370461 13.400610\np_110  100  0.95 Sigma_10   proposed 0.9292  9.498593  9.389817  9.433560\np_119  100  0.95 Sigma_10 bonferroni 0.9038 13.670953 13.538172 13.511211\np_120  300  0.95  Sigma_1   proposed 0.9486  7.008602  7.008663  7.006469\np_1110 300  0.95  Sigma_1 bonferroni 0.9658  9.705779  9.664276  9.671580\np_121  300  0.95  Sigma_2   proposed 0.9364  6.995565  6.981772  7.018299\np_1111 300  0.95  Sigma_2 bonferroni 0.9526  9.690747  9.666004  9.669688\np_131  300  0.95  Sigma_3   proposed 0.9322  6.989780  6.987645  6.997938\np_1121 300  0.95  Sigma_3 bonferroni 0.9474  9.691351  9.661174  9.701256\np_141  300  0.95  Sigma_4   proposed 0.9370  6.984812  6.997522  6.995045\np_1131 300  0.95  Sigma_4 bonferroni 0.9592  9.652083  9.671698  9.654119\np_151  300  0.95  Sigma_5   proposed 0.9356  7.012250  7.005759  6.988130\np_1141 300  0.95  Sigma_5 bonferroni 0.9532  9.702445  9.639717  9.643229\np_161  300  0.95  Sigma_6   proposed 0.9394  7.001247  7.002618  6.997710\np_1151 300  0.95  Sigma_6 bonferroni 0.9552  9.672447  9.693714  9.681670\np_171  300  0.95  Sigma_7   proposed 0.9376  6.984042  7.002154  6.979538\np_1161 300  0.95  Sigma_7 bonferroni 0.9538  9.630017  9.697683  9.676495\np_181  300  0.95  Sigma_8   proposed 0.9388  7.000826  7.021135  7.006591\np_1171 300  0.95  Sigma_8 bonferroni 0.9594  9.668751  9.702430  9.664471\np_191  300  0.95  Sigma_9   proposed 0.9394  6.984444  6.981461  6.985158\np_1181 300  0.95  Sigma_9 bonferroni 0.9632  9.677808  9.666808  9.650490\np_1101 300  0.95 Sigma_10   proposed 0.9428  6.979555  7.000865  6.994919\np_1191 300  0.95 Sigma_10 bonferroni 0.9594  9.628663  9.670059  9.691317\np_122  500  0.95  Sigma_1   proposed 0.9478  6.442213  6.431800  6.431035\np_1112 500  0.95  Sigma_1 bonferroni 0.9872  9.220788  9.215836  9.216843\np_123  500  0.95  Sigma_2   proposed 0.9356  6.451498  6.436058  6.421873\np_1113 500  0.95  Sigma_2 bonferroni 0.9816  9.229456  9.256825  9.177914\np_132  500  0.95  Sigma_3   proposed 0.9344  6.442507  6.442941  6.479097\np_1122 500  0.95  Sigma_3 bonferroni 0.9846  9.204266  9.240803  9.268556\np_142  500  0.95  Sigma_4   proposed 0.9436  6.444022  6.450989  6.451312\np_1132 500  0.95  Sigma_4 bonferroni 0.9888  9.224976  9.242345  9.259192\np_152  500  0.95  Sigma_5   proposed 0.9332  6.436432  6.439831  6.426897\np_1142 500  0.95  Sigma_5 bonferroni 0.9854  9.187284  9.222307  9.224303\np_162  500  0.95  Sigma_6   proposed 0.9304  6.436484  6.419339  6.430151\np_1152 500  0.95  Sigma_6 bonferroni 0.9810  9.230408  9.183674  9.186471\np_172  500  0.95  Sigma_7   proposed 0.9426  6.458407  6.445668  6.435083\np_1162 500  0.95  Sigma_7 bonferroni 0.9842  9.260465  9.229504  9.203002\np_182  500  0.95  Sigma_8   proposed 0.9404  6.444871  6.446294  6.442985\np_1172 500  0.95  Sigma_8 bonferroni 0.9834  9.235269  9.234818  9.233284\np_192  500  0.95  Sigma_9   proposed 0.9390  6.423952  6.436780  6.442117\np_1182 500  0.95  Sigma_9 bonferroni 0.9874  9.228690  9.245564  9.252124\np_1102 500  0.95 Sigma_10   proposed 0.9396  6.444146  6.430918  6.436981\np_1192 500  0.95 Sigma_10 bonferroni 0.9850  9.228618  9.210389  9.209842\n\n\n\nmk &lt;- get_upper_limit(uninorm)\nget_tolerance_interval(uninorm,mk)\n\n[1] -0.2003685  4.9905541\n\nhist(log(uninorm))\n\n\n\n\n\n\n\nnptol.int(uninorm, alpha = 0.05, P = 0.95, side = 2, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n\n  alpha    P 2-sided.lower 2-sided.upper\n1  0.05 0.95     0.1768112      5.782058\n\nhist((uninorm))\n\n\n\n\n\n\n\n\n\nhist(kde_cdf(datas[,1], datas[,1], h), probability = TRUE, \n     main = \"Histogram of PIT-transformed values\",\n     xlab = \"PIT values\")\nabline(h = 1, col = \"red\", lwd = 2) \n\n\n\n\n\n\n\n\n\n# generated from a lognormal dist\nhist(datas[,1])\n\n\n\n\n\n\n\n\n\n# taking the log of a variate from lognormal dist makes it normally distributed\nhist(log(datas[,1]))\n\n\n\n\n\n\n\n\nProbability Integral Transform"
  },
  {
    "objectID": "initial_notes.html#nonparametric-simultaneous-tolerance-intervals-for-small-dimensions-based-on-kernel-density-estimates",
    "href": "initial_notes.html#nonparametric-simultaneous-tolerance-intervals-for-small-dimensions-based-on-kernel-density-estimates",
    "title": "",
    "section": "Nonparametric simultaneous tolerance intervals for small dimensions based on kernel density estimates",
    "text": "Nonparametric simultaneous tolerance intervals for small dimensions based on kernel density estimates\n\nAbstract\n\nThis study aims to contribute to the existing knowledge on the computation of STIs by developing procedures to compute nonparametric STIs with accurate coverage.\n\nThe tolerance interval’s property of containing a specified proportion of sampled population values with high degree of confidence makes its computation meaningful.\n\nWhenever there are several populations, it may be of interest to perform simultaneous inference. For this reason, this study proposes methods to construct simultaneous tolerance intervals (STIs).\n\nFurthermore, since in many cases of practical applications the underlying distribution is unknown, the proposed STIs are derived under a nonparametric setting.\n\nThe proposed procedures andalgorithms are then assessed through performance metrics, such as estimated coverage probabilities and expected lengths.\n\nThe performance of the proposed methodology is also compared with the Bonferroni correction approach.\n\nThe proposed methods show accurate results, with coverage probabilities close to the nominal level.\n\nThe nonparametric STIs computed using the proposed methods are generally better than the ones obtained through a Bonferroni-corrected approach.\n\nA real-life application on the assessment of liver function is presented to illustrate the use of the proposed method.\n\n\n\nMore Definitions\nSimultaneous inference\n- Whenever there are several populations, it is of interest to construct simultaneous statistical intervals from the sample data.\n- Having several simultaneous interval estimates that are naïvely constructed only increases the likelihood that at least one of such inferential statements does not hold true.\n- For this reason, this study considers integrating the criteria for simultaneous inference in computing statistical tolerance intervals so that the resulting coverage probabilities are close to the nominal confidence level.\nCurrent studies\n- The STIs that are available in literature rely on the assumption of normality. However, in many applications, this assumption is unwarranted. For instance, Wright and Royston (1999) point out that positive skewness is common among laboratory measurements, which are the bases for the construction of reference intervals. Thus, there is a need to establish STIs through nonparametric means.\n- While approaches to compute nonparametric tolerance regions are available in some existing studies [see, for instance Young and Mathew (2020) and Lucagbo (2021)], these studies do not use the criterion for simultaneous inference.\nKernel density estimation\n- The methodologies proposed in this study involve estimating the Cumulative Distribution Function (CDF) through kernel density estimation (KDE), which is a generalization of density estimation through histograms.\n- A weight function in histogram construction is simply replaced by another function, called the kernel function, denoted by \\(K(\\cdot)\\), which satisfies the ff conditions: \\(K(\\cdot) \\ge 0\\) and \\(\\int^{\\infty}_{-\\infty} K(x) dx = 1\\)\n- This function is often taken as a symmetric density function, such as a standard normal density function.\n- KDE, as a nonparametric statistical tool, estimates the unknown Probability Density Function (PDF) or CDF.\n- KDE Methodology: Suppose that \\(X_1, X_2, \\dots, X_n\\) is a random sample from some unknown PDF \\(f(x)\\) and CDF \\(F(x)\\). Then, KDE estimates the PDF as \\(\\hat{f}(x) = \\frac{1}{n} \\sum^n_{i=1} \\frac{1}{h} K \\left( \\frac{x-X_i}{h} \\right)\\) where\n- \\(h\\) is the bandwidth that acts as a smoothing parameter. Here, larger values of \\(h\\) result in smoother density estimates. As it gets smaller, the density gets rougher. Optimal choices for \\(h\\) include Silverman’s rule of thumb given by: \\(h = 0.9 n^{-\\frac{1}{5}} min \\left\\{ S, \\frac{IQR}{1.34}\\right\\}\\) where \\(S\\) is the standard deviation of the sample data and IQR is its interquartile range.\n- Common choices for \\(K(\\cdot)\\) include Gaussian kernel and the Epanichikov kernel.\n- This study uses the Gaussian kernel and estimtes the CDF by plugging the PDF of the standard normal distribution into \\(K(\\cdot)\\) and integrating the resulting quantity, giving \\(\\hat{F}(x)=\\frac{1}{n} \\sum^n_{i=1} \\Phi \\left( \\frac{x-X_i}{h} \\right)\\) where \\(\\hat{F}(x)\\) is the estimate of \\(F(x)\\) and \\(\\Phi\\) is the CDF of the standard normal distribution.\n\n\nMethodology\n\nData layout & statistical criterion\nHere discusses the data to be used in computing STIs and the criterion to be followed.\nData: random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\) of multivariate measurements with dimension \\(p\\), coming from an unknown continuous distributions, say \\(F_X(\\cdot)\\). Each \\(\\mathbf{X}_i = \\left( X_{i1}, X_{i2}, \\dots, X_{ip}\\right)'\\) is a \\((p \\times 1)\\) column vector of measurements taken from the \\(i\\)th subject, \\(i = 1, 2, \\dots, n\\)\n\n\nTwo sided STIs\nObjective: We want to construct two-sided STIs for each component of \\(\\mathbf{X} = (X_1, X_2, \\dots, X_p)'\\). That is, we want to find a region of the form \\((c_1, d_1) \\times (c_2, d_2) \\times \\dots \\times (c_p, d_p)\\) where \\(c_j\\) and \\(d_j\\), \\(j = 1, 2, \\dots, p\\), are functions of the random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\) such that\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(c_j &lt; X_j &lt;d_j \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nThe quantities \\(\\gamma\\) and \\(\\left( 1-\\alpha\\right)\\) are between 0 and 1 and are referred to as the content probability and confidence level.\nCriterion: With a confidence level of \\(100(1-\\alpha)\\%\\), each marginal interval \\((c_j, d_j), j = 1, 2, \\dots, p\\), should have a content of at least \\(\\gamma\\).\n\n\nOne sided STI\nObjective: We want to construct 1-sided STIs.\nThese only have either lower limits only or upper limits only. That is, we want to find a region of the forms\n\\[\n(c_1, \\infty) \\times (c_2, \\infty) \\times \\dots, (c_p, \\infty)\n\\]\nand\n\\[\n(-\\infty, d_1) \\times (-\\infty, d_2) \\times \\dots (-\\infty, d_p)\n\\]\nwhere \\(c_j\\) and \\(d_j\\), \\(j=1, 2, \\dots p\\), are still functions of the random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\), such that\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(X_j &gt; c_j \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nand\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(X_j &lt; d_j \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nIdea:The process of obtaining simultaneous tolerance limits for the random vector \\(X = \\left( X_1, X_2, \\dots, X_p \\right)\\) with \\(p\\)-variate distribution \\(F_X(\\cdot)\\) can also be thought of as constructing simultaneous tolerance limits for the \\(p\\) populations represented by the \\(p\\) components of \\(\\mathbf{X}\\).\nNotes: Methodologies here make no assumptions about the correlation structure of \\(\\mathbf{X}\\) and so procedures also apply to case where \\(X_1, X_2, \\dots, X_p\\) are independent.\n\n\nMethod A: nonparametric two-sided STIs\nReference: Lucagbo (2021), who develops nonparametric rectangular tolerance regions. The main idea of the proposed methods is to transform each component in \\(\\mathbf{X}\\).\nLet \\(F_j(\\cdot)\\) be the CDF of the continuous random variable \\(X_j\\), the \\(j\\)th component of \\(\\mathbf{X}\\). Moreover, let \\(k \\equiv k(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n)\\) and \\(k' \\equiv k'(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n)\\) be some functions of the random sample. The \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\) confidence two-sided STIs for \\(\\mathbf{X}\\) are set to be of the following form:\n\\[\n\\prod^p_{j=1} \\left( F^{-1}_j (k'), F^{-1}_j(k) \\right)\n\\]\nFor each \\(j = 1, 2, \\dots, p\\), the end points of the \\(j\\)th interval are expressed as quantiles of \\(X_j\\). And aligned with the criterion, the values of \\(k\\) and \\(k'\\) are computed to satisfy the following condition:\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(F_j^{-1} \\left(k'\\right) &lt; X_j &lt; F_j^{-1} \\left(k\\right) \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nEquivalently,\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left( k' &lt; F \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nNote that \\(F_j(X_j), j = 1, 2, \\dots, p\\) are identically distributed as \\(U(0, 1)\\) random variables. Hence, the choice of common \\(k\\) and \\(k'\\) for the \\(p\\) components.\nMoreover, using the symmetry property of the uniform distribution, we can choose \\(k'\\) as \\(1-k\\) (ex: Since \\(k' &lt; k\\), let k = 0.7. Since dist is uniform, its max is 1. Hence, \\(k' = 1 - k = 1-0.7 = 0.3 \\rightarrow k'=0.3 &lt; k=0.7\\)). Thus, the criterion can be written as\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left( 1-k &lt; F_j \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\\\\n\\Rightarrow\n\\\\\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( 1-k &lt; F_j \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha \\\n\\\\\\text{take the min prob across j for w/c the prob that at least proportion gamma is within the interval}\n\\\\\n\\text{this is mostly correct/acceptable prob for all j than taking the max one}\n\\]\nSince the CDF \\(F_j(\\cdot), j = 1, 2, \\dots p\\) are unknown, we estimate them marginally via KDE, through the procedure in definitions section\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( 1-k &lt; \\hat{F}_j \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha\n\\]\nis also equivalent to\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( \\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\} \\left( X_j \\right) &lt; k \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha\n\\]\nExplanation:\n\\[\n1-k &lt; \\hat{F}_j \\left( X_j \\right) &lt; k\n\\\\\n\\equiv\n\\\\\n1-k &lt; \\hat{F}_j\\left( X_j \\right) \\ \\text{and} \\  \\hat{F}_j \\left( X_j \\right) &lt; k\n\\\\\n\\equiv\n\\\\\n1-\\hat{F}_j\\left( X_j \\right) &lt; k \\ \\text{and} \\  \\hat{F}_j \\left( X_j \\right) &lt; k\n\\\\\n\\text{this shows that both 1-F and F are less than k and is mathematically equal to saying}\n\\\\\n\\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\} &lt; k\n\\]\nDefine \\(Y_j = \\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\}\\) and disregard the “minimum” condition for now.\nNotice \\(k\\) is consistent with the definition of upper tolerance limit of the random variable \\(Y_j\\) (see: 1-sided STI). So, if we focus only on one component \\(Y_j\\), we could estimate \\(k\\) by the upper tolerance limit of \\(Y_j\\) to be computed from the data using this procedure (nonparametric tolerance intervals) by computing the nonparametric upper tolerance limit based on \\(Y_{1j}, Y_{2j}, \\dots, Y_{nj}\\), where \\(Y_{ij} = \\max \\left\\{ \\hat{F}_j(X_{ij}) , 1 - \\hat{F}_j(X_{ij})\\right\\}, i=1,2,\\dots,n\\).\nSince \\(p&gt;1\\) and to account for the “minimum” condition (This means that the weakest (i.e., smallest) probability across all \\(p\\) dimensions should still meet the threshold \\(\\gamma\\)), we obtain an estimate of \\(k\\), say \\(\\hat{k}\\), by taking the maximum of the marginal upper tolerance limits of the \\(Y_j\\)s.\n\\[\n\\hat{k} = \\max \\left\\{ k_1, k_1, \\dots, k_p\\right\\}\n\\]\nwhere \\(k_j\\) is a marginal upper tolerance limit for the probability condition in dimension \\(j\\).\nUsing this conservative estimate \\(\\hat{k}\\) ensures that the minimum of the inner probabilities as shown below is at least \\(\\gamma\\); i.e., in at least \\((1-\\alpha)\\) fraction of all datasets, the weakest probability among all \\(p\\) dimensions meets of exceeds \\(\\gamma\\)\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ \\min\\limits_{1 \\le j \\le p} P_{X_j} \\left( \\max \\left\\{ \\hat{F}_j(X_j) , 1 - \\hat{F}_j(X_j)\\right\\}  &lt; \\hat{k} \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma \\right\\} = 1 - \\alpha\n\\]\nNow, going back to the original form:\n\\[\nP_{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n} \\left\\{ P_{X_j} \\left(F_j^{-1} (1-\\hat{k}) &lt; X_j &lt; F_j^{-1} (\\hat{k}) \\mid \\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\right) \\ge \\gamma, \\forall j = 1, 2, \\dots, p \\right\\} = 1 - \\alpha\n\\]\nWe say, the \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\) confidence two sided STIs for the data are given by\n\\[\n\\prod^p_{j=1}\\left( c_j, d_j \\right) = \\prod^p_{j=1}\\left( \\hat{F}_j^{-1}(1-\\hat{k}), \\hat{F}_j^{-1}(\\hat{k}) \\right)\n\\]\n\nAlgorithm: Nonparametric two-sided STIs\n\nData are given by \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n \\overset{iid}{\\sim}F_{\\mathbf{X}}(\\mathbf{x})\\) where \\(\\mathbf{X}_i = \\left( X_{i1}, X_{i2}, \\dots, X_{ip}\\right)'\\) is a \\((p \\times 1)\\) vector of measurements taken from the \\(i\\)th subject, \\(i = 1, 2, \\dots, n\\). Also \\(F_{\\mathbf{X}}(\\mathbf{x})\\) is some unknown (continuous) distribution, where \\(F_j(x)\\) is the unknown distribution of the \\(j\\)th component\nFor each \\(j = 1, 2, \\dots, p\\), obtain the estimate \\(\\hat{F}_j(x)\\) via KDE using \\(X_{1j}, X_{2j}, \\dots, X_{nj}\\).\nCompute \\(U_{ij} = \\hat{F}_j(X_{ij}), i = 1, 2, \\dots, n\\) and \\(j = 1, 2, \\dots, p\\)\nCompute \\(Y_{ij} = \\max \\left\\{ U_{ij}, 1-U_{ij} \\right\\}, i = 1, 2, \\dots, n\\) and \\(j = 1, 2, \\dots, p\\)\nFor each \\(j=1, 2, \\dots, p\\), compute the \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\)-confidence nonparametric upper tolerance limit based on \\(Y_{1j}, Y_{2j}, \\dots, Y_{nj}\\) using the methodology here (nonparametric tolerance intervals). Call this \\(k_j, j = 1, 2, \\dots, p\\)\nGet the maximum of the \\(k_j\\)s. Call this \\(\\hat{k} = \\max\\{k_1, k_2, \\dots, k_p\\}\\)\nFor each \\(j=1, 2, \\dots, p\\), compute \\(c_j =\\hat{F}_j^{-1}(1-\\hat{k})\\) and \\(d_j = \\hat{F}_j^{-1}(\\hat{k})\\)\nThe \\(\\gamma\\)-content, \\(100(1-\\alpha)\\%\\)-confidence nonparametric two-sided STIs are given by \\((c_j, d_j), j =1, 2, \\dots, p\\)\n\ncommon \\(\\hat{k}\\) is to be used across all estimated distributions \\(\\hat{F}_j\\)\n\n\nSimulation A: nonparametric two-sided STIs\nThis is aimed to evaluate the performance of the proposed methodology.\n\nTo reflect the potential skewness of the observations, the data are generated from a multivariate lognormal distribution with mean vector \\(\\mathbf{0}\\) and covariance matrix \\(\\Sigma\\) in logarithmic scale.\nDGP will be done using “compositions” R package.\nThe confidence level and content are set at \\(1-\\alpha=0.95\\) and \\(\\gamma = 0.95\\).\nWe use the sample sizes \\(n =100,n=300\\), and \\(n=500\\) for the simulations.\nThe covariance matrix \\(\\Sigma\\) is also varied to represent different correlation structures. We run simulations for the values of \\(\\Sigma\\) of the form \\((1-\\rho)\\mathbf{I}_p + \\rho1_p1_p'\\). Here \\(\\mathbf{I}_p\\) is the \\((p \\times p)\\) identity matrix and \\(1_p\\) is a \\((p \\times 1)\\) column vector of 1s. This choice of \\(\\Sigma\\) is a correlation matrix that assumes an exchangeable correlation structure. We run simulations for \\(p=2\\) and \\(p=3\\) for the ff covariance matrices:\n\n\\[\n\\Sigma_1 = 0.05\\mathbf{I}_p + 0.95\\ 1_p1'_p =\n\\begin{bmatrix}\n1 & 0.95 & \\ldots & 0.95\\\\\n0.95 & 1 & \\ldots & 0.95\\\\\n0.95 & 0.95 & \\ddots & 0.95\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n0.95 & 0.95 & \\ldots & 1\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_2 = 0.5\\mathbf{I}_p + 0.5\\ 1_p1'_p =\n\\begin{bmatrix}\n1 & 0.5 & \\ldots & 0.5\\\\\n0.5 & 1 & \\ldots & 0.5\\\\\n0.5 & 0.5 & \\ddots & 0.5\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n0.5 & 0.5 & \\ldots & 1\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_3 = 0.8\\mathbf{I}_p + 0.2\\ 1_p1'_p =\n\\begin{bmatrix}\n1 & 0.2 & \\ldots & 0.2\\\\\n0.2 & 1 & \\ldots & 0.2\\\\\n0.2 & 0.2 & \\ddots & 0.2\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n0.2 & 0.2 & \\ldots & 1\\\\\n\\end{bmatrix}\n\\]\nIn addition, we also run simulations for the following non-exchangeable structures \\(\\Sigma\\):\n\\[\n\\Sigma_4=\n\\begin{bmatrix}\n1 & -0.95 & -0.95 \\\\\n-0.95 & 1 & 0.95 \\\\\n-0.95 & 0.95 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_5=\n\\begin{bmatrix}\n1 & -0.5 & -0.5 \\\\\n-0.5 & 1 & 0.5 \\\\\n-0.5 & 0.5 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_6=\n\\begin{bmatrix}\n1 & -0.2 & -0.2 \\\\\n-0.2 & 1 & 0.2 \\\\\n-0.2 & 0.2 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_7=\n\\begin{bmatrix}\n1 & 0.9 & 0.5 \\\\\n0.9 & 1 & 0.1 \\\\\n0.5 & 0.1 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_8=\n\\begin{bmatrix}\n1 & 0.5 & 0.5 \\\\\n0.5 & 1 & 0.95 \\\\\n0.5 & 0.95 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_9=\n\\begin{bmatrix}\n1 & 0.2 & 0.2 \\\\\n0.2 & 1 & 0.95 \\\\\n0.2 & 0.95 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\Sigma_{10}=\n\\begin{bmatrix}\n1 & -0.5 & -0.5 \\\\\n-0.5 & 1 & 0.95 \\\\\n-0.5 & 0.95 & 1\n\\end{bmatrix}\n\\]\nThe choices for the covariance matrices \\(\\Sigma_1\\) to \\(\\Sigma_{10}\\) are made so that they include as wide variety as possible of the correlation values in terms of both size and direction.\nAside: An exchangeable variance-covariance matrix is a structured covariance matrix where all off-diagonal elements are equal, meaning that each pair of variables has the same covariance.\nUnivariate case\nFurthermore, although this study is primarily concerned with multivariate measurements, we shall also investigate the performance of the KDE-based method in the univariate case and compare it with the performance of the standard approach to compute nonparametric tolerance intervals, which is that of Wilks (1941).\n\nFor the univariate case, we generate the simulated samples from the univariate lognormal distribution with log-scale mean 0.\nThe log-scale variances used are \\(\\sigma^2 = 0.95, \\sigma^2=0.5\\), and \\(\\sigma^2 = 0.20\\).\n\nNote that since the proposed methods apply mainly to the multivariate case, in the univariate care we refrain from saying “proposed method”, and instead say “KDE-based method” even though the univariate case is a special case of the multivariate case.\nThe Gaussian kernel is used in obtaining the KDE. Moreover, the bandwidth \\(h\\) used in the simulations is Silverman’s rule of thumb, which is the preferred bandwidth for the Gaussian kernel. Some numerical simulations (not reported here) have also been performed using a different bandwidth choice and the results are very similar.\nIn Step 7 of Algorithm A, the inverse of the estimated distribution functions are calculated using the GoFKernel R package. In addition, nonparametric tolerance limits described are implemented in R through the tolerance R package. To estimate the coverage probability, this study uses \\(M=5000\\) simulated samples in running Monte Carlo simulations.\n\n\nSimulation A: Performance Evaluation\n\nLook at the estimated coverage probabilities. The desired or nominal confidence level is 0.95.\n\nFor method A, also compute the expected lengths of the tolerance intervals for each component.\nAlgorithm: Performance evaluation of the Nonparametric two-sided STIs obtained from Algo A through estimated coverage probability and expected lengths\n\nGenerate the random sample \\(\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\) from the multivariate lognormal distribution with log-scale mean \\(\\boldsymbol{\\mu}\\) and given log-scale covariance matrix \\(\\Sigma\\). Let \\(\\mu_j\\) be the \\(j\\)th element of \\(\\boldsymbol{\\mu}\\) and \\(\\sigma^2_j\\) be the \\(j\\)th diagonal element of \\(\\Sigma\\).\nCompute the 2-sided tolerance limits \\(c_j\\) and \\(d_j\\), \\(j = 1, 2, \\dots, p\\) using the procedure in Algo A.\nCompute \\(\\min_{1 \\leq j \\leq p} \\left\\{ \\Phi \\left( \\log d_j; \\mu_j, \\sigma^2_j\\right) - \\Phi \\left( \\log c_j; \\mu_j, \\sigma^2_j\\right) \\right\\}\\) where \\(\\Phi \\left( x; \\mu_j, \\sigma^2_j\\right)\\) denotes the CDF of the \\(N \\left( \\mu_j, \\sigma_j^2 \\right)\\)"
  },
  {
    "objectID": "initial_notes.html#tolerance-intervals",
    "href": "initial_notes.html#tolerance-intervals",
    "title": "Prerequisite Studies",
    "section": "",
    "text": "an interval that contains at least a specified proportion \\(\\gamma\\) of the population, with a specified degree of confidence, \\(100 \\left( 1 - \\alpha\\right)\\%\\)\nLet \\(\\mathcal{X} = \\left\\{ X_1, X_2, \\dots, X_n \\right\\}\\) be a random sample from univariate distribution \\(F(\\cdot)\\) and suppose \\(X \\sim F(\\cdot)\\), where \\(X\\) is independent of the random sample, then a subset of \\(\\mathbb{R}\\), say \\(T = T (\\mathcal{X})\\), computed from the random sample, is a \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval for \\(X\\) if it satisfies the ff: (The probability (over all possible samples \\(\\mathcal{X}\\)) that the tolerance interval \\(T\\) constructed from the sample will capture at least a proportion \\(\\gamma\\) of the population is equal to \\(1-\\alpha\\))\nIf I keep collecting samples and building intervals over and over, then 95% of the time, those intervals will contain at least 90% of the population. - That’s a (\\(\\gamma\\), \\(1-\\alpha\\)) tolerance interval.\n\n\\[\nP_{\\mathcal{X}} \\left\\{ P_X \\left( X \\in T \\mid \\mathcal{X} \\right) \\geq \\gamma \\right\\} = 1 - \\alpha\n\\]\n\nset \\(T\\) could be of the form:\n\n\\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\) for a two-sided tolerance interval\n\\(\\left(-\\infty, U(\\mathcal{X}) \\right)\\) for a one-sided upper tolerance interval\n\\(\\left( L(\\mathcal{X}), +\\infty) \\right)\\) for a one-sided lower tolerance interval\n\\(U(\\mathcal{X})\\) and \\(L(\\mathcal{X})\\): upper and lower tolerance limits\n\nsample use case: deriving appropriate process capability limits (\\(U(\\mathcal{X})\\) and \\(L(\\mathcal{X})\\)) for a manufactured product (\\(X\\)), so that with a given level of confidence (\\(1 - \\alpha\\)), they contain the capability measurements of at least a specified proportion of units (\\(\\gamma\\)) from the sampled manufacturing process (ie, gusto mo may confidence ka na yung tolerance interval mo, contains \\(\\gamma\\) proportion nung samples)\ntwo-sided interval\n\nWhat interval will contain \\(p\\) percent of the population measurements?\n\nLower tolerance limit: \\(Y_L = \\hat{Y} - k_2s\\)\nUpper tolerance limit: \\(Y_L = \\hat{Y} + k_2s\\)\n\none-sided intervals\n\nWhat interval guarantees that \\(p\\) percent of population measurements will not fall below a lower limit? Lower tolerance limit: \\(Y_L = \\hat{Y} - k_1s\\)\nWhat interval guarantees that \\(p\\) percent of population measurements will not exceed an upper limit? Lower tolerance limit: \\(Y_U = \\hat{Y} + k_1s\\)\n\n\n\n\n\nUse the method for a parametric distribution if you can safely assume that your sample comes from a population that follows that distribution.\nIf your data follow a parametric distribution, then a method that uses that distribution is more precise and economical than the nonparametric method. A method that uses a distribution achieves smaller margins of error with fewer observations, as long as the chosen distribution is appropriate for your data.\n\nMinitab\n\n\n\n\nSuppose \\(X_{(1)}, X_{(2)}, \\dots, X_{(n)}\\) are order statistics of a random sample from a population with a univariate continuous distribution function \\(F(x)\\).\nWilk’s approach: 2 sided interval\n\nsets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\)\nThe values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\). It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that\nthe nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\nWilk’s approach: 1 sided interval: Lower tolerance limit\n\nsets \\(L(\\mathcal{X}) = X_{(r)}\\) as the lower limit of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval.\nThe value of \\(r\\) is taken to be the largest integer for which \\(P(Y \\ge r \\mid n, 1- \\gamma) \\ge 1 - \\alpha\\) where \\(Y \\sim \\text{Binomial}(n, 1-\\gamma)\\).\nThe non parametric one-sided lower tolerance interval is given by \\((X_{(r)}, \\infty)\\)\n\nWilk’s approach: 1 sided interval: Upper tolerance limit\n\nsets \\(U(\\mathcal{X}) = X_{S}\\), where it can be shown that\n\\(s=n-r+1\\) and \\(r\\) is the value derived for the lower tolerance limit.\nThe nonparametric one-sided upper tolerance interval is given by \\((-\\infty, X_{(n-r+1)})\\)\n\nThere is a minimum sample size requirement in computing the nonparametric tolerance intervals. This depends on the values of \\(\\gamma\\) and \\(1-\\alpha\\)\nThe coverage probabilities associated with nonparametric tolerance intervals are known to be conservative.\nAlthough in this study we shall only be using Wilks’ approach, we mention that recently some developments aimed at improving coverage probabilities of nonparametric tolerance intervals have been available in literature.\n\n\n\n\nSuppose that we take a sample of \\(N=25\\) silicon wafers from a lot and measure their thickness in order to find tolerance limits within which a proportion \\(p = 0.90\\) of the wafers in the lot fall with confidence \\(\\alpha = 0.99\\). Since the standard deviation, \\(s\\), is computed from the sample of 25 wafers, the degree of freedom is \\(\\nu = N - 1\\)\n\nThe dataParametric approachNonparametric approach\n\n\n\n## \"Direct\" calculation of a tolerance interval.\n\n## Read data and name variables.\n\n# URL of the dataset\n#url &lt;- \"https://www.itl.nist.gov/div898/handbook/datasets/MPC62.DAT\"\n\n# Download the file (you can specify a path where you want to save it)\n# download.file(url, destfile = \"MPC62.DAT\")\n\n# Load the dataset into R\nmdat = read.table(\"rdata/MPC62.DAT\",header=FALSE, skip=50)\ncolnames(mdat) = c(\"cr\", \"wafer\", \"mo\", \"day\", \"h\", \"min\", \"op\", \n                 \"hum\", \"probe\", \"temp\", \"y\", \"sw\", \"df\")\n\nhead(mdat)\n\n     cr wafer mo day  h min op hum probe   temp      y    sw df\n1 51939   137  3  24 18   1  1  42  2362 23.003 97.070 0.085  5\n2 51939   137  3  25 12  41  1  35  2362 23.115 97.049 0.052  5\n3 51939   137  3  25 15  57  1  33  2362 23.196 97.048 0.038  5\n4 51939   137  3  28 10  10  2  47  2362 23.383 97.084 0.036  5\n5 51939   137  3  28 13  31  2  44  2362 23.491 97.106 0.049  5\n6 51939   137  3  28 17  33  1  43  2362 23.352 97.014 0.036  5\n\n\n\nlibrary(tolerance)\n\ntolerance package, version 3.0.0, Released 2024-04-18\nThis package is based upon work supported by the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\n\n\n\nThrough tolerance package\n\npout = normtol.int(mdat$y, \n                   alpha=0.01, # chosen such that 1-alpha is the conf level\n                   P=.90, # proportion of the pop to be covered by this TI\n                   side=2, # 1-sided or 2-sided tolerance interval\n                   method = \"HE2\", # method for calculating the k-factors\n                   m=100 # maxnum of subintervals to use in the integrate fcn\n                   )\npout\n\n  alpha   P    x.bar 2-sided.lower 2-sided.upper\n1  0.01 0.9 97.06984        97.003      97.13668\n\n\nTI Interpretation: In this example, you can be 99% confident that at least 90% of all thickness measurements are between approximately 97.0030038 and 97.1366762.\nManually deriving the endpoints:\nThe \\(k\\) factors are determined so that the intervals cover at least a proportion \\(p\\) of the population with confidence \\(\\alpha\\). The value of \\(p\\) is also referred to as the coverage factor. This assumes normal distribution. This is the approximate value for \\(k_2\\) from Howe, 1969.\n\\[\n\\sqrt{\n\\nu \\left(1+ \\frac{1}{N} \\right) \\times \\frac {z_{\\left( \\frac{1+p}{2}\\right)}^2}\n{\\chi^2_{1-\\alpha, \\nu}}\n}\n\\]\n\n## Compute the approximate k2 factor for a two-sided tolerance interval. \n## For this example, the standard deviation is computed from the sample,\n## so the degrees of freedom are nu = N - 1.\nN = 25\nnu = N - 1 # df used to estimate the sd\np = 0.90 # proportion\ng = 0.99 # alpha\nz2 = (qnorm((1+p)/2))**2 # critical values of the normal distribution\nc2 = qchisq(1-g,nu) # lower critical values of the chi-square distribution\nk2 = sqrt(nu*(1 + 1/N)*z2/c2)\nk2\n\n[1] 2.494063\n\n## Compute the exact k2 factor for a two-sided tolerance interval using \n## the K.factor function in the tolerance library\n## \"HE2\" is a second method due to Howe, which performs similarly to the \n## Weissberg-Beatty method, but is computationally simpler\nK2 = K.factor(n=N, f=nu, alpha=1-g, P=p, side=2, method=\"HE2\", m=1000)\nK2\n\n[1] 2.494063\n\n\n\npout$x.bar-K2*sd(mdat$y); pout$x.bar+K2*sd(mdat$y)\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\n\nplottol(pout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\nParametric and univariate\nGiven a sample size \\(n\\), sample mean \\(\\bar{x}\\), and sample standard deviation \\(s\\), the two-sided normal tolerance interval is:\n\\[\n\\bar{x} \\pm k \\cdot s\n\\]\nwhere \\(k\\) is a factor based on:\n\\(n\\), confidence level, \\((1-\\alpha)\\), and population coverage \\(\\gamma\\)\nIn R, to find the smallest value \\(m\\) such that \\(P(Y \\le m-1) \\ge 1-\\alpha\\) for a binomial distribution \\(Y \\sim \\text{Binomial}(n, \\gamma)\\), you can use the cumulative distribution function (CDF) for the Binomial distribution, which is available in the pbinom() function.\nThe condition \\(P(Y \\le m-1) \\ge 1-\\alpha\\) suggests that you are looking for the quantile \\(m\\) such that the cumulative probability is at least \\(1-\\alpha\\).\nRef\n\n\nThrough tolerance package\n\nnpout = nptol.int(mdat$y,\n                  alpha = 0.01, \n                  P = 0.90, \n                  side = 2, \n                  method = \"WILKS\",\n                  upper = NULL, \n                  lower = NULL)\nnpout\n\n  alpha   P 2-sided.lower 2-sided.upper\n1  0.01 0.9        97.014        97.114\n\n\nTI Interpretation: You can be 99% confident that at least 90% of all thickness measurements are between approximately 97.014 and 97.114.\nManually deriving the endpoints:\nWilk’s approach: 2 sided interval\n- sets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\)\n- The values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\).\n- It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that - the nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\n# 1 &lt;= r &lt; s &lt; N = 25\n# let s = n−r+1\n# s - r = (n-r+1) - r = m \n# (n-m+1)/2 = r\n\n# Parameters\nn &lt;- 25    # Number of trials\ngamma &lt;- 0.90 # Probability of success in each trial\nalpha &lt;- 0.01 # Significance level (1 - confidence level)\n\n# Find the smallest m such that P(Y &lt;= m-1) &gt;= 1 - alpha ie find the \n# smallest value of m such that the cumulative probability just reaches \n# or exceeds the confidence level 1-alpha\nm &lt;- qbinom(1 - alpha, size = n, prob = gamma)\n\nr = (n-m+1)/2\ns = m+r\nsort(mdat$y)[ceiling(r)]; sort(mdat$y)[floor(n-r+1)]\n\n[1] 97.014\n\n\n[1] 97.114\n\n\n\nplottol(npout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")"
  },
  {
    "objectID": "initial_notes.html#parametric-tolerance-intervals",
    "href": "initial_notes.html#parametric-tolerance-intervals",
    "title": "",
    "section": "Parametric Tolerance Intervals",
    "text": "Parametric Tolerance Intervals\n\nUse the method for a parametric distribution if you can safely assume that your sample comes from a population that follows that distribution.\nIf your data follow a parametric distribution, then a method that uses that distribution is more precise and economical than the nonparametric method. A method that uses a distribution achieves smaller margins of error with fewer observations, as long as the chosen distribution is appropriate for your data.\n\nMinitab"
  },
  {
    "objectID": "initial_notes.html#nonparametric-tolerance-intervals",
    "href": "initial_notes.html#nonparametric-tolerance-intervals",
    "title": "",
    "section": "Nonparametric Tolerance Intervals",
    "text": "Nonparametric Tolerance Intervals\n\nSuppose \\(X_{(1)}, X_{(2)}, \\dots, X_{(n)}\\) are order statistics of a random sample from a population with a univariate continuous distribution function \\(F(x)\\).\nWilk’s approach: 2 sided interval\n\nsets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\)\nThe values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\). It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that\nthe nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\nWilk’s approach: 1 sided interval: Lower tolerance limit\n\nsets \\(L(\\mathcal{X}) = X_{(r)}\\) as the lower limit of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval.\nThe value of \\(r\\) is taken to be the largest integer for which \\(P(Y \\ge r \\mid n, 1- \\gamma) \\ge 1 - \\alpha\\) where \\(Y \\sim \\text{Binomial}(n, 1-\\gamma)\\).\nThe non parametric one-sided lower tolerance interval is given by \\((X_{(r)}, \\infty)\\)\n\nWilk’s approach: 1 sided interval: Upper tolerance limit\n\nsets \\(U(\\mathcal{X}) = X_{S}\\), where it can be shown that\n\\(s=n-r+1\\) and \\(r\\) is the value derived for the lower tolerance limit.\nThe nonparametric one-sided upper tolerance interval is given by \\((-\\infty, X_{(n-r+1)})\\)\n\nThere is a minimum sample size requirement in computing the nonparametric tolerance intervals. This depends on the values of \\(\\gamma\\) and \\(1-\\alpha\\)\nThe coverage probabilities associated with nonparametric tolerance intervals are known to be conservative.\nAlthough in this study we shall only be using Wilks’ approach, we mention that recently some developments aimed at improving coverage probabilities of nonparametric tolerance intervals have been available in literature."
  },
  {
    "objectID": "initial_notes.html#example",
    "href": "initial_notes.html#example",
    "title": "",
    "section": "Example",
    "text": "Example\nSuppose that we take a sample of \\(N=25\\) silicon wafers from a lot and measure their thickness in order to find tolerance limits within which a proportion \\(p = 0.90\\) of the wafers in the lot fall with confidence \\(\\alpha = 0.99\\). Since the standard deviation, \\(s\\), is computed from the sample of 25 wafers, the df are \\(\\nu = N - 1\\)\n\nThe dataParametric approachNonparametric approach\n\n\n\n## \"Direct\" calculation of a tolerance interval.\n\n## Read data and name variables.\n\n# URL of the dataset\n#url &lt;- \"https://www.itl.nist.gov/div898/handbook/datasets/MPC62.DAT\"\n\n# Download the file (you can specify a path where you want to save it)\n# download.file(url, destfile = \"MPC62.DAT\")\n\n# Load the dataset into R\nmdat = read.table(\"MPC62.DAT\",header=FALSE, skip=50)\ncolnames(mdat) = c(\"cr\", \"wafer\", \"mo\", \"day\", \"h\", \"min\", \"op\", \n                 \"hum\", \"probe\", \"temp\", \"y\", \"sw\", \"df\")\n\nhead(mdat)\n\n     cr wafer mo day  h min op hum probe   temp      y    sw df\n1 51939   137  3  24 18   1  1  42  2362 23.003 97.070 0.085  5\n2 51939   137  3  25 12  41  1  35  2362 23.115 97.049 0.052  5\n3 51939   137  3  25 15  57  1  33  2362 23.196 97.048 0.038  5\n4 51939   137  3  28 10  10  2  47  2362 23.383 97.084 0.036  5\n5 51939   137  3  28 13  31  2  44  2362 23.491 97.106 0.049  5\n6 51939   137  3  28 17  33  1  43  2362 23.352 97.014 0.036  5\n\n\n\n\nThe \\(k\\) factors are determined so that the intervals cover at least a proportion \\(p\\) of the population with confidence \\(\\alpha\\).\nThe value of \\(p\\) is also referred to as the coverage factor.\nUnivariate Tolerance Intervals (Two-sided) assuming normal distribution\nThis is the approximate value for \\(k_2\\) from Howe, 1969.\n\\[\n\\sqrt{\n\\nu \\left(1+ \\frac{1}{N} \\right) \\times \\frac {z_{\\left( \\frac{1+p}{2}\\right)}^2}\n{\\chi^2_{1-\\alpha, \\nu}}\n}\n\\]\n\n## Compute the approximate k2 factor for a two-sided tolerance interval. \n## For this example, the standard deviation is computed from the sample,\n## so the degrees of freedom are nu = N - 1.\nN = 25\nnu = N - 1 # df used to estimate the sd\np = 0.90 # proportion\ng = 0.99 # alpha\nz2 = (qnorm((1+p)/2))**2 # z critical values of the normal distribution\nc2 = qchisq(1-g,nu) # chi X lower critical values of the chi-square distribution\nk2 = sqrt(nu*(1 + 1/N)*z2/c2)\nk2\n\n[1] 2.494063\n\n\n2-sided tolerance factor thru tolerance package\n\n## Compute the exact k2 factor for a two-sided tolerance interval using \n## the K.factor function in the tolerance library\n## \"HE2\" is a second method due to Howe, which performs similarly to the Weissberg-Beatty method, but is computationally simpler\nlibrary(tolerance)\n\ntolerance package, version 3.0.0, Released 2024-04-18\nThis package is based upon work supported by the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\nK2 = K.factor(n=N, f=nu, alpha=1-g, P=p, side=2, method=\"HE2\", m=1000)\nK2\n\n[1] 2.494063\n\n\ntolerance package - parametric\n\nlibrary(tolerance)\npout = normtol.int(mdat$y, \n                   alpha=0.01, # The level chosen such that 1-alpha is the confidence level.\n                   P=.90, # The proportion of the population to be covered by this tolerance interval.\n                   side=2, # 1-sided or 2-sided tolerance interval\n                   method = \"HE2\", # method for calculating the k-factors\n                   m=100 # maximum number of subintervals to be used in the integrate function; larger more accurate\n                   )\npout\n\n  alpha   P    x.bar 2-sided.lower 2-sided.upper\n1  0.01 0.9 97.06984        97.003      97.13668\n\n\nHighlighting the endpoints from pout:\nIn this example, you can be 99% confident that at least 90% of all thickness measurements are between approximately 97.0030038 and 97.1366762.\n\npout$`2-sided.lower`; pout$`2-sided.upper`\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\nManually deriving the endpoints:\n\npout$x.bar-K2*sd(mdat$y); pout$x.bar+K2*sd(mdat$y)\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\n\nplottol(pout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\nRef\n\n\ntolerance package - nonparametric\n\nnpout = nptol.int(mdat$y,\n                  alpha = 0.01, \n                  P = 0.90, \n                  side = 2, \n                  method = \"WILKS\",\n                  upper = NULL, \n                  lower = NULL)\nnpout\n\n  alpha   P 2-sided.lower 2-sided.upper\n1  0.01 0.9        97.014        97.114\n\n\nHighlighting the endpoints from npout:\nYou can be 99% confident that at least 90% of all thickness measurements are between approximately 97.014 and 97.114.\n\nnpout$`2-sided.lower`; npout$`2-sided.upper`\n\n[1] 97.014\n\n\n[1] 97.114\n\n\nManually deriving the endpoints:\nWilk’s approach: 2 sided interval - sets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\) - The values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\). It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that - the nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\n# 1 &lt;= r &lt; s &lt; N = 25\n\n\n# let s = n−r+1\n# s - r = (n-r+1) - r = m \n# (n-m+1)/2 = r\n\n# Parameters\nn &lt;- 25    # Number of trials\ngamma &lt;- 0.90 # Probability of success in each trial\nalpha &lt;- 0.01 # Significance level (1 - confidence level)\n\n# Find the smallest m such that P(Y &lt;= m-1) &gt;= 1 - alpha\n# ie find the smallest value of m such that the cumulative probability just reaches or exceeds the confidence level 1-alpha\nm &lt;- qbinom(1 - alpha, size = n, prob = gamma)\n\nr = (n-m+1)/2\ns = m+r\nsort(mdat$y)[ceiling(r)]; sort(mdat$y)[floor(n-r+1)]\n\n[1] 97.014\n\n\n[1] 97.114\n\n\n\nplottol(npout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\n\n\n\n\nAside\n\nset.seed(5)\nsimulated &lt;- rlnorm(1000, 0, 1)\nhist(simulated)\n\n\n\n\n\n\n\n\n\nfind_silver_bw &lt;- function(sample){\n  n &lt;- length(sample)\n  iqr &lt;- IQR(sample)\n  sd &lt;- sd(sample)\n  return(0.9*n^(-1/5)*min(c(sd, iqr/1.34)))\n}\n\nget_grid &lt;- function(sample){\n  lb &lt;- min(sort(sample)) - sd(sample)\n  ub &lt;- max(sort(sample)) + sd(sample)\n  return(seq(from=lb, to=ub, by=0.0001))\n}\n\ngaussian_kernel &lt;- function(bandwidth, grid, datapoint) {\n  return((1/(bandwidth*sqrt(2*pi))) * exp(-0.5*((grid-datapoint)/bandwidth)^2))\n#  return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))\n}\n\n\n# fcn to estimate \nkde_manual &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    sum(gaussian_kernel(bandwidth, grid_val, datapoints) / n)\n  })\n}\n\nkde_manual_presum &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    gaussian_kernel(bandwidth, grid_val, datapoints)\n  })\n}\n\n\n# KDE for CDF estimation using Gaussian kernel\nkde_cdf &lt;- function(grid, datapoints, bandwidth) {\n  sapply(grid, function(grid_val) {\n    mean(pnorm((grid_val - datapoints) / bandwidth))\n  })\n}\n\n\nestimated_cdf_sim &lt;- kde_cdf(get_grid(simulated), simulated, find_silver_bw(simulated))\n\n\nplot(get_grid(simulated), estimated_cdf_sim, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDFs Comparison\")\n\n\n\n\n\n\n\n\n\nestimated_pdf_sim &lt;- kde_manual(get_grid(simulated), simulated, find_silver_bw(simulated))\n\n\nplot(get_grid(simulated), estimated_pdf_sim, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated PDF\",\n     main = \"Estimated CDFs Comparison\")\n\n\n\n\n\n\n\n\n\nlibrary(\"GoFKernel\")\n\nLoading required package: KernSmooth\n\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\nkde_cdf_value &lt;- function(x, datapoints, bandwidth) {\n  mean(pnorm((x - datapoints) / bandwidth))\n}\n\nkde_cdf_inverse &lt;- function(probabilities, datapoints, bandwidth, lower = -10, upper = 10) {\n  sapply(probabilities, function(p) {\n    root_func &lt;- function(x) kde_cdf_value(x, datapoints, bandwidth) - p\n    uniroot(root_func, lower = lower, upper = upper)$root\n  })\n}\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\n\n\n\n# =================\n\nU_mat &lt;- kde_cdf(simulated, simulated, find_silver_bw(simulated))\n#U_mat &lt;- sapply(p_data, function(i) kde_cdf(i, p_data, h))\n\n# Step 4: Compute Y_ij = max{U_ij, 1 - U_ij}\nY_mat &lt;- pmax(U_mat, 1 - U_mat)\n\n#u&lt;-data.frame(original = transformed_data, complement = 1 - transformed_data)\n#y1 &lt;- apply(u, 1, max)\nnpout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                upper = NULL, lower = NULL)\np &lt;- npout$`1-sided.upper`\n\nf.inv &lt;- inverse(kde_cdf_function(simulated,find_silver_bw(simulated)),lower=-100,upper=100)\n  \n#f.inv(p);f.inv(1-p); qlnorm(1-p); exp(qnorm(1-p))\n\n\npnorm(log(f.inv(p)),0,1) - pnorm(log(f.inv(1-p)),0,1)\n\n[1] 0.9274381\n\n\n\nplnorm(f.inv(p),0,1) - plnorm(f.inv(1-p),0,1)\n\n[1] 0.9274381\n\n\n\nf.inv &lt;- inverse(function(i)kde_cdf(i, simulated,find_silver_bw(simulated)),lower=-1000,upper=1000)\n  \np &lt;- 0.97\nf.inv(p); qnorm(p)\n\n[1] 6.773139\n\n\n[1] 1.880794\n\n\n\nf &lt;- function(x) pbeta(x, shape1=2, shape2=3)\nf.inv &lt;- inverse(f,lower=0,upper=1)\nf.inv(.2)\n\n[1] 0.2123161\n\nqbeta(p=0.2, shape1=2, shape2=3)\n\n[1] 0.2123171"
  },
  {
    "objectID": "initial_notes.html#univariate-example",
    "href": "initial_notes.html#univariate-example",
    "title": "",
    "section": "Univariate Example",
    "text": "Univariate Example\nSuppose that we take a sample of \\(N=25\\) silicon wafers from a lot and measure their thickness in order to find tolerance limits within which a proportion \\(p = 0.90\\) of the wafers in the lot fall with confidence \\(\\alpha = 0.99\\). Since the standard deviation, \\(s\\), is computed from the sample of 25 wafers, the df are \\(\\nu = N - 1\\)\n\nThe dataParametric approachNonparametric approach\n\n\n\n## \"Direct\" calculation of a tolerance interval.\n\n## Read data and name variables.\n\n# URL of the dataset\n#url &lt;- \"https://www.itl.nist.gov/div898/handbook/datasets/MPC62.DAT\"\n\n# Download the file (you can specify a path where you want to save it)\n# download.file(url, destfile = \"MPC62.DAT\")\n\n# Load the dataset into R\nmdat = read.table(\"MPC62.DAT\",header=FALSE, skip=50)\ncolnames(mdat) = c(\"cr\", \"wafer\", \"mo\", \"day\", \"h\", \"min\", \"op\", \n                 \"hum\", \"probe\", \"temp\", \"y\", \"sw\", \"df\")\n\nhead(mdat)\n\n     cr wafer mo day  h min op hum probe   temp      y    sw df\n1 51939   137  3  24 18   1  1  42  2362 23.003 97.070 0.085  5\n2 51939   137  3  25 12  41  1  35  2362 23.115 97.049 0.052  5\n3 51939   137  3  25 15  57  1  33  2362 23.196 97.048 0.038  5\n4 51939   137  3  28 10  10  2  47  2362 23.383 97.084 0.036  5\n5 51939   137  3  28 13  31  2  44  2362 23.491 97.106 0.049  5\n6 51939   137  3  28 17  33  1  43  2362 23.352 97.014 0.036  5\n\n\n\nlibrary(tolerance)\n\ntolerance package, version 3.0.0, Released 2024-04-18\nThis package is based upon work supported by the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\n\n\n\nThrough tolerance package\n\npout = normtol.int(mdat$y, \n                   alpha=0.01, # chosen such that 1-alpha is the conf level\n                   P=.90, # proportion of the pop to be covered by this TI\n                   side=2, # 1-sided or 2-sided tolerance interval\n                   method = \"HE2\", # method for calculating the k-factors\n                   m=100 # maxnum of subintervals to use in the integrate fcn\n                   )\npout\n\n  alpha   P    x.bar 2-sided.lower 2-sided.upper\n1  0.01 0.9 97.06984        97.003      97.13668\n\n\nTI Interpretation: In this example, you can be 99% confident that at least 90% of all thickness measurements are between approximately 97.0030038 and 97.1366762.\nManually deriving the endpoints:\nThe \\(k\\) factors are determined so that the intervals cover at least a proportion \\(p\\) of the population with confidence \\(\\alpha\\). The value of \\(p\\) is also referred to as the coverage factor. This assumes normal distribution. This is the approximate value for \\(k_2\\) from Howe, 1969.\n\\[\n\\sqrt{\n\\nu \\left(1+ \\frac{1}{N} \\right) \\times \\frac {z_{\\left( \\frac{1+p}{2}\\right)}^2}\n{\\chi^2_{1-\\alpha, \\nu}}\n}\n\\]\n\n## Compute the approximate k2 factor for a two-sided tolerance interval. \n## For this example, the standard deviation is computed from the sample,\n## so the degrees of freedom are nu = N - 1.\nN = 25\nnu = N - 1 # df used to estimate the sd\np = 0.90 # proportion\ng = 0.99 # alpha\nz2 = (qnorm((1+p)/2))**2 # critical values of the normal distribution\nc2 = qchisq(1-g,nu) # lower critical values of the chi-square distribution\nk2 = sqrt(nu*(1 + 1/N)*z2/c2)\nk2\n\n[1] 2.494063\n\n## Compute the exact k2 factor for a two-sided tolerance interval using \n## the K.factor function in the tolerance library\n## \"HE2\" is a second method due to Howe, which performs similarly to the \n## Weissberg-Beatty method, but is computationally simpler\nK2 = K.factor(n=N, f=nu, alpha=1-g, P=p, side=2, method=\"HE2\", m=1000)\nK2\n\n[1] 2.494063\n\n\n\npout$x.bar-K2*sd(mdat$y); pout$x.bar+K2*sd(mdat$y)\n\n[1] 97.003\n\n\n[1] 97.13668\n\n\n\nplottol(pout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\nRef\n\n\nThrough tolerance package\n\nnpout = nptol.int(mdat$y,\n                  alpha = 0.01, \n                  P = 0.90, \n                  side = 2, \n                  method = \"WILKS\",\n                  upper = NULL, \n                  lower = NULL)\nnpout\n\n  alpha   P 2-sided.lower 2-sided.upper\n1  0.01 0.9        97.014        97.114\n\n\nTI Interpretation: You can be 99% confident that at least 90% of all thickness measurements are between approximately 97.014 and 97.114.\nManually deriving the endpoints:\nWilk’s approach: 2 sided interval\n- sets values \\(L(\\mathcal{X}) = X_{(r)}\\) and \\(U(\\mathcal{X}) = X_{(s)}\\) where \\(r &lt; s\\), as the limits of the \\(\\gamma\\)-content \\(100 \\left( 1 - \\alpha\\right)\\%\\)-confidence tolerance interval \\(\\left( L(\\mathcal{X}), U(\\mathcal{X}) \\right)\\)\n- The values of \\(r\\) and \\(s\\) are chosen to satisfy \\(1 \\le r \\le s \\le n\\) and \\(s-r=m\\) where \\(m\\) is the smallest value for which \\(P(Y \\le m-1) \\ge 1-\\alpha\\), and \\(Y \\sim \\text{Binomial}(n, \\gamma)\\).\n- It is customary to take the value of \\(s\\) to be equal to \\(n-r+1\\), as suggested by Wilks et.al. so that - the nonparametric interval is given by \\((X_{(r)}, X_{(n-r+1)})\\)\n\n# 1 &lt;= r &lt; s &lt; N = 25\n# let s = n−r+1\n# s - r = (n-r+1) - r = m \n# (n-m+1)/2 = r\n\n# Parameters\nn &lt;- 25    # Number of trials\ngamma &lt;- 0.90 # Probability of success in each trial\nalpha &lt;- 0.01 # Significance level (1 - confidence level)\n\n# Find the smallest m such that P(Y &lt;= m-1) &gt;= 1 - alpha ie find the \n# smallest value of m such that the cumulative probability just reaches \n# or exceeds the confidence level 1-alpha\nm &lt;- qbinom(1 - alpha, size = n, prob = gamma)\n\nr = (n-m+1)/2\ns = m+r\nsort(mdat$y)[ceiling(r)]; sort(mdat$y)[floor(n-r+1)]\n\n[1] 97.014\n\n\n[1] 97.114\n\n\n\nplottol(npout, mdat$y, plot.type = \"both\", side = \"two\", x.lab = \"X\")\n\n\n\n\n\n\n\n\n\n\n\n\nAside\n\nLognormal & normal distribution\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(X\\) is normally distributed.\nIf \\(Y \\sim \\{Lognormal}(\\mu, \\sigma^2)\\), then \\(\\log Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nThis means a lognormal random variable is one whose logarithm is normally distributed.\nIf you assume the data itself is normal (norm): The raw data \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nIf you assume the log of the data is normal (lnorm): You are modeling \\(\\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) so \\(X \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\)\nMean is zero in the log scale: This implies\n\\[\n\\log(Y) \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nwhich is equivalent to\n\\[\nY \\sim \\text{Lognormal}(\\mu, \\sigma^2)\n\\]\nThis also tells that the mean of \\(Y\\) is not 0. In fact, \\[\n\\mathbb{E}[Y] =  e^{\\mu+\\sigma^2/2}\n\\]\nSo if \\(\\mu = 0\\) then,\n\\[\n\\mathbb{E}[Y] =  e^{\\sigma^2/2} &gt; 1\n\\]\n\nset.seed(5)\nsimulated &lt;- rlnorm(1000, 0, 1)\nhist(simulated)\n\n\n\n\n\n\n\n\n\nfind_silver_bw &lt;- function(sample){\n  n &lt;- length(sample)\n  iqr &lt;- IQR(sample)\n  sd &lt;- sd(sample)\n  return(0.9*n^(-1/5)*min(c(sd, iqr/1.34)))\n}\n\nget_grid &lt;- function(sample){\n  lb &lt;- min(sort(sample)) - sd(sample)\n  ub &lt;- max(sort(sample)) + sd(sample)\n  return(seq(from=lb, to=ub, by=0.0001))\n}\n\ngaussian_kernel &lt;- function(bandwidth, grid, datapoint) {\n  return((1/(bandwidth*sqrt(2*pi))) * exp(-0.5*((grid-datapoint)/bandwidth)^2))\n#  return((1 / sqrt(2 * pi)) * exp(-0.5 * u^2))\n}\n\n\n# fcn to estimate \nkde_manual &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    sum(gaussian_kernel(bandwidth, grid_val, datapoints) / n)\n  })\n}\n\nkde_manual_presum &lt;- function(grid, datapoints, bandwidth) {\n  n &lt;- length(datapoints)\n  sapply(grid, function(grid_val) {\n    gaussian_kernel(bandwidth, grid_val, datapoints)\n  })\n}\n\n\n# KDE for CDF estimation using Gaussian kernel\nkde_cdf &lt;- function(grid, datapoints, bandwidth) {\n  sapply(grid, function(grid_val) {\n    mean(pnorm((grid_val - datapoints) / bandwidth))\n  })\n}\n\n\nestimated_cdf_sim &lt;- kde_cdf(get_grid(simulated), simulated, find_silver_bw(simulated))\n\n\nplot(get_grid(simulated), estimated_cdf_sim, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDFs Comparison\")\n\n\n\n\n\n\n\n\n\nestimated_pdf_sim &lt;- kde_manual(get_grid(simulated), simulated, find_silver_bw(simulated))\n\n\nplot(get_grid(simulated), estimated_pdf_sim, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated PDF\",\n     main = \"Estimated CDFs Comparison\")\n\n\n\n\n\n\n\n\n\nlibrary(\"GoFKernel\")\n\nLoading required package: KernSmooth\n\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\nkde_cdf_value &lt;- function(x, datapoints, bandwidth) {\n  mean(pnorm((x - datapoints) / bandwidth))\n}\n\nkde_cdf_inverse &lt;- function(probabilities, datapoints, bandwidth, lower = -10, upper = 10) {\n  sapply(probabilities, function(p) {\n    root_func &lt;- function(x) kde_cdf_value(x, datapoints, bandwidth) - p\n    uniroot(root_func, lower = lower, upper = upper)$root\n  })\n}\n\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\n\n\n\n# =================\n\nU_mat &lt;- kde_cdf(simulated, simulated, find_silver_bw(simulated))\n#U_mat &lt;- sapply(p_data, function(i) kde_cdf(i, p_data, h))\n\n# Step 4: Compute Y_ij = max{U_ij, 1 - U_ij}\nY_mat &lt;- pmax(U_mat, 1 - U_mat)\n\n#u&lt;-data.frame(original = transformed_data, complement = 1 - transformed_data)\n#y1 &lt;- apply(u, 1, max)\nnpout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                upper = NULL, lower = NULL)\np &lt;- npout$`1-sided.upper`\n\nf.inv &lt;- inverse(kde_cdf_function(simulated,find_silver_bw(simulated)),lower=-100,upper=100)\n  \n#f.inv(p);f.inv(1-p); qlnorm(1-p); exp(qnorm(1-p))\n\n\npnorm(log(f.inv(p)),0,1) - pnorm(log(f.inv(1-p)),0,1)\n\n[1] 0.9274381\n\n\nlower_bound &lt;- f.inv(1 - maxk) upper_bound &lt;- f.inv(maxk)\n\nplnorm(f.inv(p),0,1) - plnorm(f.inv(1-p),0,1)\n\n[1] 0.9274381\n\n\n\nf.inv(1-p)\n\n[1] 0.1130149\n\n\n\np\n\n[1] 0.9377674\n\n\n\nf.inv(p)\n\n[1] 4.817659\n\n\n\nf.inv &lt;- inverse(function(i)kde_cdf(i, simulated,find_silver_bw(simulated)),lower=-1000,upper=1000)\n  \np &lt;- 0.97\nf.inv(p); qnorm(p)\n\n[1] 6.773139\n\n\n[1] 1.880794\n\n\n\nf &lt;- function(x) pbeta(x, shape1=2, shape2=3)\nf.inv &lt;- inverse(f,lower=0,upper=1)\nf.inv(.2)\n\n[1] 0.2123161\n\nqbeta(p=0.2, shape1=2, shape2=3)\n\n[1] 0.2123171"
  },
  {
    "objectID": "initial_notes.html#related-literature",
    "href": "initial_notes.html#related-literature",
    "title": "",
    "section": "Related Literature",
    "text": "Related Literature\n\nKernel density estimationDGP\n\n\nReference 1 Plotting KDE\nKernel - Kernel functions are used to estimate density of random variables and as weighing function in non-parametric regression.\n\nx_data &lt;- sort(c(65, 75, 67, 79, 81, 91))\nx_data_sd &lt;- sd(x_data)\nx_data_iqr &lt;- IQR(x_data)\nx_data_n &lt;- length(x_data)\nh_silver &lt;- 0.9*x_data_n^(-1/5)*min(c(x_data_sd, x_data_iqr/1.34))\n\n\nxi &lt;- 65#x_data[1]\nh &lt;- 5.5 #h_silver\nmin_val &lt;- 40#min(x_data)-round(h)\nmax_val &lt;- 120#max(x_data)+round(h)\nx &lt;- seq(from = min_val, to = max_val, by = 1) # grid\n\n\nmanual1\n\n# Estimate PDF\noutmat &lt;- t(kde_manual_presum(x, x_data, h))\nestimated_density &lt;- kde_manual(x, x_data, h)\n\n\n# Estimate CDF\nestimated_cdf &lt;- kde_cdf(x, x_data, h)\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\nrownames(outmat) &lt;- x\ncolnames(outmat) &lt;- x_data\nmat &lt;- cbind(grid = rownames(outmat), outmat)\nrownames(outmat) &lt;- 1:nrow(outmat)\n\ndf_long &lt;- as.data.frame(mat,as.numeric) %&gt;%\n  pivot_longer(cols = colnames(outmat), names_to = \"sample\", values_to = \"k\")\n\ndf_long$grid &lt;- as.numeric(df_long$grid)\ndf_long$sample &lt;- as.factor(df_long$sample)\ndf_long$k &lt;- as.numeric(df_long$k)\ndf_long &lt;- df_long %&gt;% mutate(norm_kern = k/6)\ncomp &lt;- df_long %&gt;% group_by(grid) %&gt;% summarize(composite=sum(k)/x_data_n)\nggplot() +\n  geom_line(data = df_long, aes(x = grid, y = norm_kern, color = sample, group = sample), alpha = 0.75) +\n  geom_line(data = comp, aes(x = grid, y = composite), alpha = 0.5) +\n  xlim(min_val, max_val) +\n  geom_vline(xintercept = c(65, 67, 75, 79, 81, 91), linetype = \"dotted\", size = 0.2) +\n  theme_minimal() +\n  ggtitle(\"Kernels at different data points\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n#df_long %&gt;% filter(sample==65) %&gt;% select(k) %&gt;% pull() %&gt;% hist()\n\n#estimated_pdf &lt;- function(n, kernel_matrix){\n#  return(rowSums(kernel_matrix)/n)\n#}\n\n\nx_data &lt;- sort(c(65, 75, 67, 79, 81, 91))\nx_data_sd &lt;- sd(x_data)\nx_data_iqr &lt;- IQR(x_data)\nx_data_n &lt;- length(x_data)\nh_silver &lt;- 0.9*x_data_n^(-1/5)*min(c(x_data_sd, x_data_iqr/1.34))\n\n\nxi &lt;- 65#x_data[1]\nh &lt;- 5.5 #h_silver\nmin_val &lt;- 40#min(x_data)-round(h)\nmax_val &lt;- 120#max(x_data)+round(h)\nx &lt;- seq(from = min_val, to = max_val, by = 1) # grid\n\n\n\nmanual2\n\n# Assume: x_grid and pdf_vals are already defined and same length\n\n# Approximate CDF using left Riemann sums\ndx &lt;- diff(x)            # Spacing between x points (assumes equally spaced)\ndx &lt;- c(dx, tail(dx, 1))      # Pad to same length\n\ncdf_from_pdf &lt;- cumsum(comp[['composite']] * dx)\n\nplot(x, cdf_from_pdf, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDFs Comparison\")\n\nlines(x, estimated_cdf, lwd = 2, col = \"red\", lty = 2)\n\n# Add empirical CDF for reference\nlines(x, ecdf(x_data)(x), col = \"darkgreen\", lwd = 2, lty = 3)\nlegend(\"bottomright\", legend = c(\"Numerical Integration\", \"Kernel CDF (pnorm)\", \"Empirical CDF\"),\nc(1, 2, 3), lwd = 2)\n\n\n\n\n\n\n\n#legend(\"bottomright\", legend = c(\"From PDF (numerical integration)\", \"From pnorm (kernel CDF)\"),\n    #   col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\nf &lt;- function(x) pnorm(x, mean = 0, sd = 1)\nf.inv &lt;- inverse(f,lower=-100,upper=100)\nf.inv(.2)\n\n[1] -0.8416276\n\n\n\n\n\n\nset.seed(1029)\nuninorm &lt;- rlnorm(100,0,1)\n\n\nnpout = nptol.int(uninorm, alpha = 0.05, P = 0.95, side = 2, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\nnpout\n\n  alpha    P 2-sided.lower 2-sided.upper\n1  0.05 0.95     0.1768112      5.782058\n\n\n\n# STEP 1\nlibrary(\"compositions\")\n\nWelcome to compositions, a package for compositional data analysis.\nFind an intro with \"? compositions\"\n\n\n\nAttaching package: 'compositions'\n\n\nThe following objects are masked from 'package:stats':\n\n    anova, cor, cov, dist, var\n\n\nThe following object is masked from 'package:graphics':\n\n    segments\n\n\nThe following objects are masked from 'package:base':\n\n    %*%, norm, scale, scale.default\n\ngamma &lt;- 0.95\nalpha &lt;- 0.05\nn&lt;-500\nMyVar &lt;- matrix(c(\n1,0.95,0.95,\n0.95,1,0.95,\n0.95,0.95,1),byrow=TRUE,nrow=3)\nMyMean &lt;- c(0,0,0)\np &lt;- 3\n\n#library(\"MASS\")\nset.seed(1724532)\ndatas &lt;- rnorm.rplus(n,MyMean,MyVar)\n#datas &lt;- mvrnorm(n,MyMean,MyVar, tol = 1e-06, empirical = FALSE)\ncolnames(datas) &lt;- c(\"p_1\", \"p_2\", \"p_3\")\nmat &lt;- cbind(grid = rownames(datas), datas)\nrownames(datas) &lt;- sapply(1:n, function(i) paste0(\"n_\", sprintf(\"%03d\", i)))\n\nh &lt;- find_silver_bw(datas[,1])\nx &lt;- get_grid(datas[,1])\n\n#p1_outmat &lt;-sapply(datas[,1], function(xi) gaussian_kernel(xi = xi, x = x, h = h))\n#dx &lt;- diff(x)\n#dx &lt;- c(dx, tail(dx, 1))\n\nestimated_cdf &lt;- kde_cdf(x, datas[,1], h)\n\n#cdf_vals &lt;- cumsum(estimated_pdf(n ,p1_outmat) * dx) #ESTIMATED CDF\n\nplot(x, estimated_cdf, type = \"l\", lwd = 2,\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDF\")\n\n\n\n\n\n\n\nplot(x, estimated_cdf, type = \"p\",\n     xlab = \"x\", ylab = \"Estimated CDF\",\n     main = \"Estimated CDF\")\n\n\n\n\n\n\n\n\n\nestimated_density &lt;- kde_manual(x, datas[,1], h)\nplot(x, estimated_density, type = \"l\", lwd = 2,\n      xlab = \"x\", ylab = \"Estimated CDF\",\n      main = \"Estimated PDF\")\n\n\n\n\n\n\n\nhist(estimated_density)\n\n\n\n\n\n\n\n\n\nget_upper_limit &lt;- function(p_data){\n  h &lt;- find_silver_bw(p_data)\n  #x &lt;- get_grid(p_data)\n  #estimated_cdf &lt;- kde_cdf(x, p_data, h)\n  #transformed_data &lt;- transform_data(p_data, estimated_cdf, x)\n  \n\n  #U_mat &lt;- kde_cdf(p_data, p_data, h)\n  #U_mat &lt;- sapply(p_data, function(i) kde_cdf(i, p_data, h))\n  \n  \n  cdf_j &lt;- kde_cdf_function(p_data, h)\n  U_mat &lt;- sapply(p_data, cdf_j)\n  \n  # Step 4: Compute Y_ij = max{U_ij, 1 - U_ij}\n  Y_mat &lt;- pmax(U_mat, 1 - U_mat)\n  \n  #u&lt;-data.frame(original = transformed_data, complement = 1 - transformed_data)\n  #y1 &lt;- apply(u, 1, max)\n  npout = nptol.int(Y_mat, alpha = alpha, P = gamma, side = 1, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n  return(npout$`1-sided.upper`)\n}\n\nget_upper_limit(datas[,1])\n\n[1] 0.9665198\n\n\n\n# step 5\nk_j &lt;- apply(datas, 2, get_upper_limit)\n\n\nk_j\n\n      p_1       p_2       p_3 \n0.9665198 0.9654732 0.9674770 \n\n\n\n# step 6\nmax(k_j)\n\n[1] 0.967477\n\n\n\n# step 7\n\n#p_data &lt;- datas#[,1]\n\n# Custom CDF function\nlibrary(\"GoFKernel\")\nkde_cdf_function &lt;- function(datapoints, bandwidth) {\n  function(x) mean(pnorm((x - datapoints) / bandwidth))\n}\n\nget_tolerance_interval &lt;- function(p_data, maxk=max(k_j)){\n  h &lt;- find_silver_bw(p_data)\n  cdf_fun &lt;- kde_cdf_function(p_data,h)\n\n  #f.inv &lt;- inverse(function(i) kde_cdf(i, p_data, h),#kde_cdf(i, p_data, h),\n  #                 lower=-100,upper=100)\n  inv_cdf &lt;- inverse(cdf_fun,lower=min(p_data) - 3 * h,upper=max(p_data) + 3 * h)\n  lower_bound &lt;- inv_cdf(1 - maxk)\n  upper_bound &lt;- inv_cdf(maxk)\n\n\n  #print(f.inv(1 - maxk))\n  return(c(lower_bound, upper_bound))\n}\n\n\nSIM\n\n# STEP 1\nlibrary(\"compositions\")\nlibrary(\"MASS\")\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ngamma &lt;- 0.95\nalpha &lt;- 0.05\nn &lt;- 500\nMyVar &lt;- matrix(c(\n1,0.95,0.95,\n0.95,1,0.95,\n0.95,0.95,1),byrow=TRUE,nrow=3)\nMyMean &lt;- c(0,0,0)\np &lt;- 3\n\nset.seed(7778)\n\n# STEP 1: Generate the random sample X1,X2,:::,Xn from the multivariate lognormal distribution\nset.seed(123) # for reproducibility\nlog_data &lt;- mvrnorm(n = n, mu = MyMean, Sigma = MyVar)\n\n# Convert to lognormal data by exponentiation\ndatas &lt;- exp(log_data)\n\n# Optional: Treat the data as compositional (e.g., close parts to sum to 1)\n# For example, using acomp() from compositions package\n#comp_data &lt;- acomp(lognormal_data)\n\n\nX&lt;-datas\nhist(X[,1], col = rgb(1, 0, 0, 0.4), xlim = range(datas), main = \"Overlaid Histograms\", xlab = \"Values\")\nhist(X[,2], col = rgb(0, 1, 0, 0.4), add = TRUE)\nhist(X[,3], col = rgb(0, 0, 1, 0.4), add = TRUE)\nlegend(\"topright\", legend = c(\"1\", \"2\", \"3\"),\n       fill = c(rgb(1, 0, 0, 0.4), rgb(0, 1, 0, 0.4), rgb(0, 0, 1, 0.4)))\n\n\n\n\n\n\n\ncolnames(X) &lt;- c(\"p_1\", \"p_2\", \"p_3\")\nmat &lt;- cbind(grid = rownames(X), X)\nrownames(X) &lt;- sapply(1:n, function(i) paste0(\"n_\", sprintf(\"%03d\", i)))\n\n\npar(mfrow = c(1, 3))\nhist((X[,1]), main = \"1\", xlab = \"LOG Values\", col = \"skyblue\")\nhist((X[,2]), main = \"2\", xlab = \"LOG Values\", col = \"salmon\")\nhist((X[,3]), main = \"3\", xlab = \"LOG Values\", col = \"lightgreen\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\nk_j &lt;- apply(X, 2, get_upper_limit)\nk_j\n\n      p_1       p_2       p_3 \n0.9673421 0.9651831 0.9663632 \n\nmaxkj &lt;- max(k_j)\nmax(k_j)\n\n[1] 0.9673421\n\n\n\ntols &lt;- apply(X, 2, function(s) get_tolerance_interval(s,maxk=maxkj))\ntols\n\n             p_1         p_2          p_3\n[1,] -0.04147051 -0.02162749 -0.004976622\n[2,]  5.57903556  5.47783908  5.764448735\n\n\n\nsummary(datas)\n\n       V1                V2                 V3          \n Min.   : 0.0411   Min.   : 0.04218   Min.   : 0.04067  \n 1st Qu.: 0.5153   1st Qu.: 0.50900   1st Qu.: 0.52154  \n Median : 1.0054   Median : 0.95882   Median : 0.99798  \n Mean   : 1.5296   Mean   : 1.53999   Mean   : 1.51669  \n 3rd Qu.: 1.9128   3rd Qu.: 1.82951   3rd Qu.: 1.78991  \n Max.   :16.4272   Max.   :14.63768   Max.   :17.19995  \n\n\n\np_data &lt;- datas[,1]\nh &lt;- find_silver_bw(p_data)\n\n#f.inv &lt;- inverse(function(i) kde_cdf(i, p_data, h),#kde_cdf(i, p_data, h),\n#                 lower=-100,upper=100)\nf.inv &lt;- inverse(kde_cdf_function(p_data,h),lower=-100,upper=100)\nlower_bound &lt;- f.inv(1 - maxkj)\nupper_bound &lt;- f.inv(maxkj)\nlower_bound;upper_bound \n\n[1] -0.04147688\n\n\n[1] 5.579051\n\n\n\nstep3 &lt;- function(i){\n  return(plnorm(tols[2,i],MyMean[i],MyVar[i,i]) - plnorm(tols[1,i],MyMean[i],MyVar[i,i]))\n}\n\nstep4 &lt;- function(i){\n  return(tols[2,i] - tols[1,i])\n}\n\n\nI &lt;- min(sapply(1:3, step3)) #&gt;= gamma\nI\n\n[1] 0.9555013\n\n#Lj &lt;- sapply(1:3, step4)\n\n\n# Bonferoni\n\nbonferonni &lt;- function(p_data) {\n  \n  if (length(p_data) &gt;= 119) {\n    to &lt;- nptol.int(p_data, alpha = alpha/3, P = gamma, side = 2, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n\n    lower_bound &lt;- to$`2-sided.lower`\n    upper_bound &lt;- to$`2-sided.upper`\n      \n  }\n  \n  else {\n    lower_bound &lt;- min(p_data)\n    upper_bound &lt;- max(p_data)\n  }\n\n  return(c(lower_bound, upper_bound))\n}\n\n\nrun_simulation &lt;- function(n, MyMean, MyVar, gamma = 0.90, n_sim = 5000) {\n  results &lt;- replicate(n_sim, {\n    log_data &lt;- MASS::mvrnorm(n = n, mu = MyMean, Sigma = MyVar)\n    datas &lt;- exp(log_data)\n    colnames(datas) &lt;- paste0(\"p_\", seq_along(MyMean))\n    rownames(datas) &lt;- sprintf(\"n_%03d\", seq_len(n))\n    \n    k_j &lt;- apply(datas, 2, get_upper_limit)\n    tols &lt;- apply(datas, 2, function(s) get_tolerance_interval(s, maxk = max(k_j)))\n    bonf &lt;- apply(datas, 2, bonferonni)\n\n    step3 &lt;- function(i){\n      plnorm(tols[2, i], mean = MyMean[i], sd = sqrt(MyVar[i, i])) -\n        plnorm(tols[1, i], mean = MyMean[i], sd = sqrt(MyVar[i, i]))\n    }\n    \n    step4 &lt;- function(i){\n      tols[2, i] - tols[1, i]\n    }\n\n    step3b &lt;- function(i){\n      plnorm(bonf[2, i], mean = MyMean[i], sd = sqrt(MyVar[i, i])) -\n        plnorm(bonf[1, i], mean = MyMean[i], sd = sqrt(MyVar[i, i]))\n    }\n    \n    step4b &lt;- function(i){\n      bonf[2, i] - bonf[1, i]\n    }\n\n    I &lt;- min(sapply(seq_along(MyMean), step3)) &gt;= gamma\n    Lj &lt;- sapply(seq_along(MyMean), step4)\n    \n    Ib &lt;- min(sapply(seq_along(MyMean), step3b)) &gt;= gamma\n    Ljb &lt;- sapply(seq_along(MyMean), step4b)\n\n    list(Lj = Lj, I = I, Ljb = Ljb, Ib = Ib)\n  }, simplify = FALSE)\n\n  # Collect and summarize results\n  I_vals &lt;- sapply(results, function(res) res$I)\n  Lj_vals &lt;- t(sapply(results, function(res) res$Lj))\n  Ib_vals &lt;- sapply(results, function(res) res$Ib)\n  Ljb_vals &lt;- t(sapply(results, function(res) res$Ljb))\n\n  list(\n    mean_I = mean(I_vals),\n    colMeans_Lj = colMeans(Lj_vals),\n    mean_Ib = mean(Ib_vals),\n    colMeans_Ljb = colMeans(Ljb_vals),\n    raw_results = results # optional if you want full access later\n  )\n}\n\n\np &lt;- 3  # or any integer\n\n# Sigma_2 = 0.5 * I_p + 0.5 * 1_p 1_p'\nSigma_2 &lt;- 0.5 * diag(p) + 0.5 * matrix(1, p, p)\n\n# Sigma_3 = 0.8 * I_p + 0.2 * 1_p 1_p'\nSigma_3 &lt;- 0.8 * diag(p) + 0.2 * matrix(1, p, p)\n\n\n# Sigma_4\nSigma_4 &lt;- matrix(c(\n  1,   -0.95, -0.95,\n -0.95, 1,     0.95,\n -0.95, 0.95,  1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_5\nSigma_5 &lt;- matrix(c(\n  1,  -0.5, -0.5,\n -0.5,  1,   0.5,\n -0.5,  0.5, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_6\nSigma_6 &lt;- matrix(c(\n  1,  -0.2, -0.2,\n -0.2,  1,   0.2,\n -0.2,  0.2, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_7\nSigma_7 &lt;- matrix(c(\n  1,   0.9,  0.5,\n  0.9, 1,    0.1,\n  0.5, 0.1,  1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_8\nSigma_8 &lt;- matrix(c(\n  1,   0.5,  0.5,\n  0.5, 1,    0.95,\n  0.5, 0.95, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_9\nSigma_9 &lt;- matrix(c(\n  1,   0.2,  0.2,\n  0.2, 1,    0.95,\n  0.2, 0.95, 1\n), nrow = 3, byrow = TRUE)\n\n# Sigma_10\nSigma_10 &lt;- matrix(c(\n  1,   -0.5, -0.5,\n -0.5,  1,    0.95,\n -0.5,  0.95, 1\n), nrow = 3, byrow = TRUE)\n\n\n# library(foreach)\n# library(doParallel)\n# # Detect available cores and register parallel backend\n# n_cores &lt;- parallel::detectCores() - 1\n# cl &lt;- makeCluster(n_cores)\n# registerDoParallel(cl)\n\n\nget_sigma_matrices &lt;- function(p = 3) {\n  list(\n    Sigma_1 = matrix(c(\n      1,0.95,0.95,\n      0.95,1,0.95,\n      0.95,0.95,1),byrow=TRUE,nrow=3),\n    Sigma_2  = 0.5 * diag(p) + 0.5 * matrix(1, p, p),\n    Sigma_3  = 0.8 * diag(p) + 0.2 * matrix(1, p, p),\n    Sigma_4  = matrix(c(1, -0.95, -0.95, -0.95, 1, 0.95, -0.95, 0.95, 1), 3, 3, byrow=TRUE),\n    Sigma_5  = matrix(c(1, -0.5, -0.5, -0.5, 1, 0.5, -0.5, 0.5, 1), 3, 3, byrow=TRUE),\n    Sigma_6  = matrix(c(1, -0.2, -0.2, -0.2, 1, 0.2, -0.2, 0.2, 1), 3, 3, byrow=TRUE),\n    Sigma_7  = matrix(c(1, 0.9, 0.5, 0.9, 1, 0.1, 0.5, 0.1, 1), 3, 3, byrow=TRUE),\n    Sigma_8  = matrix(c(1, 0.5, 0.5, 0.5, 1, 0.95, 0.5, 0.95, 1), 3, 3, byrow=TRUE),\n    Sigma_9  = matrix(c(1, 0.2, 0.2, 0.2, 1, 0.95, 0.2, 0.95, 1), 3, 3, byrow=TRUE),\n    Sigma_10 = matrix(c(1, -0.5, -0.5, -0.5, 1, 0.95, -0.5, 0.95, 1), 3, 3, byrow=TRUE)\n  )\n}\n\n\n# MyMean &lt;- rep(0, 3)\n# gamma &lt;- 0.95\n# sigma_list &lt;- get_sigma_matrices()\n# n_values &lt;- c(100, 300, 500)\n# \n# results_df &lt;- foreach(n = n_values, .combine = rbind, .packages = c(\"MASS\", \"GoFKernel\", \"tolerance\")) %:%\n#   foreach(sigma_name = names(sigma_list), .combine = rbind) %dopar% {\n#     MyVar &lt;- sigma_list[[sigma_name]]\n#     res &lt;- run_simulation(n, MyMean, MyVar, gamma, n_sim = 5000)\n# \n#     # Create data.frames for proposed and bonferroni methods\n#     proposed_row &lt;- data.frame(\n#       n = n,\n#       gamma = gamma,\n#       sigma = sigma_name,\n#       method = \"proposed\",\n#       mean_I = res$mean_I,\n#       Lj_1 = res$colMeans_Lj[1],\n#       Lj_2 = res$colMeans_Lj[2],\n#       Lj_3 = res$colMeans_Lj[3]\n#     )\n# \n#     bonf_row &lt;- data.frame(\n#       n = n,\n#       gamma = gamma,\n#       sigma = sigma_name,\n#       method = \"bonferroni\",\n#       mean_I = res$mean_Ib,\n#       Lj_1 = res$colMeans_Ljb[1],\n#       Lj_2 = res$colMeans_Ljb[2],\n#       Lj_3 = res$colMeans_Ljb[3]\n#     )\n# \n#     rbind(proposed_row, bonf_row)\n#   }\n\n#stopCluster(cl)\n\n\nload(\"results_df.RData\")\nresults_df\n\n         n gamma    sigma     method mean_I      Lj_1      Lj_2      Lj_3\np_1    100  0.95  Sigma_1   proposed 0.9414  9.432202  9.439163  9.436803\np_11   100  0.95  Sigma_1 bonferroni 0.9218 13.351928 13.385767 13.424624\np_12   100  0.95  Sigma_2   proposed 0.9174  9.393701  9.448329  9.468494\np_111  100  0.95  Sigma_2 bonferroni 0.8936 13.300489 13.375498 13.469624\np_13   100  0.95  Sigma_3   proposed 0.9118  9.417383  9.355093  9.488714\np_112  100  0.95  Sigma_3 bonferroni 0.8952 13.514072 13.539249 13.607965\np_14   100  0.95  Sigma_4   proposed 0.9214  9.442166  9.428619  9.410113\np_113  100  0.95  Sigma_4 bonferroni 0.9150 13.397610 13.483725 13.504636\np_15   100  0.95  Sigma_5   proposed 0.9172  9.508856  9.453177  9.408880\np_114  100  0.95  Sigma_5 bonferroni 0.8932 13.547451 13.607561 13.320073\np_16   100  0.95  Sigma_6   proposed 0.9182  9.437318  9.381537  9.388663\np_115  100  0.95  Sigma_6 bonferroni 0.8864 13.457183 13.359136 13.412068\np_17   100  0.95  Sigma_7   proposed 0.9286  9.479048  9.490746  9.487169\np_116  100  0.95  Sigma_7 bonferroni 0.8978 13.601785 13.501508 13.512159\np_18   100  0.95  Sigma_8   proposed 0.9334  9.440805  9.425150  9.420785\np_117  100  0.95  Sigma_8 bonferroni 0.9092 13.525467 13.447443 13.531976\np_19   100  0.95  Sigma_9   proposed 0.9244  9.459112  9.430767  9.371577\np_118  100  0.95  Sigma_9 bonferroni 0.8996 13.531866 13.370461 13.400610\np_110  100  0.95 Sigma_10   proposed 0.9292  9.498593  9.389817  9.433560\np_119  100  0.95 Sigma_10 bonferroni 0.9038 13.670953 13.538172 13.511211\np_120  300  0.95  Sigma_1   proposed 0.9486  7.008602  7.008663  7.006469\np_1110 300  0.95  Sigma_1 bonferroni 0.9658  9.705779  9.664276  9.671580\np_121  300  0.95  Sigma_2   proposed 0.9364  6.995565  6.981772  7.018299\np_1111 300  0.95  Sigma_2 bonferroni 0.9526  9.690747  9.666004  9.669688\np_131  300  0.95  Sigma_3   proposed 0.9322  6.989780  6.987645  6.997938\np_1121 300  0.95  Sigma_3 bonferroni 0.9474  9.691351  9.661174  9.701256\np_141  300  0.95  Sigma_4   proposed 0.9370  6.984812  6.997522  6.995045\np_1131 300  0.95  Sigma_4 bonferroni 0.9592  9.652083  9.671698  9.654119\np_151  300  0.95  Sigma_5   proposed 0.9356  7.012250  7.005759  6.988130\np_1141 300  0.95  Sigma_5 bonferroni 0.9532  9.702445  9.639717  9.643229\np_161  300  0.95  Sigma_6   proposed 0.9394  7.001247  7.002618  6.997710\np_1151 300  0.95  Sigma_6 bonferroni 0.9552  9.672447  9.693714  9.681670\np_171  300  0.95  Sigma_7   proposed 0.9376  6.984042  7.002154  6.979538\np_1161 300  0.95  Sigma_7 bonferroni 0.9538  9.630017  9.697683  9.676495\np_181  300  0.95  Sigma_8   proposed 0.9388  7.000826  7.021135  7.006591\np_1171 300  0.95  Sigma_8 bonferroni 0.9594  9.668751  9.702430  9.664471\np_191  300  0.95  Sigma_9   proposed 0.9394  6.984444  6.981461  6.985158\np_1181 300  0.95  Sigma_9 bonferroni 0.9632  9.677808  9.666808  9.650490\np_1101 300  0.95 Sigma_10   proposed 0.9428  6.979555  7.000865  6.994919\np_1191 300  0.95 Sigma_10 bonferroni 0.9594  9.628663  9.670059  9.691317\np_122  500  0.95  Sigma_1   proposed 0.9478  6.442213  6.431800  6.431035\np_1112 500  0.95  Sigma_1 bonferroni 0.9872  9.220788  9.215836  9.216843\np_123  500  0.95  Sigma_2   proposed 0.9356  6.451498  6.436058  6.421873\np_1113 500  0.95  Sigma_2 bonferroni 0.9816  9.229456  9.256825  9.177914\np_132  500  0.95  Sigma_3   proposed 0.9344  6.442507  6.442941  6.479097\np_1122 500  0.95  Sigma_3 bonferroni 0.9846  9.204266  9.240803  9.268556\np_142  500  0.95  Sigma_4   proposed 0.9436  6.444022  6.450989  6.451312\np_1132 500  0.95  Sigma_4 bonferroni 0.9888  9.224976  9.242345  9.259192\np_152  500  0.95  Sigma_5   proposed 0.9332  6.436432  6.439831  6.426897\np_1142 500  0.95  Sigma_5 bonferroni 0.9854  9.187284  9.222307  9.224303\np_162  500  0.95  Sigma_6   proposed 0.9304  6.436484  6.419339  6.430151\np_1152 500  0.95  Sigma_6 bonferroni 0.9810  9.230408  9.183674  9.186471\np_172  500  0.95  Sigma_7   proposed 0.9426  6.458407  6.445668  6.435083\np_1162 500  0.95  Sigma_7 bonferroni 0.9842  9.260465  9.229504  9.203002\np_182  500  0.95  Sigma_8   proposed 0.9404  6.444871  6.446294  6.442985\np_1172 500  0.95  Sigma_8 bonferroni 0.9834  9.235269  9.234818  9.233284\np_192  500  0.95  Sigma_9   proposed 0.9390  6.423952  6.436780  6.442117\np_1182 500  0.95  Sigma_9 bonferroni 0.9874  9.228690  9.245564  9.252124\np_1102 500  0.95 Sigma_10   proposed 0.9396  6.444146  6.430918  6.436981\np_1192 500  0.95 Sigma_10 bonferroni 0.9850  9.228618  9.210389  9.209842\n\n\n\nmk &lt;- get_upper_limit(uninorm)\nget_tolerance_interval(uninorm,mk)\n\n[1] -0.2003685  4.9905541\n\nhist(log(uninorm))\n\n\n\n\n\n\n\nnptol.int(uninorm, alpha = 0.05, P = 0.95, side = 2, method = \"WILKS\",\n                  upper = NULL, lower = NULL)\n\n  alpha    P 2-sided.lower 2-sided.upper\n1  0.05 0.95     0.1768112      5.782058\n\nhist((uninorm))\n\n\n\n\n\n\n\n\n\nhist(kde_cdf(datas[,1], datas[,1], h), probability = TRUE, \n     main = \"Histogram of PIT-transformed values\",\n     xlab = \"PIT values\")\nabline(h = 1, col = \"red\", lwd = 2) \n\n\n\n\n\n\n\n\n\n# generated from a lognormal dist\nhist(datas[,1])\n\n\n\n\n\n\n\n\n\n# taking the log of a variate from lognormal dist makes it normally distributed\nhist(log(datas[,1]))\n\n\n\n\n\n\n\n\nProbability Integral Transform"
  },
  {
    "objectID": "initial_notes.html#lognormal-normal-distribution",
    "href": "initial_notes.html#lognormal-normal-distribution",
    "title": "Prerequisite Studies",
    "section": "",
    "text": "\\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) means \\(X\\) is normally distributed.\n\\(Y \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\) means \\(\\log Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nA lognormal random variable is one whose logarithm is normally distributed.\nIf you assume the data itself is normal (norm): The raw data \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nIf you assume the log of the data is normal (lnorm): You are modeling \\(\\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) so \\(X \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\)\nMean is zero in the log scale means \\(\\log(Y) \\sim \\mathcal{N}(0, \\sigma^2)\\) which is equivalent to \\(Y \\sim \\text{Lognormal}(\\mu, \\sigma^2)\\)\nThis also tells that the mean of \\(Y\\) is not 0. In fact, \\(\\mathbb{E}[Y] =  e^{\\mu+\\sigma^2/2}\\) so if \\(\\mu = 0\\) then, \\(\\mathbb{E}[Y] =  e^{\\sigma^2/2} &gt; 1\\)\n\nset.seed(5)\nlog_x &lt;- rnorm(1000, 0, 1)\nset.seed(5)\nlnorm_x &lt;- rlnorm(1000, 0, 1)\n\npar(mfrow = c(1, 2))\nhist(log_x, main = \"Histogram: log(x)\", xlab = \"log(x)\")\n\nhist(exp(log_x), main = \"exp(log_x) & lnorm_x\", xlab = \"exp(log_x) & lnorm(x)\",\n     col = rgb(1, 0, 0, 0.5), xlim = range(c(exp(log_x), lnorm_x)), breaks = 30)\n\n# Overlay histogram of lnorm_x\nhist(lnorm_x, col = rgb(0, 0, 1, 0.3), add = TRUE, breaks = 30)\n\n# Add a legend\nlegend(\"topright\", legend = c(\"log_y\", \"lnorm_x\"),\n       fill = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.2)))\n\n\n\n\n\n\n\n\n\nsum(exp(log_x)==lnorm_x)\n\n[1] 1000"
  },
  {
    "objectID": "initial_notes.html#probability-integral-transform",
    "href": "initial_notes.html#probability-integral-transform",
    "title": "Prerequisite Studies",
    "section": "",
    "text": "hist(kde_cdf(exp(log_x), exp(log_x), find_silver_bw(exp(log_x))), probability = TRUE, \n     main = \"Histogram of PIT-transformed values\",\n     xlab = \"PIT values\")\nabline(h = 1, col = \"red\", lwd = 2) \n\n\n\n\n\n\n\n\nProbability Integral Transform"
  },
  {
    "objectID": "initial_notes.html#simulation-a-nonparametric-two-sided-stis",
    "href": "initial_notes.html#simulation-a-nonparametric-two-sided-stis",
    "title": "Prerequisite Studies",
    "section": "Simulation A: Nonparametric two-sided STIs",
    "text": "Simulation A: Nonparametric two-sided STIs\n\nbonferonni &lt;- function(p_data) {\n  if (length(p_data) &gt;= 119) {\n    to &lt;- nptol.int(p_data, alpha = alpha / 3, P = gamma, side = 2, method = \"WILKS\",\n                    upper = NULL, lower = NULL)\n    lower_bound &lt;- to$`2-sided.lower`\n    upper_bound &lt;- to$`2-sided.upper`\n  } else {\n    lower_bound &lt;- min(p_data)\n    upper_bound &lt;- max(p_data)\n  }\n\n  return(setNames(c(lower_bound, upper_bound), c(\"c_j\", \"d_j\")))\n}\n\nbonf_results &lt;- lapply(seq_len(ncol(X)), function(j) {\n  bonferonni(X[, j])\n})\n\nnames(bonf_results) &lt;- paste0(\"Bonf_i\", seq_along(bonf_results))\n\n\nget_TI_info &lt;- function(tol_int, mu, var) {\n  tol_int &lt;- as.data.frame(tol_int)\n  info &lt;- lapply(seq_along(tol_int), function(j) {\n    lower &lt;- tol_int[1,j]\n    upper &lt;- tol_int[2,j]\n    \n    # Coverage on CDF scale (difference of estimated quantiles)\n    ti_length &lt;- as.numeric(upper - lower)\n    \n    # TI length on original/lognormal scale\n    coverage &lt;- plnorm(upper, meanlog = mu[j], sdlog = sqrt(var[j, j])) -\n                 plnorm(lower, meanlog = mu[j], sdlog = sqrt(var[j, j]))\n\n    # Return named vector\n    setNames(c(ti_length, coverage), c(\"TI_length\", \"Coverage\"))\n  })\n\n  # Name each component list\n  names(info) &lt;- paste0(\"p_\", seq_along(tol_int))\n  return(info)\n}\n\nget_TI_info(cj_dj, MyMean, MyVar)\n\n$p_1\nTI_length  Coverage \n5.6205282 0.9571945 \n\n$p_2\nTI_length  Coverage \n5.4994908 0.9555015 \n\n$p_3\nTI_length  Coverage \n5.7694379 0.9600882 \n\nget_TI_info(bonf_results, MyMean, MyVar)\n\n$p_1\nTI_length  Coverage \n7.3146278 0.9646748 \n\n$p_2\nTI_length  Coverage \n9.6564636 0.9750563 \n\n$p_3\nTI_length  Coverage \n7.1378655 0.9628328 \n\n\n\n# Assuming get_TI_info(), cj_dj, bonf_results, MyMean, MyVar are defined\n\n# Run get_TI_info on both lists\nti_info_cj_dj &lt;- get_TI_info(cj_dj, MyMean, MyVar)\nti_info_bonf &lt;- get_TI_info(bonf_results, MyMean, MyVar)\n\n# Combine with prefixed names\ncombined &lt;- c(\n  setNames(ti_info_cj_dj, paste0(\"cj_dj_\", names(ti_info_cj_dj))),\n  setNames(ti_info_bonf, paste0(\"bonf_\", names(ti_info_bonf)))\n)\n\n# Convert to a data frame (base R)\ndf_combined &lt;- do.call(\n  rbind,\n  lapply(names(combined), function(nm) {\n    vals &lt;- combined[[nm]]\n    data.frame(\n      source = nm,\n      TI_length = vals[\"TI_length\"],\n      Coverage = vals[\"Coverage\"],\n      row.names = NULL\n    )\n  })\n)\n\n# View result\nprint(df_combined)\n\n     source TI_length  Coverage\n1 cj_dj_p_1  5.620528 0.9571945\n2 cj_dj_p_2  5.499491 0.9555015\n3 cj_dj_p_3  5.769438 0.9600882\n4  bonf_p_1  7.314628 0.9646748\n5  bonf_p_2  9.656464 0.9750563\n6  bonf_p_3  7.137866 0.9628328\n\n\n\nrun_simulation &lt;- function(n, MyMean, MyVar, gamma = 0.90, n_sim = 100) {\n  results &lt;- replicate(n_sim, {\n    # Step 1: Simulate lognormal data\n    log_data &lt;- MASS::mvrnorm(n = n, mu = MyMean, Sigma = MyVar)\n    datas &lt;- exp(log_data)\n    colnames(datas) &lt;- paste0(\"p_\", seq_along(MyMean))\n    rownames(datas) &lt;- sprintf(\"n_%03d\", seq_len(n))\n    \n    # Step 2: Compute tolerance intervals\n    k_j &lt;- apply(datas, 2, get_upper_limit)\n    tols &lt;- apply(datas, 2, function(s) get_tolerance_interval(s, maxk = max(k_j)))\n    bonf &lt;- apply(datas, 2, bonferonni)\n\n    # Step 3: Use get_ti_info to compute coverage and width\n    proposed_info &lt;- as.data.frame(get_TI_info(\n        tols,\n        MyMean,\n        MyVar\n      ))\n  \n    bonf_info &lt;- as.data.frame(get_TI_info(\n        bonf,\n        MyMean,\n        MyVar\n      ))\n\n    # Step 4: Aggregate results\n    I  &lt;- min(proposed_info[\"Coverage\", ]) &gt;= gamma\n    \n    Lj &lt;- unlist(proposed_info[1, ])\n    Ib  &lt;- min(bonf_info[\"Coverage\", ]) &gt;= gamma\n    Ljb &lt;- unlist(bonf_info[\"TI_length\", ])\n    list(Lj = Lj, I = I, Ljb = Ljb, Ib = Ib)\n  }, simplify = FALSE)\n\n  # Step 5: Summarize results\n\n  I_vals   &lt;- sapply(results, function(res) res$I)\n  Lj_vals  &lt;- t(sapply(results, function(res) res$Lj))\n  Ib_vals  &lt;- sapply(results, function(res) res$Ib)\n  Ljb_vals &lt;- t(sapply(results, function(res) res$Ljb))\n\n  list(\n    mean_I = mean(I_vals),\n    colMeans_Lj = colMeans(Lj_vals),\n    mean_Ib = mean(Ib_vals),\n    colMeans_Ljb = colMeans(Ljb_vals),\n    raw_results = results  # Optional: retain raw simulation results\n  )\n}\n\n\nlibrary(foreach)\nlibrary(doParallel)\n# Detect available cores and register parallel backend\nn_cores &lt;- parallel::detectCores() - 1\ncl &lt;- makeCluster(n_cores)\nregisterDoParallel(cl)\n\np &lt;- 2\nMyMean &lt;- c(0,0)\ngamma &lt;- 0.95\nsigma_list &lt;- get_sigma_matrices(p=2)\nn_values &lt;- c(100, 300, 500)\n\nresults_df &lt;- foreach(n = n_values, .combine = rbind, .packages = c(\"MASS\", \"GoFKernel\", \"tolerance\")) %:%\n  foreach(sigma_name = names(sigma_list), .combine = rbind) %dopar% {\n    MyVar &lt;- sigma_list[[sigma_name]]\n    res &lt;- run_simulation(n, MyMean, MyVar, gamma, n_sim = 5000)\n\n    # Create data.frames for proposed and bonferroni methods\n    proposed_row &lt;- data.frame(\n      n = n,\n      gamma = gamma,\n      sigma = sigma_name,\n      method = \"proposed\",\n      mean_I = res$mean_I,\n      Lj_1 = res$colMeans_Lj[1],\n      Lj_2 = res$colMeans_Lj[2]\n    )\n\n    bonf_row &lt;- data.frame(\n      n = n,\n      gamma = gamma,\n      sigma = sigma_name,\n      method = \"bonferroni\",\n      mean_I = res$mean_Ib,\n      Lj_1 = res$colMeans_Ljb[1],\n      Lj_2 = res$colMeans_Ljb[2]\n    )\n\n    rbind(proposed_row, bonf_row)\n  }\n\nstopCluster(cl)"
  },
  {
    "objectID": "initial_notes.html#results",
    "href": "initial_notes.html#results",
    "title": "Prerequisite Studies",
    "section": "Results",
    "text": "Results\n\np = 3 Two sided\n\nload(\"rdata/results_df_p3.RData\")\n\nprint(results_df_p3, row.names = FALSE)\n\n   n gamma    sigma     method mean_I      Lj_1      Lj_2      Lj_3\n 100  0.95 Sigma_01   proposed 0.9400  9.496373  9.426451  9.457114\n 100  0.95 Sigma_01 bonferroni 0.9246 13.529148 13.512152 13.581769\n 100  0.95 Sigma_02   proposed 0.9224  9.476397  9.454583  9.429717\n 100  0.95 Sigma_02 bonferroni 0.8980 13.557224 13.513533 13.567895\n 100  0.95 Sigma_03   proposed 0.9092  9.436488  9.456816  9.514163\n 100  0.95 Sigma_03 bonferroni 0.8908 13.437192 13.560851 13.440324\n 100  0.95 Sigma_04   proposed 0.9252  9.505402  9.410441  9.407397\n 100  0.95 Sigma_04 bonferroni 0.9184 13.540863 13.427347 13.364099\n 100  0.95 Sigma_05   proposed 0.9218  9.414336  9.385001  9.469016\n 100  0.95 Sigma_05 bonferroni 0.8900 13.451716 13.405573 13.382672\n 100  0.95 Sigma_06   proposed 0.9162  9.456062  9.448307  9.417581\n 100  0.95 Sigma_06 bonferroni 0.8980 13.635608 13.498049 13.417974\n 100  0.95 Sigma_07   proposed 0.9252  9.485933  9.463129  9.435450\n 100  0.95 Sigma_07 bonferroni 0.9104 13.610213 13.475205 13.546910\n 100  0.95 Sigma_08   proposed 0.9352  9.421951  9.449796  9.536372\n 100  0.95 Sigma_08 bonferroni 0.9088 13.509756 13.621758 13.627901\n 100  0.95 Sigma_09   proposed 0.9326  9.498207  9.451215  9.457437\n 100  0.95 Sigma_09 bonferroni 0.9084 13.641917 13.553537 13.508107\n 100  0.95 Sigma_10   proposed 0.9306  9.501163  9.443952  9.466741\n 100  0.95 Sigma_10 bonferroni 0.9080 13.568412 13.560501 13.618996\n 300  0.95 Sigma_01   proposed 0.9546  6.987736  6.975796  6.975661\n 300  0.95 Sigma_01 bonferroni 0.9624  9.604331  9.630227  9.636809\n 300  0.95 Sigma_02   proposed 0.9324  6.995183  7.014769  6.989628\n 300  0.95 Sigma_02 bonferroni 0.9532  9.689917  9.711909  9.695054\n 300  0.95 Sigma_03   proposed 0.9352  7.011311  7.007623  6.997655\n 300  0.95 Sigma_03 bonferroni 0.9578  9.694466  9.653752  9.692042\n 300  0.95 Sigma_04   proposed 0.9426  7.015959  6.980505  6.978948\n 300  0.95 Sigma_04 bonferroni 0.9662  9.718292  9.642310  9.626886\n 300  0.95 Sigma_05   proposed 0.9356  6.999507  7.007756  6.995460\n 300  0.95 Sigma_05 bonferroni 0.9536  9.677869  9.703684  9.696216\n 300  0.95 Sigma_06   proposed 0.9360  6.977905  6.999628  7.010432\n 300  0.95 Sigma_06 bonferroni 0.9524  9.629321  9.695132  9.695546\n 300  0.95 Sigma_07   proposed 0.9354  7.004488  6.982654  6.996523\n 300  0.95 Sigma_07 bonferroni 0.9516  9.670867  9.652987  9.690976\n 300  0.95 Sigma_08   proposed 0.9454  6.991545  7.010114  7.007085\n 300  0.95 Sigma_08 bonferroni 0.9572  9.653466  9.726920  9.712015\n 300  0.95 Sigma_09   proposed 0.9448  7.003142  7.012875  7.023069\n 300  0.95 Sigma_09 bonferroni 0.9584  9.697583  9.704238  9.720274\n 300  0.95 Sigma_10   proposed 0.9374  6.994651  6.985055  6.998118\n 300  0.95 Sigma_10 bonferroni 0.9558  9.681742  9.615575  9.636666\n 500  0.95 Sigma_01   proposed 0.9556  6.449672  6.450191  6.452175\n 500  0.95 Sigma_01 bonferroni 0.9874  9.253450  9.246494  9.260319\n 500  0.95 Sigma_02   proposed 0.9346  6.441431  6.445940  6.429047\n 500  0.95 Sigma_02 bonferroni 0.9840  9.222195  9.227726  9.225374\n 500  0.95 Sigma_03   proposed 0.9274  6.438055  6.432108  6.435164\n 500  0.95 Sigma_03 bonferroni 0.9824  9.213446  9.247640  9.235903\n 500  0.95 Sigma_04   proposed 0.9418  6.442618  6.455335  6.450027\n 500  0.95 Sigma_04 bonferroni 0.9892  9.242553  9.259845  9.255597\n 500  0.95 Sigma_05   proposed 0.9340  6.423071  6.430700  6.433542\n 500  0.95 Sigma_05 bonferroni 0.9818  9.198471  9.209359  9.194897\n 500  0.95 Sigma_06   proposed 0.9294  6.446906  6.453713  6.426771\n 500  0.95 Sigma_06 bonferroni 0.9828  9.210224  9.251935  9.226937\n 500  0.95 Sigma_07   proposed 0.9442  6.440030  6.432327  6.440906\n 500  0.95 Sigma_07 bonferroni 0.9856  9.235764  9.221775  9.199010\n 500  0.95 Sigma_08   proposed 0.9398  6.448272  6.438224  6.447164\n 500  0.95 Sigma_08 bonferroni 0.9866  9.212267  9.219326  9.240178\n 500  0.95 Sigma_09   proposed 0.9428  6.452612  6.436362  6.439131\n 500  0.95 Sigma_09 bonferroni 0.9838  9.237434  9.227103  9.246770\n 500  0.95 Sigma_10   proposed 0.9384  6.447133  6.446725  6.447741\n 500  0.95 Sigma_10 bonferroni 0.9852  9.233378  9.214613  9.237776\n\n\n\n\np = 2 Two sided\n\nload(\"rdata/results_df_p2.RData\")\n\nprint(results_df_p2, row.names = FALSE)\n\n   n gamma    sigma     method mean_I      Lj_1      Lj_2\n 100  0.95 Sigma_01   proposed 0.9502  9.464721  9.425839\n 100  0.95 Sigma_01 bonferroni 0.9432 13.675320 13.580122\n 100  0.95 Sigma_02   proposed 0.9464  9.357110  9.416104\n 100  0.95 Sigma_02 bonferroni 0.9324 13.320568 13.376452\n 100  0.95 Sigma_03   proposed 0.9376  9.373560  9.454572\n 100  0.95 Sigma_03 bonferroni 0.9250 13.415416 13.476728\n 300  0.95 Sigma_01   proposed 0.9584  6.959330  6.951846\n 300  0.95 Sigma_01 bonferroni 0.9722  9.687155  9.681706\n 300  0.95 Sigma_02   proposed 0.9514  6.959129  6.977847\n 300  0.95 Sigma_02 bonferroni 0.9690  9.663349  9.716652\n 300  0.95 Sigma_03   proposed 0.9548  6.975644  6.946055\n 300  0.95 Sigma_03 bonferroni 0.9688  9.734940  9.662002\n 500  0.95 Sigma_01   proposed 0.9580  6.410187  6.413091\n 500  0.95 Sigma_01 bonferroni 0.9890  9.205941  9.211603\n 500  0.95 Sigma_02   proposed 0.9444  6.396822  6.402755\n 500  0.95 Sigma_02 bonferroni 0.9898  9.223034  9.191342\n 500  0.95 Sigma_03   proposed 0.9458  6.412759  6.407571\n 500  0.95 Sigma_03 bonferroni 0.9868  9.216039  9.216604"
  }
]